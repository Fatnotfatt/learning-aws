[
{
	"uri": "http://localhost:1313/learning-aws/1-worklog/1.6-week6/",
	"title": "Worklog Week 6",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Review fundamental Database Concepts: relational model, primary/foreign keys, ACID, normalization, OLTP vs OLAP. Understand Amazon RDS as a managed relational database service on AWS: engines, Multi-AZ, read replicas, backup, and scaling. Learn the benefits of Amazon Aurora compared to standard RDS engines: performance, high availability, automatic storage scaling, MySQL/PostgreSQL compatibility. Get familiar with Amazon Redshift as a petabyte-scale data warehouse for analytics, and distinguish it from RDS (OLTP workloads). Learn how Amazon ElastiCache (Redis / Memcached) provides an in-memory cache layer to reduce latency and offload backend databases. Practice Database Schema Conversion \u0026amp; Migration using AWS DMS and AWS Schema Conversion Tool (SCT) for moving databases to AWS. Tasks for the Week: Day Task Start Date Completion Date Reference 2 - Review Database Concepts (Module 06-01): relational model, ACID, transactions, indexing, normalization, OLTP vs OLAP. - Map traditional on-premises database concepts to AWS cloud services. 13/10/2025 13/10/2025 Class material – Module 06-01 3 - Study Amazon RDS \u0026amp; Amazon Aurora theory (Module 06-02). - Learn about supported engines, Multi-AZ, automated backups, snapshots, read replicas, and scaling. - Compare RDS vs Aurora in terms of performance, availability, and cost. 14/10/2025 14/10/2025 https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html, https://aws.amazon.com/rds/aurora/ 4 - Study Amazon Redshift \u0026amp; Amazon ElastiCache (Module 06-03). - Distinguish OLTP (RDS/Aurora) vs OLAP (Redshift) and in-memory cache layer (ElastiCache). - Explore common use cases: data warehouse \u0026amp; BI, caching sessions, leaderboard, rate limiting, etc. 15/10/2025 15/10/2025 https://aws.amazon.com/redshift/, https://aws.amazon.com/elasticache/ 5 - Lab Practice: Module 06-Lab 5 – Amazon Relational Database Service (Amazon RDS). + Create an RDS instance, configure security group, parameter group, backups. + Connect from a client, run queries, and test behavior (e.g., failover/Multi-AZ if available in the lab). 16/10/2025 16/10/2025 https://000005.awsstudygroup.com/ 6 - Lab Practice: Module 06-Lab 43 – Database Schema Conversion \u0026amp; Migration. + Use AWS Schema Conversion Tool (SCT) to analyze and convert schema from source DB to RDS/Aurora/Redshift target. + Use AWS Database Migration Service (DMS) to migrate data (full load and change data capture if supported in the lab). - Summarize and review all AWS Database Services covered in Week 6. 17/10/2025 17/10/2025 https://000043.awsstudygroup.com/ Achievements in Week 6: Consolidated understanding of core database concepts:\nRelational tables, primary/foreign keys, relational integrity, and basic indexing. ACID properties of transactions and why they matter in OLTP workloads. Clear distinction between OLTP vs OLAP and how this maps to AWS services. Gained hands-on familiarity with Amazon RDS:\nCreated and managed RDS instances via AWS Management Console. Reviewed supported engines (MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, Aurora) and their typical use cases. Practiced configuring Multi-AZ, automated backups, snapshots, monitoring, and basic scaling options. Understood the strengths of Amazon Aurora:\nAurora as a cloud-native, MySQL/PostgreSQL-compatible database with significantly improved performance over standard engines. Aurora DB cluster architecture, with a distributed storage layer across multiple AZs. Reader and writer endpoints, automatic storage scaling, and high availability design. Built a big-picture view of Amazon Redshift \u0026amp; ElastiCache:\nRedshift as a columnar, petabyte-scale data warehouse optimized for analytics and BI workloads. How Redshift differs from RDS/Aurora: optimized for complex queries over large datasets rather than transactional workloads. ElastiCache (Redis/Memcached) as a fully managed, low-latency in-memory cache layer to increase throughput and reduce load on backend databases. Completed the main labs of the week:\nModule 06-Lab 5 – Amazon RDS: Deployed an RDS instance, connected from a client, executed basic SQL queries. Observed the impact of configuration changes (instance class, storage, backup settings) on behavior and management. Module 06-Lab 43 – Database Schema Conversion \u0026amp; Migration: Used AWS Schema Conversion Tool (SCT) to assess and convert schemas, identifying what can be auto-converted vs. what needs manual adjustment. Used AWS Database Migration Service (DMS) to migrate data from a source database into RDS/Aurora/Redshift targets according to the lab scenario. After Week 6, established a clear mental model of the AWS database ecosystem:\nFrom traditional database concepts → RDS/Aurora for OLTP → Redshift for OLAP → ElastiCache for caching → DMS/SCT for migration, ready to apply in real-world architectures. "
},
{
	"uri": "http://localhost:1313/learning-aws/1-worklog/1.5-week5/",
	"title": "Worklog Week 5",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives This week, my main goal was to master the concepts and security services in AWS, including the shared responsibility model, access management, encryption, and resource protection.\nI also familiarized myself with various AWS tools and services to apply them in hands-on practice.\nSpecific objectives include:\nUnderstand the Shared Responsibility Model of AWS. Master key AWS security services: IAM, Cognito, Security Hub, KMS, Identity Center. Improve resource management and security through IAM Permissions Boundaries, Resource Tags, and encryption techniques. Weekly Tasks Day Task Start Date Completion Date Reference Source 2 - Study the theory of the Shared Responsibility Model and AWS security principles. - Review documentation for AWS security services: + Amazon IAM + Amazon Cognito + AWS Identity Center + AWS KMS + AWS Security Hub 04/10/2025 04/10/2025 AWS Study Group 3 - Hands-on: + Configure and use AWS Security Hub to monitor and detect security issues. + Create and manage IAM Users, Roles, and Policies for AWS accounts. + Create IAM Groups and manage access permissions for user groups. 05/10/2025 05/10/2025 AWS Study Group 4 - Hands-on: + Optimize EC2 costs using Lambda for automated start/stop of EC2 instances. + Manage EC2 access via Resource Tags using IAM. + Configure IAM Permission Boundaries to limit user privileges. + Encrypt data using AWS KMS. 06/10/2025 06/10/2025 AWS Study Group 5 - Advanced Practice: + Learn and apply security methods in AWS Organizations for multi-account management. + Enhance proficiency in AWS Identity Center for managing and synchronizing users and groups across AWS services. 07/10/2025 07/10/2025 AWS Study Group Results Achieved in Week 5 During this week, I achieved significant progress in understanding and applying AWS security services, effectively bridging theory with practice. Specifically:\nUnderstanding and Applying the AWS Shared Responsibility Model\nI fully grasped that AWS is responsible for the security of the cloud infrastructure, while users are responsible for securing their own data and applications. This clarified my role in ensuring compliance and protection when deploying services on AWS. Theoretical Knowledge of AWS Core Security Services\nAmazon IAM: Learned how to create and manage Users, Roles, and Policies to control user and group access. Amazon Cognito: Studied user management and authentication for AWS applications. AWS Identity Center: Understood how to link and manage user access across multiple AWS services. AWS Security Hub: Configured and utilized it to monitor and detect security threats. AWS KMS: Practiced encrypting data at rest and securing sensitive data using encryption keys. Practical Implementation of AWS Security Services\nSuccessfully installed and configured AWS Security Hub for continuous monitoring and vulnerability detection. Configured IAM Permissions Boundaries to restrict user privileges and prevent unauthorized access. EC2 Cost Optimization with Lambda: Automated the shutdown of unused EC2 instances to minimize operational costs. EC2 Access Control via IAM \u0026amp; Resource Tags: Applied IAM Policies that use Tags to precisely define access scope. Enhanced AWS Resource Management Skills\nCreated and managed IAM Groups and Policies, improving group-based access control. Learned how to manage multiple AWS accounts using AWS Organizations, ensuring consistent security policies across the organization. LAB PRACTICE Table of Contents Lab 18 Lab 22 VPC EC2 Slack Lambda + EventBridge Test Results Lab 27 Lab 28 Regions \u0026amp; EC2 Tags Lab 30 Lab 33 Lab 18 Illustration:\nLab 22 VPC VPC configuration illustrations:\nEC2 EC2 configuration illustrations:\nSlack Note (New UI): Re-select the channel during setup to get the correct Webhook URL.\nLambda + EventBridge Lambda and EventBridge configuration illustrations:\nTest Results Lab 27 Illustrations:\nLab 28 Illustrations:\nRegions \u0026amp; EC2 EC2 in ap-northeast-1 (Tokyo)\nEC2 in us-east-1 (North Virginia)\nTags Sample key/value pairs used:\nKey Value Name Example Team Beta Team Alpha Team TEST Tag interface examples:\nName = Example, Team = Beta\nName = Example, Team = Alpha\nTeam = TEST\nLab 30 Policies: IAM: Check Permission: Lab 33 Policies Role User KMS S3 CloudTrail Athena Test after applying ACLs Conclusion During Week 5, I significantly improved my ability to use AWS security and access management tools.\nThese knowledge and skills form a solid foundation for implementing advanced security solutions and cost optimization in future AWS projects.\nThe lab exercises reinforced theoretical understanding and enhanced my practical skills for working effectively in real AWS environments.\n"
},
{
	"uri": "http://localhost:1313/learning-aws/1-worklog/1.4-week4/",
	"title": "Worklog Week 4",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives Gain an in-depth understanding of AWS’s core storage service Amazon S3. Master the key concepts: bucket, object, storage class, access point, static website hosting, and CORS. Study hybrid storage and data migration solutions such as AWS Storage Gateway and AWS Snow Family. Get familiar with Amazon FSx for Windows File Server and the automated backup service AWS Backup. Practice deploying, managing, and integrating AWS storage services in a real-world environment. Tasks to Be Completed in Week 4 Day Task Start Date Completion Date References 2 - Study the theory of AWS Storage Service (S3) – Module 04-01.\n- Get familiar with the concepts of Bucket, Object, and the storage mechanism. 29/09/2025 29/09/2025 https://docs.aws.amazon.com/s3/ 3 - Learn about Access Point and Storage Class in S3 – Module 04-02.\n- Distinguish between storage classes: Standard, IA, Glacier, Deep Archive. 30/09/2025 30/09/2025 https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html 4 - Explore S3 Static Website \u0026amp; CORS, Access Control, Object Key, Performance, and Glacier – Module 04-03. 01/10/2025 01/10/2025 https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html 5 - Hands-on: Module 04-Lab13 – Deploy AWS Backup to the System.\n- Hands-on: Module 04-Lab14 – VM Import/Export. 02/10/2025 02/10/2025 Lab13, Lab14 6 - Hands-on: Module 04-Lab24 – Using File Storage Gateway.\n- Hands-on: Module 04-Lab25 – Amazon FSx for Windows File Server.\n- Review and consolidate all AWS storage services. 03/10/2025 03/10/2025 Lab24, Lab25 Results Achieved in Week 4 Gained a clear understanding of the architecture and operating principles of Amazon S3, including:\nHow to create and manage Buckets, Objects, and Access Policies. Different Storage Classes and strategies for optimizing storage costs. How to configure S3 Static Website Hosting and handle CORS for web applications. Became familiar with S3 Glacier – a cold storage service that helps save costs for infrequently accessed data.\nGained a solid understanding of Hybrid Storage \u0026amp; Data Migration through:\nAWS Snow Family (Snowcone, Snowball, Snowmobile). AWS Storage Gateway – a solution to connect on-premises systems with AWS Cloud. Successfully completed the following labs:\nLab 13 – AWS Backup Goal: Configure and deploy resource backups with AWS Backup. Step 1:\nStep 2:\nStep 3:\nStep 4:\nSuccess:\nLab 14 – VM Import/Export Goal: Perform VM Import/Export – migrate virtual machines between the local environment and AWS. Step 1:\nSuccess:\nStep 2:\nStep 3:\nStep 4:\nStep 5:\nStep 6 (successfully uploaded the VM to EC2 (AMIs)):\nStep 7:\nStep 8 (Internet connectivity test):\nStep 9:\nStep 10 (Done):\nLab 24 – File Storage Gateway Goal: Configure File Storage Gateway – create and link file storage between on-premises systems and AWS. Note: The account must be upgraded.\nStep 1 – After creating the S3 bucket, create the File Storage Gateway (FSG):\nStep 2 – EC2 settings:\nStep 3:\nLab 25 – Amazon FSx for Windows File Server Goal: Deploy a file storage system for Windows using Amazon FSx for Windows File Server. Step 1 (Lambda error – Node.js version):\nStep 2:\nCompleted the entire Module 04 – AWS Storage Services, building a solid foundation to move on to compute, database, and security services in the following weeks. "
},
{
	"uri": "http://localhost:1313/learning-aws/1-worklog/1.3-week3/",
	"title": "Worklog Week 3",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Gain a solid understanding of Amazon EC2 and its ecosystem (AMI, Backup, Key Pair, EBS, Instance Store, User Data, Metadata). Learn about EC2 Auto Scaling and its role in elasticity and cost optimization. Explore related compute services including EFS, FSx, Lightsail, and AWS MGN. Strengthen AWS storage knowledge with hands-on labs covering S3, AWS Backup, and Storage Gateway. Develop practical skills in configuring, managing, and scaling EC2 workloads. Tasks for this week: Day Task Start Date Completion Date Reference 1 - Theory: + EC2 overview: AMI, Backup, Key Pair 22/09/2025 22/09/2025 AWS EC2 Documentation 2 - Theory: + EBS (Elastic Block Store) + Instance Store 23/09/2025 23/09/2025 AWS EBS Documentation 3 - Theory: + EC2 User Data + EC2 Metadata 24/09/2025 24/09/2025 AWS EC2 User Guide 4 - Theory: + EC2 Auto Scaling + Related services: EFS, FSx, Lightsail, MGN 25/09/2025 25/09/2025 AWS Auto Scaling 5 - Hands-on: + Lab 57: Start with Amazon S3 26/09/2025 26/09/2025 AWS Study Group - Lab57 6 - Hands-on: + Lab 13: Deploy AWS Backup to the System 27/09/2025 27/09/2025 AWS Study Group - Lab13 7 - Hands-on: + Lab 24: Using AWS Storage Gateway 28/09/2025 28/09/2025 AWS Study Group - Lab24 Week 3 Achievements: Built strong theoretical knowledge of Amazon EC2, including:\nAMI and backup strategies for resilience. Key Pair usage for secure SSH authentication. Differences between EBS (persistent storage) and Instance Store (ephemeral storage). How User Data and Metadata scripts automate instance initialization and provide dynamic configuration. The role of EC2 Auto Scaling in maintaining performance and cost efficiency. Expanded understanding of related services:\nAmazon EFS and FSx for shared and high-performance file storage. Amazon Lightsail as a simplified alternative for small-scale workloads. AWS MGN for migrating workloads into AWS. Completed practical labs:\nLaunched and managed an S3 bucket (Lab57). Implemented AWS Backup to protect workloads (Lab13). Integrated on-premises systems with AWS using Storage Gateway (Lab24). Key skills acquired:\nConfidently distinguish storage types (EBS vs Instance Store vs EFS vs FSx). Automate EC2 lifecycle with User Data and Auto Scaling. Combine backup and hybrid storage solutions to create more resilient architectures. Direction:\nThis week provided a deeper dive into compute and storage fundamentals. By combining theoretical concepts with hands-on practice, I strengthened my ability to not only launch and manage EC2 instances but also design scalable, reliable, and cost-efficient architectures that integrate with AWS’s broader storage and migration services.\n"
},
{
	"uri": "http://localhost:1313/learning-aws/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Get familiar with the First Cloud Journey (FCJ) team. Understand AWS core services and learn how to interact with them via Console \u0026amp; CLI. Tasks completed this week: Day Task Start Date Completion Date Reference 2 - Meet and get to know FCJ members.\n- Read and understand internship rules and guidelines. 2025-09-02 2025-09-02 3 - Study AWS and its main service groups:\n+ Compute + Storage + Networking + Database + \u0026hellip; 2025-09-03 2025-09-03 https://000001.awsstudygroup.com/en/ 4 - Register AWS Free Tier account.\n- Explore AWS Console \u0026amp; CLI.\n- Practice:\n+ Create AWS account + Install \u0026amp; configure AWS CLI + Basic CLI tasks 2025-09-04 2025-09-04 https://000001.awsstudygroup.com/en/ 5 - Configure basic security:\n+ Setup Virtual MFA Device + Create admin group \u0026amp; admin user + Account authentication support + Create Budget 2025-09-05 2025-09-06 https://000007.awsstudygroup.com/en/ 6 - Practice cost management:\n+ Create Cost Budget + Create Usage Budget + Reservation Instance (RI) + Savings Plans Budget 2025-09-06 2025-09-06 https://000007.awsstudygroup.com/en/ 7 - Submit AWS support request and manage responses.\n- Write worklog \u0026amp; self-assess AWS fundamentals.\n- Prepare for Week 2 goals. 2025-09-07 2025-09-07 https://000009.awsstudygroup.com/en/ Week 1 Outcomes: Onboarding completed:\nConnected with FCJ members. Understood internship guidelines and basic rules. AWS foundation knowledge:\nLearned what AWS is and its main service categories: Compute Storage Networking Database \u0026hellip; Account setup \u0026amp; configuration:\nSuccessfully registered AWS Free Tier account. Configured basic security: enabled MFA, created admin group \u0026amp; admin user. Created budgets to monitor costs: Cost Budget, Usage Budget, RI, Savings Plans. Management tools:\nPracticed AWS Management Console: navigating and using services via GUI. Installed and configured AWS CLI with: Access Key, Secret Key, default Region. Hands-on with AWS CLI:\nChecked account and configuration info. Listed regions. Viewed EC2 information. Created and managed key pairs. Monitored running services. Console \u0026amp; CLI integration:\nManaged AWS resources in parallel using Console and CLI. Compared approaches and gained insights on when to use each tool. Personal reflection:\nCompleted Week 1 worklog (submitted late in Week 2). Assessed current understanding and set targets for the upcoming week. "
},
{
	"uri": "http://localhost:1313/learning-aws/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Accelerating Enterprise ML Experimentation with Amazon SageMaker AI and Comet By: Vikesh Pandey, Naufal Mir and Sarah Ostermeier \u0026ndash; Date: September 22, 2025\nTopics: Amazon SageMaker AI, SageMaker Unified Studio, Partner solutions, Sarah Ostermeier\nAs organizations scale their machine learning (ML) operations from proof-of-concept to production, managing experiments, tracking model lineage, and ensuring reproducibility become significantly more complex. The main reason is that data scientists and ML engineers often experiment with numerous combinations of hyperparameters, model architectures, and dataset versions, generating large amounts of metadata that need to be tracked to ensure reproducibility and regulatory compliance. As ML models scale across multiple teams and regulatory requirements increase, experiment tracking becomes mandatory rather than just a \u0026ldquo;best practice\u0026rdquo;.\nAmazon SageMaker AI provides managed infrastructure for enterprises to scale ML workloads, handling compute provisioning, distributed training, and deployment without worrying about infrastructure. However, teams still need a robust experiment tracking system, model comparison capabilities, and collaboration beyond basic logging.\nComet is a comprehensive ML experiment management platform that automatically tracks, compares, and optimizes ML experiments throughout the model lifecycle. It provides data scientists and ML engineers with powerful tools for tracking, model monitoring, hyperparameter optimization, and collaborative model development. It also includes Opik — Comet\u0026rsquo;s open-source platform for LLM (large language model) observability and development.\nComet is available in SageMaker AI as a Partner AI App, as a fully managed capability for experimentation, with enterprise-grade security, seamless workflow integration, and a simple procurement process through AWS Marketplace.\nThis combination addresses the needs of end-to-end enterprise ML workflows: SageMaker AI handles infrastructure and compute, while Comet provides the experiment management, model registry, and production monitoring capabilities that teams need for regulatory compliance and operational efficiency. In this article, we demonstrate a complete fraud detection workflow using SageMaker AI + Comet, showcasing the reproducibility and audit-ready logging that modern enterprises require.\nMaking Comet \u0026ldquo;Enterprise-ready\u0026rdquo; on SageMaker AI Before diving into the deployment guide, organizations need to define their operating model and determine how to deploy Comet. AWS recommends setting up Comet following a federated operating model: Comet is centrally managed in a shared services account, and each ML data team has its own autonomous environment. Each operating model has its own pros and cons. (Refer to SageMaker Studio Administration Best Practices for details).\nIn this architecture, there are typically two roles:\nAdministrator \u0026ndash; responsible for setting up shared infrastructure and environments for use-case teams\nUser \u0026ndash; ML practitioners from use-case teams who use the established environment to solve business problems\nComet works well with both SageMaker AI and Amazon SageMaker (SageMaker AI uses the integrated environment in SageMaker Studio IDE; SageMaker uses Unified Studio IDE). Here, we use SageMaker Studio in the example.\nAdministrator Journey When a team wants to deploy a fraud detection use-case, the admin performs:\nComplete the prerequisite steps to set up Partner AI Apps \u0026mdash; grant permissions so Comet can assume the user\u0026rsquo;s SageMaker AI role and manage Comet subscription through AWS Marketplace.\\\nIn the SageMaker AI console, go to Applications and IDEs → Partner AI Apps → Comet to view details.\nDisplays subscription details, pricing model, and estimated Comet infrastructure costs. Select Go to Marketplace to subscribe to Comet from AWS Marketplace.\nSelect \u0026ldquo;View purchase options\u0026rdquo; and fill in the subscription information.\nAfter subscription is complete, the admin begins configuring Comet.\nWhen deploying Comet, add the fraud detection team\u0026rsquo;s project lead as an admin to manage the Comet dashboard. The Comet deployment process takes a few minutes. (Refer to the Partner AI App provisioning guide for details).\nSet up the SageMaker AI domain following the Use custom setup for Amazon SageMaker AI guide. As a best practice, provide a pre-signed domain URL so the use-case team can access the Comet UI without logging into the SageMaker console.\nAdd team members to the domain and enable Comet access when configuring the domain.\nAfter these steps, the SageMaker AI domain is ready for users to log in and begin working.\nUser Journey (ML Practitioner) When the environment is ready, the user performs:\nLog into the SageMaker AI domain via the pre-signed URL.\nAutomatically redirects to SageMaker Studio IDE, with username and IAM execution role pre-configured by the admin. Create a JupyterLab Space following the JupyterLab user guide.\nBegin the fraud detection use-case by launching a notebook.\nThe admin has already granted data access through the necessary S3 buckets. To use Comet\u0026rsquo;s API, install the comet_ml package and configure environment variables following the Set up Partner AI Apps SDKs guide.\nIn SageMaker Studio, select Partner AI Apps → Open Comet to access the Comet UI.\nBegin the experiment workflow. Solution Overview This use-case highlights common challenges in enterprises:\nImbalanced datasets (e.g., only ~0.17% of transactions are fraudulent)\nMultiple iteration rounds\nRequirements for complete reproducibility and audit compliance\nData \u0026amp; model lineage must be recorded in detail\nUsing the Credit Card Fraud Detection dataset, with binary labels \u0026mdash; 1 for fraud, 0 for legitimate. The following steps illustrate the key parts of the implementation (complete code is available in Comet\u0026rsquo;s GitHub repo).\nPrerequisites Configure the imports and Comet + SageMaker environment variables:\n# Comet ML for experiment tracking\nimport comet_ml\nfrom comet_ml import Experiment, API, Artifact\nfrom comet_ml.integration.sagemaker import log_sagemaker_training_job_v1\nAWS_PARTNER_APP_AUTH = True\nAWS_PARTNER_APP_ARN = \u0026lt;Your_AWS_PARTNER_APP_ARN\u0026gt;\nCOMET_API_KEY = \u0026lt;Your_Comet_API_Key\u0026gt;\nCOMET_WORKSPACE = \u0026lsquo;\u0026lt;your-comet-workspace-name\u0026gt;\u0026rsquo;\nCOMET_PROJECT_NAME = \u0026lsquo;\u0026lt;your-comet-project-name\u0026gt;\u0026rsquo;\nThe AWS_PARTNER_APP_ARN and COMET_API_KEY variables are obtained from the Comet details page in SageMaker.\nCOMET_WORKSPACE and COMET_PROJECT_NAME are the workspace and project names you will use to group experiments.\nPreparing the Dataset A key feature of Comet is automatic dataset versioning \u0026amp; lineage tracking. This enables full audit trails of which data was used to train each model \u0026mdash; crucial in regulated environments.\nExample:\n# Create dataset Artifact to track the original dataset\ndataset_artifact = Artifact(\nname=\u0026ldquo;fraud-dataset\u0026rdquo;,\nartifact_type=\u0026ldquo;dataset\u0026rdquo;,\naliases=[\u0026ldquo;raw\u0026rdquo;]\n)\ndataset_artifact.add_remote(s3_data_path, metadata={\n\u0026ldquo;dataset_stage\u0026rdquo;: \u0026ldquo;raw\u0026rdquo;,\n\u0026ldquo;dataset_split\u0026rdquo;: \u0026ldquo;not_split\u0026rdquo;,\n\u0026ldquo;preprocessing\u0026rdquo;: \u0026ldquo;none\u0026rdquo;\n})\nArtifact allows tagging of dataset files and associated metadata\nRaw data is added to the artifact for Comet to track the dataset source\nStarting a Comet Experiment After the artifact has been logged, you start an experiment and Comet will automatically record background metadata, environment, libraries, code, etc.\nexperiment_1 = comet_ml.Experiment(\nproject_name=COMET_PROJECT_NAME,\nworkspace=COMET_WORKSPACE,\n)\nexperiment_1.log_artifact(dataset_artifact)\nExperiment automatically begins recording information\nlog_artifact logs the dataset artifact to the experiment for traceability\nData Preprocessing Preprocessing steps include:\nRemoving duplicate records\nDropping unnecessary columns\nSplitting data into train/validation/test sets\nFeature standardization using scikit-learn\u0026rsquo;s StandardScaler\nThe preprocessing code is written in the preprocess.py file and run as a SageMaker Processing Job:\nprocessor = SKLearnProcessor(\nframework_version=\u0026lsquo;1.0-1\u0026rsquo;,\nrole=sagemaker.get_execution_role(),\ninstance_count=1,\ninstance_type=\u0026lsquo;ml.t3.medium\u0026rsquo;\n)\nprocessor.run(\ncode=\u0026lsquo;preprocess.py\u0026rsquo;,\ninputs=[ProcessingInput(source=s3_data_path, destination=\u0026rsquo;/opt/ml/processing/input\u0026rsquo;)],\noutputs=[ProcessingOutput(source=\u0026rsquo;/opt/ml/processing/output\u0026rsquo;, destination=f\u0026rsquo;s3://{bucket_name}/{processed_data_prefix}\u0026rsquo;)]\n)\nWhen the job starts, SageMaker AI creates an instance, processes the data, and then releases the resource.\nPreprocessing results are saved to S3.\nAfter completion, you create a new version of the dataset artifact to track the processed data:\npreprocessed_dataset_artifact = Artifact(\nname=\u0026ldquo;fraud-dataset\u0026rdquo;,\nartifact_type=\u0026ldquo;dataset\u0026rdquo;,\naliases=[\u0026ldquo;preprocessed\u0026rdquo;],\nmetadata={\n\u0026ldquo;description\u0026rdquo;: \u0026ldquo;Credit card fraud detection dataset\u0026rdquo;,\n\u0026ldquo;fraud_percentage\u0026rdquo;: f\u0026quot;{fraud_percentage:.3f}%\u0026quot;,\n\u0026ldquo;dataset_stage\u0026rdquo;: \u0026ldquo;preprocessed\u0026rdquo;,\n\u0026ldquo;preprocessing\u0026rdquo;: \u0026ldquo;StandardScaler + train/val/test split\u0026rdquo;,\n}\n)\npreprocessed_dataset_artifact.add_remote(\nuri=f\u0026rsquo;s3://{bucket_name}/{processed_data_prefix}',\nlogical_path=\u0026lsquo;split_data\u0026rsquo;\n)\nexperiment_1.log_artifact(preprocessed_dataset_artifact)\nArtifacts with the same name but different aliases allow Comet to manage versioning\nAdditional metadata helps document what was done (split, preprocessing\u0026hellip;)\nComet + SageMaker AI Experiment Workflow To accelerate rapid experimentation, you should organize the workflow into utility functions that can be called multiple times with different hyperparameters while ensuring consistent logging and evaluation.\nKey functions:\ntrain() \u0026mdash; creates an XGBoost training job on SageMaker:\nestimator = Estimator(\nimage_uri=xgboost_image,\nrole=execution_role,\ninstance_count=1,\ninstance_type=\u0026lsquo;ml.m5.large\u0026rsquo;,\noutput_path=model_output_path,\nsagemaker_session=sagemaker_session_obj,\nhyperparameters=hyperparameters_dict,\nmax_run=1800 # maximum time (seconds)\n)\nestimator.fit({\n\u0026rsquo;train\u0026rsquo;: train_channel,\n\u0026lsquo;validation\u0026rsquo;: val_channel\n})\nlog_training_job() \u0026mdash; logs training metadata to Comet and links the model: log_sagemaker_training_job_v1(\nestimator=training_estimator,\nexperiment=api_experiment\n)\nlog_model_to_comet() \u0026mdash; logs model artifact to Comet: experiment.log_remote_model(\nmodel_name=model_name,\nuri=model_artifact_path,\nmetadata=metadata\n)\ndeploy_and_evaluate_model() \u0026mdash; deploys endpoint and evaluates, logs metrics: predictor = estimator.deploy(initial_instance_count=1, instance_type=\u0026ldquo;ml.m5.xlarge\u0026rdquo;)\nexperiment.log_metrics(metrics)\nexperiment.log_confusion_matrix(matrix=cm, labels=[\u0026lsquo;Normal\u0026rsquo;, \u0026lsquo;Fraud\u0026rsquo;])\nfpr, tpr, _ = roc_curve(y_test, y_pred_prob_as_np_array)\nexperiment.log_curve(\u0026ldquo;roc_curve\u0026rdquo;, x=fpr, y=tpr)\nComplete prediction and evaluation code is available in the GitHub repo. Running the Experiments You can try multiple experiments by calling utility functions with different hyperparameter configurations and comparing results to select the optimal configuration for your use-case.\nFor example, the first experiment (baseline):\nhyperparameters_v1 = {\n\u0026lsquo;objective\u0026rsquo;: \u0026lsquo;binary:logistic\u0026rsquo;,\n\u0026rsquo;num_round\u0026rsquo;: 100,\n\u0026rsquo;eval_metric\u0026rsquo;: \u0026lsquo;auc\u0026rsquo;,\n\u0026rsquo;learning_rate\u0026rsquo;: 0.15,\n\u0026lsquo;booster\u0026rsquo;: \u0026lsquo;gbtree\u0026rsquo;\n}\nestimator_1 = train(\nmodel_output_path=f\u0026quot;s3://{bucket_name}/{model_output_prefix}/1\u0026quot;,\nexecution_role=role,\nsagemaker_session_obj=sagemaker_session,\nhyperparameters_dict=hyperparameters_v1,\ntrain_channel_loc=train_channel_location,\nval_channel_loc=validation_channel_location\n)\nlog_training_job(experiment_key = experiment_1.get_key(), training_estimator=estimator_1)\nlog_model_to_comet(\nexperiment = experiment_1,\nmodel_name=\u0026ldquo;fraud-detection-xgb-v1\u0026rdquo;,\nmodel_artifact_path=estimator_1.model_data,\nmetadata=metadata\n)\ndeploy_and_evaluate_model(\nexperiment=experiment_1,\nestimator=estimator_1,\nX_test_scaled=X_test_scaled,\ny_test=y_test\n)\nWhen running a Comet experiment from a Jupyter notebook, you need to call experiment_1.end() to ensure all information is recorded and saved to the Comet server.\nAfter the baseline experiment completes, you can launch the next experiment with different hyperparameters and compare the two experiments in the Comet UI.\nViewing Experiments in Comet UI To access the UI, you can get the URL from SageMaker Studio IDE or print it from the notebook using experiment_2.url.\nScreenshots of the Comet interface show experiments being compared \u0026mdash; these details are for illustration purposes and do not represent actual experiments.\n(Note: insert Comet UI screenshot here)\nClean Up Due to the ephemeral nature of SageMaker infrastructure (processing, training) \u0026mdash; it automatically shuts down after the job completes. However, you still need to:\nShut down JupyterLab Space when not in use (following the Idle shutdown guide).\nUnsubscribe from Comet if not continuing to use it (to avoid charges) \u0026mdash; the subscription will auto-renew if not cancelled.\nBenefits of SageMaker + Comet Integration Streamlined Model Development The SageMaker-Comet combination reduces manual burden when running experiments. While SageMaker handles infrastructure provisioning, Comet automatically logs hyperparameters, metrics, code, libraries, system information \u0026mdash; no additional configuration needed.\nComet supports visualization beyond simple metric charts: integrated charts enable quick experiment comparison; custom Python panels help you debug model behavior, optimize hyperparameters, or create custom visuals when default tools don\u0026rsquo;t meet needs.\nEnterprise Collaboration \u0026amp; Governance In enterprise environments, this combination creates a strong foundation for scaling ML projects in heavily regulated environments. SageMaker ensures consistent, secure ML environments; Comet supports collaboration with complete artifact flows and lineage. This helps avoid errors when teams cannot reproduce previous results.\nComplete ML Lifecycle Integration Unlike fragmented solutions that only support training or monitoring, SageMaker + Comet supports the entire ML lifecycle.\nModels can be registered in Comet\u0026rsquo;s model registry with versioning and management.\nSageMaker handles deployment.\nComet maintains lineage and approval workflow for promotion.\nComet monitors model performance and tracks data drift after deployment \u0026mdash; creating a feedback loop where information from production influences subsequent experiments.\nConclusion In this article, we presented how to integrate SageMaker and Comet to create a fully managed ML environment supporting reproducibility and experiment tracking. To complement your SageMaker workflow, you can deploy Comet directly within the SageMaker environment through AWS Marketplace.\nAbout the Authors "
},
{
	"uri": "http://localhost:1313/learning-aws/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Using Apache Airflow workflows to orchestrate data processing on Amazon SageMaker Unified Studio by Vinod Jayendra, Kamen Sharlandjiev, Sean Bjurstrom, and Suba Palanisamy\nSEPTEMBER 22, 2025\nOrchestrating machine learning pipelines is a complex task, especially when data processing, model training, and deployment are performed across multiple services and tools. In this article, we will walk through a practical \u0026ldquo;end-to-end\u0026rdquo; example — building, testing, and running an ML pipeline using SageMaker workflows through the SageMaker Unified Studio interface. These workflows are powered by Amazon Managed Workflows for Apache Airflow (Amazon MWAA).\nAlthough SageMaker Unified Studio has a visual builder (low-code) to create workflows, this article focuses on the code-based approach: writing and managing workflows as DAGs (Directed Acyclic Graphs) using Python in Apache Airflow.\nWe will examine a pipeline example that includes the following steps: ingesting weather data and taxi data, transforming \u0026amp; merging the data, then using ML to predict taxi fares — all orchestrated through SageMaker Unified Studio workflows.\nSolution overview This solution demonstrates how to use workflows in SageMaker Unified Studio to orchestrate a pipeline from data to ML model in a centralized environment. The pipeline consists of the following tasks:\nIngest \u0026amp; preprocess weather data\nUse a notebook in SageMaker Unified Studio to ingest simulated weather data, processing attributes such as time, temperature, rainfall, humidity, and wind speed.\nIngest, process, and merge taxi data\nUse a second notebook to ingest NYC taxi data (including pickup time, drop-off time, distance, passenger count, fare amount). Then process and join the taxi \u0026amp; weather data, saving the results to Amazon S3 for use in the next step.\nTrain and predict ML model\nA third notebook applies regression techniques to build a model that predicts taxi fares based on the merged data. The model is then used to make predictions on new data.\nThrough this approach, ETL (extract, transform, load) and ML steps are orchestrated within the same workflow, with full traceability of the data process and ensuring reproducibility through workflow management in SageMaker Unified Studio.\nPrerequisites Before building the workflow, you need to:\nCreate a SageMaker Unified Studio domain — follow AWS guidelines.\n(Create an Amazon SageMaker Unified Studio domain – quick setup)\nLog in to SageMaker Unified Studio domain — use the domain you created.\n(Access Amazon SageMaker Unified Studio)\nCreate a project in SageMaker Unified Studio — when creating the project, select the \u0026ldquo;All capabilities\u0026rdquo; profile to support full workflow functionality.\n(project creation guide)\nSet up workflow environment You can use workflows in SageMaker Unified Studio to set up and run a series of tasks such as notebooks, querybooks, and jobs. Workflows are written in Python code (Airflow DAGs), then you can access the Airflow UI from SageMaker for monitoring.\nSpecific steps:\nIn your project, go to Compute → Workflow environment.\nSelect Create environment to set up a new workflow environment.\nBy default, SageMaker Unified Studio will use the mw1.micro environment type — suitable for small testing.\nIf needed, you can override the default configuration (e.g., increase resources) when creating the project or adjust in blueprint deployment settings.\nDevelop workflows Workflows allow you to orchestrate notebooks, querybooks, etc. within the project. You can write Python DAGs, test, and share them with other members.\nExample:\nDownload 3 sample notebooks: Weather Data Ingestion, Taxi Ingest \u0026amp; Join, Prediction to your machine.\nIn SageMaker Unified Studio, go to Build → JupyterLab, upload the 3 notebooks.\nConfigure space: stop the current space → change instance type (e.g., ml.m5.8xlarge) → restart space.\nGo to Build → Orchestration → Workflows, select \u0026ldquo;Create new workflow\u0026rdquo; → select \u0026ldquo;Create in code editor\u0026rdquo;.\nIn the editor, create a new Python file multinotebook_dag.py in the src/workflows/dags folder. Paste the following example DAG code (modify \u0026lt;REPLACE-OWNER\u0026gt; and notebook paths accordingly):\nfrom airflow.decorators import dag\nfrom airflow.utils.dates import days_ago\nfrom workflows.airflow.providers.amazon.aws.operators.sagemaker_workflows import NotebookOperator\nWORKFLOW_SCHEDULE = \u0026lsquo;@daily\u0026rsquo;\nNOTEBOOK_PATHS = [\n\u0026lsquo;\u0026lt;FULL_PATH/Weather_Data_Ingestion.ipynb\u0026gt;\u0026rsquo;,\n\u0026lsquo;\u0026lt;FULL_PATH/Taxi_Weather_Data_Collection.ipynb\u0026gt;\u0026rsquo;,\n\u0026lsquo;\u0026lt;FULL_PATH/Prediction.ipynb\u0026gt;\u0026rsquo;\n]\ndefault_args = {\n\u0026lsquo;owner\u0026rsquo;: \u0026lsquo;\u0026lt;REPLACE-OWNER\u0026gt;\u0026rsquo;,\n}\n@dag(\ndag_id=\u0026lsquo;workflow-multinotebooks\u0026rsquo;,\ndefault_args=default_args,\nschedule_interval=WORKFLOW_SCHEDULE,\nstart_date=days_ago(2),\nis_paused_upon_creation=False,\ntags=[\u0026lsquo;MLPipeline\u0026rsquo;],\ncatchup=False\n)\ndef multi_notebook():\nprevious_task = None\nfor idx, notebook_path in enumerate(NOTEBOOK_PATHS, 1):\ncurrent_task = NotebookOperator(\ntask_id=f\u0026quot;Notebook{idx}task\u0026quot;,\ninput_config={\u0026lsquo;input_path\u0026rsquo;: notebook_path, \u0026lsquo;input_params\u0026rsquo;: {}},\noutput_config={\u0026lsquo;output_formats\u0026rsquo;: [\u0026lsquo;NOTEBOOK\u0026rsquo;]},\nwait_for_completion=True,\npoll_interval=5\n)\nif previous_task: previous_task \u0026gt;\u0026gt; current_task\rprevious_task = current_task\rmulti_notebook()\n5. * NotebookOperator is used to run each notebook, with dependencies to ensure execution order.\n* You can customize WORKFLOW_SCHEDULE (e.g., @daily, @hourly, or cron expression).\rAfter the workflow environment is created and the DAG file is synced to the project, project members can view and run the shared workflow. Test and monitor workflow Go to Build → Orchestration → Workflows, you will see the workflow running on schedule or triggered.\nWhen the workflow completes, the status changes to \u0026ldquo;success\u0026rdquo;.\nYou can view each execution for details, logs of each task.\nAccess the Airflow UI from SageMaker to view DAGs, run history, detailed logs.\ny\nResults \u0026amp; outputs The model results are written to the results directory on Amazon S3. You need to check:\nPrediction accuracy\nConsistency in relationships between variables\nIf there are anomalous results, review the data processing steps, pipeline, and model assumptions.\nClean up To avoid unnecessary costs, you should delete the created resources:\nSageMaker Unified Studio domain\nS3 buckets related to the domain\nWorkflow environments, projects if no longer in use\nConclusion In this article, we demonstrated how you can use SageMaker Unified Studio to build an integrated ML workflow, including:\nCreating a SageMaker Unified Studio project\nUsing multi-compute notebooks to process data\nBuilding a DAG workflow in Python to orchestrate the entire pipeline\nRunning and monitoring workflows in SageMaker Unified Studio\nSageMaker provides a comprehensive toolset to execute steps from data preparation, model training to deployment. When used through SageMaker Unified Studio, these tools are consolidated in a single working environment, helping eliminate friction between disparate tools.\nAs organizations build complex data applications, teams can use SageMaker + Unified Studio to collaborate effectively and operate AI/ML with high reliability. You can discover data, build models, and orchestrate workflows in a managed and controlled environment.\nAbout the authors "
},
{
	"uri": "http://localhost:1313/learning-aws/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/learning-aws/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/learning-aws/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/learning-aws/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/learning-aws/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: “GenAI-powered App-DB Modernization workshop” Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "http://localhost:1313/learning-aws/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: “GenAI-powered App-DB Modernization workshop” Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "http://localhost:1313/learning-aws/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Võ Trường Thành Phát\nPhone Number: 0707712750\nEmail: phatvttse171823@fpt.edu.vn\nUniversity: FPT University\nMajor: Software Engineer\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/learning-aws/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 7 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/learning-aws/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/learning-aws/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/learning-aws/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nOn this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/learning-aws/1-worklog/1.2-week2/",
	"title": "Worklog Week 2",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand the concept and structure of VPC (CIDR, Subnet, Route Table, ENI). Learn how to configure firewalls in VPC (NACL, Security Group). Get familiar with networking services: VPN, Direct Connect. Study and practice Load Balancer. Practice creating and configuring core components: VPC, Subnet, Route Table, IGW, EBS, Elastic IP. Learn how to connect and remote into EC2 via SSH. Get hands-on with Hybrid DNS using Route 53 Resolver. Practice connecting multiple VPCs using VPC Peering. Deploy AWS Transit Gateway to manage inter-VPC connections. Tasks for this week: Day Task Start Date Completion Date Reference 2 - Learn theory\n- What is VPC and how to optimize cloud service usage 15/09/2025 15/09/2025 AWS VPC Documentation 3 - Learn about VPC\n+ Subnet, CIDR + Route table + ENI (Elastic Network Interface) 16/09/2025 16/09/2025 YouTube - AWS VPC 4 - Configure VPC firewalls: NACL, Security Group\n- VPN, Direct Connect\n- Load Balancer\n- Extra Resources 17/09/2025 17/09/2025 YouTube - AWS Security 5 - Hands-on: + VPC + Subnet\n+ Route Table\n+ IGW\n+ EBS\n+ \u0026hellip;\n- Remote SSH into EC2\n- Learn Elastic IP 18/09/2025 18/09/2025 AWS Study Group - 000003 6 - Hands-on: + Set up Hybrid DNS with Route 53 Resolver 19/09/2025 19/09/2025 AWS Study Group - 000010 7 - Hands-on: + Set up VPC Peering 19/09/2025 19/09/2025 AWS Study Group - 000019 8 - Hands-on: + Set up AWS Transit Gateway 19/09/2025 19/09/2025 AWS Study Group - 000020 Week 2 Goals: Build a strong understanding of Amazon VPC, focusing on its key components: CIDR blocks, subnets, route tables, and ENIs. Learn how to secure VPCs using both Security Groups and Network ACLs, and understand their differences in scope and use cases. Explore AWS networking services such as VPN and Direct Connect to understand options for hybrid and enterprise connectivity. Study Elastic Load Balancing and its role in distributing traffic for high availability. Gain hands-on practice with VPC essentials: creating subnets, configuring route tables, attaching internet gateways, working with EBS, and managing Elastic IPs. Strengthen skills in accessing and managing EC2 instances securely via SSH. Implement advanced networking scenarios, including Hybrid DNS resolution with Route 53 Resolver. Practice connecting multiple VPCs through VPC Peering. Deploy AWS Transit Gateway to design and manage scalable multi-VPC architectures. Direction: The goal of Week 2 was not only to expand theoretical knowledge of AWS networking but also to translate that knowledge into practical experience, laying a foundation for building and managing complex cloud infrastructures.\n"
},
{
	"uri": "http://localhost:1313/learning-aws/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "http://localhost:1313/learning-aws/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "http://localhost:1313/learning-aws/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console Select the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor Navigate to the VPC menu by using the Search box at the top of the browser window. Click on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes. Click Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "http://localhost:1313/learning-aws/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section, you need to summarize the contents of the workshop that you plan to conduct.\nIoT Weather Platform for Lab Research A Unified AWS Serverless Solution for Real-Time Weather Monitoring 1. Executive Summary The IoT Weather Platform is designed for the ITea Lab team in Ho Chi Minh City to enhance weather data collection and analysis. It supports up to 5 weather stations, with potential scalability to 10-15, utilizing Raspberry Pi edge devices with ESP32 sensors to transmit data via MQTT. The platform leverages AWS Serverless services to deliver real-time monitoring, predictive analytics, and cost efficiency, with access restricted to 5 lab members via Amazon Cognito.\n2. Problem Statement What’s the Problem? Current weather stations require manual data collection, becoming unmanageable with multiple units. There is no centralized system for real-time data or analytics, and third-party platforms are costly and overly complex.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data, AWS Lambda and API Gateway for processing, Amazon S3 for storage (including a data lake), and AWS Glue Crawlers and ETL jobs to extract, transform, and load data from the S3 data lake to another S3 bucket for analysis. AWS Amplify with Next.js provides the web interface, and Amazon Cognito ensures secure access. Similar to Thingsboard and CoreIoT, users can register new devices and manage connections, though this platform operates on a smaller scale and is designed for private use. Key features include real-time dashboards, trend analysis, and low operational costs.\nBenefits and Return on Investment The solution establishes a foundational resource for lab members to develop a larger IoT platform, serving as a study resource, and provides a data foundation for AI enthusiasts for model training or analysis. It reduces manual reporting for each station via a centralized platform, simplifying management and maintenance, and improves data reliability. Monthly costs are $0.66 USD per the AWS Pricing Calculator, with a 12-month total of $7.92 USD. All IoT equipment costs are covered by the existing weather station setup, eliminating additional development expenses. The break-even period of 6-12 months is achieved through significant time savings from reduced manual work.\n3. Solution Architecture The platform employs a serverless AWS architecture to manage data from 5 Raspberry Pi-based stations, scalable to 15. Data is ingested via AWS IoT Core, stored in an S3 data lake, and processed by AWS Glue Crawlers and ETL jobs to transform and load it into another S3 bucket for analysis. Lambda and API Gateway handle additional processing, while Amplify with Next.js hosts the dashboard, secured by Cognito. The architecture is detailed below:\nAWS Services Used AWS IoT Core: Ingests MQTT data from 5 stations, scalable to 15. AWS Lambda: Processes data and triggers Glue jobs (two functions). Amazon API Gateway: Facilitates web app communication. Amazon S3: Stores raw data in a data lake and processed outputs (two buckets). AWS Glue: Crawlers catalog data, and ETL jobs transform and load it. AWS Amplify: Hosts the Next.js web interface. Amazon Cognito: Secures access for lab users. Component Design Edge Devices: Raspberry Pi collects and filters sensor data, sending it to IoT Core. Data Ingestion: AWS IoT Core receives MQTT messages from the edge devices. Data Storage: Raw data is stored in an S3 data lake; processed data is stored in another S3 bucket. Data Processing: AWS Glue Crawlers catalog the data, and ETL jobs transform it for analysis. Web Interface: AWS Amplify hosts a Next.js app for real-time dashboards and analytics. User Management: Amazon Cognito manages user access, allowing up to 5 active accounts. 4. Technical Implementation Implementation Phases This project has two parts—setting up weather edge stations and building the weather platform—each following 4 phases:\nBuild Theory and Draw Architecture: Research Raspberry Pi setup with ESP32 sensors and design the AWS serverless architecture (1 month pre-internship) Calculate Price and Check Practicality: Use AWS Pricing Calculator to estimate costs and adjust if needed (Month 1). Fix Architecture for Cost or Solution Fit: Tweak the design (e.g., optimize Lambda with Next.js) to stay cost-effective and usable (Month 2). Develop, Test, and Deploy: Code the Raspberry Pi setup, AWS services with CDK/SDK, and Next.js app, then test and release to production (Months 2-3). Technical Requirements\nWeather Edge Station: Sensors (temperature, humidity, rainfall, wind speed), a microcontroller (ESP32), and a Raspberry Pi as the edge device. Raspberry Pi runs Raspbian, handles Docker for filtering, and sends 1 MB/day per station via MQTT over Wi-Fi. Weather Platform: Practical knowledge of AWS Amplify (hosting Next.js), Lambda (minimal use due to Next.js), AWS Glue (ETL), S3 (two buckets), IoT Core (gateway and rules), and Cognito (5 users). Use AWS CDK/SDK to code interactions (e.g., IoT Core rules to S3). Next.js reduces Lambda workload for the fullstack web app. 5. Timeline \u0026amp; Milestones Project Timeline\nPre-Internship (Month 0): 1 month for planning and old station review. Internship (Months 1-3): 3 months. Month 1: Study AWS and upgrade hardware. Month 2: Design and adjust architecture. Month 3: Implement, test, and launch. Post-Launch: Up to 1 year for research. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services: AWS Lambda: $0.00/month (1,000 requests, 512 MB storage). S3 Standard: $0.15/month (6 GB, 2,100 requests, 1 GB scanned). Data Transfer: $0.02/month (1 GB inbound, 1 GB outbound). AWS Amplify: $0.35/month (256 MB, 500 ms requests). Amazon API Gateway: $0.01/month (2,000 requests). AWS Glue ETL Jobs: $0.02/month (2 DPUs). AWS Glue Crawlers: $0.07/month (1 crawler). MQTT (IoT Core): $0.08/month (5 devices, 45,000 messages). Total: $0.7/month, $8.40/12 months\nHardware: $265 one-time (Raspberry Pi 5 and sensors). 7. Risk Assessment Risk Matrix Network Outages: Medium impact, medium probability. Sensor Failures: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies Network: Local storage on Raspberry Pi with Docker. Sensors: Regular checks and spares. Cost: AWS budget alerts and optimization. Contingency Plans Revert to manual methods if AWS fails. Use CloudFormation for cost-related rollbacks. 8. Expected Outcomes Technical Improvements: Real-time data and analytics replace manual processes.\nScalable to 10-15 stations.\nLong-term Value 1-year data foundation for AI research.\nReusable for future projects.\n"
},
{
	"uri": "http://localhost:1313/learning-aws/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/learning-aws/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/learning-aws/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/learning-aws/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint. In Create endpoint console: Name the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "http://localhost:1313/learning-aws/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "http://localhost:1313/learning-aws/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. First cloud journey Lab for indepth understanding of Session manager. In the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "http://localhost:1313/learning-aws/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "http://localhost:1313/learning-aws/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "http://localhost:1313/learning-aws/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints. Click the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use. Connect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box Click Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $ Change to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo; Create a file named testfile2.xyz fallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "http://localhost:1313/learning-aws/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/learning-aws/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "http://localhost:1313/learning-aws/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "http://localhost:1313/learning-aws/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "http://localhost:1313/learning-aws/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "http://localhost:1313/learning-aws/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "http://localhost:1313/learning-aws/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "http://localhost:1313/learning-aws/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "http://localhost:1313/learning-aws/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/learning-aws/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]