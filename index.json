[{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"Introduction This workshop guides you through deploying a full-stack DNA Analysis application on AWS. The application allows users to analyze DNA sequences, manage results, and visualize biological data.\nApplication Architecture Frontend (React + Vite) Framework: React 18 with TypeScript UI Libraries: Material-UI, TailwindCSS, Recharts State Management: React Context API Routing: React Router v6 Form Handling: React Hook Form with Zod validation HTTP Client: Axios Hosting: S3 + CloudFront CDN Backend (Spring Boot) Framework: Spring Boot 3.x Language: Java 17 Database: MySQL 8.0 with Spring Data JPA Security: Spring Security with JWT authentication API: RESTful API with proper error handling Hosting: EC2 instances with Auto Scaling Database (RDS MySQL) Engine: MySQL 8.0.40 Instance: db.t3.micro (scalable) Storage: 20GB gp3 with encryption Backup: Automated backups with 3-7 days retention High Availability: Multi-AZ deployment (optional) AWS Architecture Network Layer VPC (10.0.0.0/16)\r├── Public Subnets (10.0.1.0/24, 10.0.3.0/24)\r│ ├── Internet Gateway\r│ ├── NAT Gateway\r│ └── Application Load Balancer\r│\r└── Private Subnets (10.0.2.0/24, 10.0.4.0/24)\r├── EC2 Instances (Auto Scaling Group)\r├── RDS MySQL (Multi-AZ)\r└── VPC Endpoints (S3, CloudWatch, SSM, Cognito) Application Flow User Browser\r│\r├─── HTTPS ──\u0026gt; CloudFront ──\u0026gt; S3 (Static Frontend)\r│\r└─── HTTPS ──\u0026gt; API Gateway ──\u0026gt; ALB ──\u0026gt; EC2 (Backend API)\r│\r└──\u0026gt; RDS MySQL Security Architecture Internet\r│\r├─── CloudFront (HTTPS only)\r│ └─── S3 Bucket Policy (CloudFront OAI)\r│\r└─── API Gateway (Resource Policy)\r└─── ALB Security Group (Port 80/443)\r└─── EC2 Security Group (Port 8080 from ALB only)\r└─── RDS Security Group (Port 3306 from EC2 only) Key Features 1. User Authentication User registration and login JWT token-based authentication AWS Cognito integration (optional) Session management 2. DNA Analysis Upload and analyze DNA sequences Support multiple file formats Batch processing capability Store analysis results 3. Data Visualization DNA analysis charts Dashboard with metrics Export results in multiple formats 4. User Management User profile management Analysis history Role-based access control Infrastructure as Code CloudFormation Template The infrastructure.yaml template includes:\nNetworking (Lines 1-400)\nVPC with DNS support 2 Public Subnets (Multi-AZ) 2 Private Subnets (Multi-AZ) Internet Gateway NAT Gateway (can be disabled for cost savings) Route Tables VPC Endpoints (S3, CloudWatch, SSM, Cognito) Compute (Lines 400-700)\nLaunch Template with User Data script Auto Scaling Group (1-4 instances) Application Load Balancer Target Group with health checks Scaling Policies (CPU-based) Storage \u0026amp; CDN (Lines 700-900)\nS3 Bucket for Frontend S3 Bucket Policy CloudFront Distribution CloudFront Origin Access Identity Database (Lines 900-1000)\nRDS MySQL Instance DB Subnet Group Automated Backups Encryption at rest Security (Lines 1000-1200)\nSecurity Groups (ALB, EC2, RDS, VPC Endpoints) IAM Roles (EC2, CloudWatch, S3) IAM Instance Profile Cognito User Pool (optional) Secrets Manager (optional) Monitoring (Lines 1200-1393)\nCloudWatch Log Groups CloudWatch Alarms (CPU, Memory) SNS Topic for alerts API Gateway with CORS Cost Optimization 1. VPC Endpoints instead of NAT Gateway Savings: ~$20-25/month\nS3 Gateway Endpoint: FREE Interface Endpoints: $7.20/endpoint/month Total: ~$28/month vs NAT Gateway $32/month + data transfer 2. Instance Sizing Development: t3.micro ($7-10/month) Production: t3.small or t3.medium\n3. RDS Optimization Single-AZ for development Multi-AZ for production Automated backups with appropriate retention 4. CloudFront Caching Reduce requests to S3 Lower latency for users Free tier: 1TB data transfer/month Best Practices Applied 1. Security ✅ Private subnets for EC2 and RDS ✅ Security Groups with least privilege ✅ IAM Roles instead of hardcoded credentials ✅ Encryption at rest and in transit ✅ VPC Endpoints for private connectivity ✅ CloudTrail for audit logging (optional)\n2. High Availability ✅ Multi-AZ deployment ✅ Auto Scaling Group ✅ Application Load Balancer ✅ RDS automated backups ✅ CloudFront global CDN\n3. Monitoring \u0026amp; Logging ✅ CloudWatch Logs for application logs ✅ CloudWatch Alarms for metrics ✅ SNS notifications ✅ Health checks on ALB and ASG\n4. Automation ✅ Infrastructure as Code with CloudFormation ✅ User Data scripts for EC2 initialization ✅ Systemd service for application management ✅ Automated deployments with scripts\nDeployment Steps Preparation (10 minutes)\nInstall AWS CLI Create EC2 Key Pair Configure parameters Deploy Infrastructure (15-20 minutes)\nValidate CloudFormation template Create stack Wait for resources to be created Deploy Backend (20-30 minutes)\nBuild JAR file Upload to S3 Deploy to EC2 Configure database connection Deploy Frontend (10-15 minutes)\nBuild React application Upload to S3 Invalidate CloudFront cache Testing (15-30 minutes)\nTest authentication Test DNA analysis features Verify monitoring Cleanup (5-10 minutes)\nDelete CloudFormation stack Verify all resources deleted Expected Outcomes After completing this workshop, you will have:\n✅ A working full-stack application on AWS ✅ Deep understanding of AWS networking and security ✅ Experience with Infrastructure as Code ✅ Knowledge of cost optimization ✅ Best practices for production deployment\nReference Resources AWS CloudFormation Documentation AWS VPC Best Practices AWS Well-Architected Framework Spring Boot on AWS React Deployment Best Practices "},{"uri":"https://Fatnotfatt.github.io/learning-aws/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Summary Report: \u0026ldquo;Building Agentic AI: Context Optimization with Amazon Bedrock\u0026rdquo; Event Objectives Introduce Building Agentic AI and Context Optimization with Amazon Bedrock Build autonomous AI agents with Amazon Bedrock through hands-on techniques Share real-world use cases for agentic workflows Introduce CloudThinker and Agentic Orchestration solution Provide hands-on workshop with real AWS environments Connect with AWS experts and AI practitioners Event Details Date: Friday, December 5, 2025 Time: 9:00 AM – 12:00 PM (Check-in opens at 8:15 AM) Location: 26th Floor, Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh Duration: 3 hours (including tea break, networking, and lunch buffet) Agenda 9:00 – 9:10 AM | Opening (10 minutes) Nguyen Gia Hung, Head of Solutions Architect, AWS\nWelcome and event introduction Overview of Building Agentic AI Context Optimization with Amazon Bedrock 9:10 – 9:40 AM | AWS Bedrock Agent Core (30 minutes) Kien Nguyen, Solutions Architect, AWS\nIntroduction to AWS Bedrock Agent Architecture and core components How Bedrock Agent works Context management and optimization Integration with other AWS services Demo: Create and configure basic Bedrock Agent 9:40 – 10:00 AM | [Use Case] Building Agentic Workflow on AWS (20 minutes) Viet Pham, Founder \u0026amp; CEO, Diaflow\nReal-world use case for building agentic workflow Architecture and best practices Challenges and solutions Context optimization in production Demo: Agentic workflow in practice 10:00 – 10:10 AM | CloudThinker Introduction (10 minutes) Thang Ton, Co-founder \u0026amp; COO, CloudThinker\nIntroduction to CloudThinker Agentic Orchestration solution Vision and roadmap Integration with Amazon Bedrock 10:10 – 10:40 AM | CloudThinker Agentic Orchestration, Context Optimization on Amazon Bedrock (L300) (30 minutes) Henry Bui, Head of Engineering, CloudThinker\nAgentic orchestration patterns Advanced context optimization techniques Integration with Amazon Bedrock Advanced use cases and best practices Performance optimization Cost optimization strategies Demo: CloudThinker platform 10:40 – 11:00 AM | Tea Break \u0026amp; Networking (20 minutes) Break time Networking with experts and participants Informal Q\u0026amp;A Complimentary refreshments 11:00 AM – 12:00 PM | CloudThinker Hack: Hands-on Workshop (60 minutes) Kha Van, Community Leader, AWS\nHands-on workshop with real AWS environments Build Bedrock Agent from scratch Practice context optimization Agentic orchestration patterns Troubleshooting and best practices Q\u0026amp;A and direct support 12:00 PM Onwards | Networking \u0026amp; Lunch Buffet Extended networking Lunch buffet Meet the experts Share experiences and learnings Meet Our Experts Nguyen Gia Hung Head of Solutions Architect, AWS\nLeading expert in AWS architecture and solutions Extensive experience in consulting and implementing cloud solutions Leader of AWS programs in Vietnam Kien Nguyen Solutions Architect, AWS\nExpert in AWS Bedrock and AI services Experience in building AI/ML solutions Expert in agentic AI and context optimization Viet Pham Founder \u0026amp; CEO, Diaflow\nEntrepreneur with experience in AI and cloud computing Expert in agentic workflows and automation Founder of Diaflow - AI workflow solution Kha Van Community Leader, AWS\nAWS Community Leader Expert in hands-on training and workshops Mentor for AWS community Thang Ton Co-Founder \u0026amp; COO, CloudThinker\nCo-founder of CloudThinker Expert in cloud orchestration and automation Expert in agentic orchestration platforms Henry Bui Head of Engineering, CloudThinker\nExpert in agentic orchestration Experience in context and performance optimization Expert in L300 technical deep-dive sessions Key Highlights Building Agentic AI Introduction to Agentic AI:\nWhat Agentic AI is and why it matters Autonomous AI agents vs traditional AI Use cases and applications Future of AI with agentic systems Context Optimization:\nWhy context optimization is important Techniques to optimize context Cost reduction strategies Performance improvement Best practices AWS Bedrock Agent Core Bedrock Agent Architecture:\nAgent: Main entity that performs tasks Knowledge Base: Information source for the agent Action Groups: Actions the agent can perform Orchestration: Flow and context management Context Management: Context optimization Key Features:\nNatural language understanding Context management and optimization Multi-step reasoning Integration with AWS services Custom actions and workflows Agentic Workflow Use Case Building Agentic Workflow:\nDesigning complex workflows with multiple agents Orchestration and coordination between agents Context optimization in workflows Error handling and retry logic Monitoring and observability Best Practices:\nDesign patterns for agentic workflows Context management strategies Performance optimization Cost optimization Security and compliance CloudThinker Agentic Orchestration Agentic Orchestration Patterns:\nSequential workflows Parallel execution Conditional branching Error recovery and fallback Context sharing between agents Context Optimization Techniques:\nContext compression and summarization Relevant information extraction Memory management Cost optimization strategies Performance tuning Integration with Amazon Bedrock:\nSeamless integration with Bedrock models Custom model selection Prompt engineering and optimization Response formatting and validation Context optimization APIs Key Takeaways Understanding Building Agentic AI What Agentic AI is: Autonomous AI agents capable of performing tasks independently Context Optimization: Techniques to optimize context and reduce costs Use cases: Real-world use cases for agentic AI Architecture: Architecture and design patterns AWS Bedrock Agent Bedrock Agent Core: Understanding how agents work and interact Context Management: Managing and optimizing context Integration: How to integrate with other AWS services Best Practices: Best practices from AWS experts Agentic Workflow Design Workflow patterns: Common patterns for agentic workflows Orchestration: How to manage and coordinate multiple agents Context Optimization: Strategies for context optimization Error handling: Strategies for error handling and recovery CloudThinker Platform Agentic orchestration: How CloudThinker addresses orchestration challenges Context optimization: Advanced techniques to optimize context Platform capabilities: Features and capabilities of CloudThinker Integration patterns: How to integrate CloudThinker into existing systems Hands-on Experience Practical skills: Practical skills in building Bedrock Agents Context optimization: Practicing context optimization techniques Troubleshooting: How to debug and troubleshoot common issues Real-world scenarios: Working with real-world scenarios Applying to Work Build Agentic AI: Use AWS Bedrock Agent to build autonomous AI agents Context Optimization: Apply context optimization techniques to reduce costs and improve performance Design Workflows: Apply agentic workflow patterns to projects Integrate CloudThinker: Evaluate and integrate CloudThinker into existing solutions Best Practices: Apply best practices from the workshop to production systems Event Experience Attending the \u0026ldquo;Building Agentic AI: Context Optimization with Amazon Bedrock\u0026rdquo; workshop was an intensive learning experience about agentic AI and context optimization. The event provided both theoretical knowledge and hands-on practice, giving me a clear understanding of how to build and optimize autonomous AI agents.\nOpening and Introduction Opening session by Nguyen Gia Hung created a professional and inspiring atmosphere. I understood the importance of Building Agentic AI and Context Optimization. Overview of event agenda helped me visualize the learning journey. AWS Bedrock Agent Core Session by Kien Nguyen provided a solid foundation about Bedrock Agent. I learned about architecture, components, and how Bedrock Agent works. Context management was a key highlight in this session. Demo of creating Bedrock Agent showed the actual process from start to finish. Real-World Use Case Use case presentation by Viet Pham illustrated how agentic workflows are used in production. Learning about real-world challenges and how to solve them. Context optimization in production was very practical and insightful. Agentic workflow demo showed performance and capabilities in practice. CloudThinker Platform CloudThinker introduction by Thang Ton introduced the platform and solution. L300 session by Henry Bui went deep into technical details and advanced patterns. Learning about advanced context optimization techniques to improve performance and reduce costs. CloudThinker platform demo showed capabilities and ease of use. Hands-on Workshop Hands-on workshop by Kha Van provided direct practice opportunities. Building Bedrock Agent from scratch with guidance from an expert. Practicing context optimization and agentic orchestration. Troubleshooting session helped me understand how to solve common issues. Direct Q\u0026amp;A provided answers to specific questions. Networking and Connections Networking sessions allowed connections with AWS experts and AI practitioners. Sharing experiences and learnings with other participants. Lunch buffet created opportunities for informal discussions and connections. Meeting experts and receiving advice about career development. Lessons Learned Agentic AI is the future: Autonomous AI agents will change how we build AI applications. Context Optimization is key: Optimizing context can significantly reduce costs and improve performance. Hands-on practice is essential: Practical experience is crucial to truly understand and apply concepts. Platform solutions matter: CloudThinker and similar platforms simplify building agentic systems. Community is valuable: Networking with experts and practitioners provides valuable insights and opportunities. Some event photos Overall, this workshop provided me with comprehensive knowledge about Building Agentic AI and Context Optimization with Amazon Bedrock. The combination of theory, real-world use cases, and hands-on practice gave me confidence to start building autonomous AI agents on AWS. Particularly, the context optimization and hands-on workshop sections provided practical skills that can be applied immediately to work. This workshop is essential for anyone wanting to understand agentic AI in depth and how to optimize context to reduce costs and improve performance.\nWhat\u0026rsquo;s Included: ✓ Technical deep-dive sessions (L300)\n✓ Live demos and use case presentations\n✓ Hands-on workshop with real AWS environments\n✓ Networking with AWS experts and AI practitioners\n✓ Complimentary refreshments and lunch buffet\nIMPORTANT: Please bring your laptop to participate in the hands-on exercises\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Well-Architected Security Pillar\u0026rdquo; Event Objectives Introduce AWS Well-Architected Framework Security Pillar Demonstrate 5 main Security pillars: IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response Share best practices and core principles about cloud security Provide practical knowledge about threats and prevention in Vietnam Guide building security architecture according to AWS Well-Architected standards Connect with security experts and cloud practitioners Event Details Date: Saturday, November 29, 2025 Time: 8:30 AM – 12:00 PM Location: AWS Vietnam Office, Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh Duration: 3.5 hours (including coffee break) Agenda 8:30 – 8:50 AM | Opening \u0026amp; Security Foundation (20 minutes) Role of Security Pillar in Well-Architected Framework Core Principles: Least Privilege: Grant minimum necessary permissions Zero Trust: Never trust, always verify Defense in Depth: Multiple layers of protection Shared Responsibility Model: AWS and customer responsibilities Top threats in cloud environment in Vietnam Q\u0026amp;A ⭐ Pillar 1 — Identity \u0026amp; Access Management 8:50 – 9:30 AM | Modern IAM Architecture (40 minutes) IAM Fundamentals:\nUsers, Roles, Policies – avoid long-term credentials Best practices for IAM setup Temporary credentials and session management IAM Identity Center:\nSingle Sign-On (SSO) configuration Permission sets and assignment Multi-account management Advanced IAM:\nService Control Policies (SCP) for multi-account Permission boundaries to limit permissions MFA (Multi-Factor Authentication) requirements Credential rotation strategies Access Analyzer to detect external access Mini Demo: Validate IAM Policy + simulate access\nCheck policy syntax and permissions Simulate access scenarios Troubleshoot common IAM issues ⭐ Pillar 2 — Detection 9:30 – 9:55 AM | Detection \u0026amp; Continuous Monitoring (25 minutes) AWS Security Services:\nCloudTrail: Organization-level logging and audit GuardDuty: Threat detection and intelligent security Security Hub: Centralized security findings Comprehensive Logging:\nVPC Flow Logs: Network traffic monitoring ALB Access Logs: Application layer monitoring S3 Access Logs: Object access tracking Logging at every layer of infrastructure Alerting \u0026amp; Automation:\nEventBridge rules for security events Automated response workflows Integration with notification systems Detection-as-Code:\nInfrastructure as Code for security rules Version control for detection rules Automated deployment and testing 9:55 – 10:10 AM | Coffee Break (15 minutes) Break time Networking with participants Informal Q\u0026amp;A ⭐ Pillar 3 — Infrastructure Protection 10:10 – 10:40 AM | Network \u0026amp; Workload Security (30 minutes) Network Security:\nVPC Segmentation: Separate network segments Private vs Public placement strategies Network isolation and security zones Security Groups vs NACLs:\nWhen to use Security Groups When to use NACLs Practical application models Best practices and common mistakes Advanced Network Protection:\nAWS WAF: Web Application Firewall AWS Shield: DDoS protection Network Firewall: Managed network firewall service Workload Protection:\nEC2 Security: Instance hardening, patch management ECS Security: Container security best practices EKS Security: Kubernetes security fundamentals Security baselines and compliance ⭐ Pillar 4 — Data Protection 10:40 – 11:10 AM | Encryption, Keys \u0026amp; Secrets (30 minutes) AWS KMS (Key Management Service):\nKey policies and access control Grants and delegation Key rotation strategies Multi-region key management Encryption at Rest:\nS3: Server-side encryption (SSE-S3, SSE-KMS, SSE-C) EBS: Volume encryption and snapshots RDS: Database encryption DynamoDB: Table encryption Encryption in Transit:\nTLS/SSL best practices Certificate management End-to-end encryption Secrets Management:\nSecrets Manager: Automated rotation patterns Parameter Store: Secure parameter storage Rotation patterns and best practices Integration with applications Data Classification \u0026amp; Access Guardrails:\nData classification frameworks Access controls based on classification Compliance and regulatory requirements ⭐ Pillar 5 — Incident Response 11:10 – 11:40 AM | IR Playbook \u0026amp; Automation (30 minutes) IR Lifecycle according to AWS:\nPrepare: Preparation and planning Detect: Incident detection Respond: Response and containment Recover: Recovery and lessons learned IR Playbooks for Common Scenarios:\n1. Compromised IAM Key:\nDetect compromised credentials Immediate response steps Key rotation and access revocation Investigation and forensics 2. S3 Public Exposure:\nDetect public buckets Immediate remediation Access review and audit Prevention strategies 3. EC2 Malware Detection:\nDetect malware and suspicious activity Isolation procedures Evidence collection Cleanup and recovery Automated Response:\nLambda functions for automated response Step Functions for complex workflows Integration with security services Playbook automation patterns Evidence Collection:\nSnapshot creation for forensics Log preservation Chain of custody Compliance with legal requirements 11:40 AM – 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A (20 minutes) Summary of 5 Security pillars Common pitfalls and frequent mistakes Vietnamese enterprise reality: Security challenges in Vietnam Compliance requirements Best practices for local context Security learning roadmap: AWS Certified Security – Specialty AWS Certified Solutions Architect – Professional Security training paths Q\u0026amp;A session Commemorative photos Key Highlights Security Foundation Principles Least Privilege:\nGrant only minimum necessary permissions to perform work Regular review and audit permissions Use temporary credentials instead of long-term keys Principle of least privilege at every layer Zero Trust:\nNever trust by default, always verify Verify identity and authorization for every request Network segmentation and micro-segmentation Continuous verification and monitoring Defense in Depth:\nMultiple layers of protection: Network, Application, Data, Identity Don\u0026rsquo;t rely on a single layer of protection Layered security controls Fail-safe defaults Shared Responsibility Model:\nAWS: Security OF the cloud (infrastructure) Customer: Security IN the cloud (data, applications, configurations) Understand responsibilities of each party Best practices for customer responsibilities Pillar 1: Identity \u0026amp; Access Management Modern IAM Architecture:\nUse IAM Roles instead of Users when possible Temporary credentials with STS IAM Identity Center for SSO Permission boundaries and SCPs Best Practices:\nEnable MFA for all users Regular credential rotation Use Access Analyzer to detect external access Least privilege policies Regular access reviews Pillar 2: Detection Comprehensive Monitoring:\nCloudTrail for audit trail GuardDuty for threat detection Security Hub for centralized view VPC Flow Logs for network monitoring Detection-as-Code:\nVersion control for detection rules Automated testing CI/CD for security rules Infrastructure as Code approach Pillar 3: Infrastructure Protection Network Security:\nVPC segmentation Security Groups and NACLs WAF, Shield, Network Firewall Private subnets and NAT gateways Workload Security:\nEC2 hardening Container security Kubernetes security Patch management Pillar 4: Data Protection Encryption:\nEncryption at rest with KMS Encryption in transit with TLS Key management best practices Secrets management Data Classification:\nClassify data by sensitivity Apply appropriate controls Access guardrails Compliance requirements Pillar 5: Incident Response IR Lifecycle:\nPrepare: Planning and tools Detect: Monitoring and alerting Respond: Containment and investigation Recover: Restoration and lessons learned Automation:\nAutomated response with Lambda Step Functions for workflows Integration with security services Playbook automation Key Takeaways Security Foundation Well-Architected Framework: Understanding Security Pillar and its role Core Principles: Least Privilege, Zero Trust, Defense in Depth Shared Responsibility: AWS and customer responsibilities Threat Landscape: Top threats in Vietnam and prevention methods IAM Best Practices Modern IAM: Use roles, temporary credentials IAM Identity Center: SSO and permission management Advanced Features: SCPs, permission boundaries, Access Analyzer Security: MFA, credential rotation, least privilege Detection \u0026amp; Monitoring Security Services: CloudTrail, GuardDuty, Security Hub Comprehensive Logging: VPC Flow Logs, ALB logs, S3 logs Alerting: EventBridge and automation Detection-as-Code: Infrastructure as Code for security Infrastructure Protection Network Security: VPC segmentation, Security Groups, NACLs Advanced Protection: WAF, Shield, Network Firewall Workload Security: EC2, ECS, EKS security Best Practices: Hardening and patch management Data Protection Encryption: At rest and in transit KMS: Key management and rotation Secrets Management: Secrets Manager and Parameter Store Data Classification: Access guardrails and compliance Incident Response IR Lifecycle: Prepare, Detect, Respond, Recover Playbooks: Common scenarios and response procedures Automation: Lambda and Step Functions Forensics: Evidence collection and preservation Applying to Work Design Security Architecture: Apply 5 pillars to architecture design Implement IAM Best Practices: Use modern IAM patterns Setup Detection: Deploy comprehensive monitoring Protect Infrastructure: Apply network and workload security Protect Data: Implement encryption and secrets management Prepare IR: Build incident response playbooks and automation Security Reviews: Regular security assessments and improvements Event Experience Attending the \u0026ldquo;AWS Well-Architected Security Pillar\u0026rdquo; workshop was an intensive learning experience about cloud security. The event provided comprehensive knowledge about the 5 Security pillars and practical best practices, helping me understand how to build secure cloud architecture.\nSecurity Foundation Opening session introduced Security Pillar in Well-Architected Framework. I learned about core principles: Least Privilege, Zero Trust, Defense in Depth. Shared Responsibility Model helped me understand AWS and customer responsibilities. Top threats in Vietnam provided real-world context for security challenges. Modern IAM Architecture IAM session went deep into modern IAM patterns and best practices. Learning about IAM Identity Center for SSO and multi-account management. Advanced features like SCPs and permission boundaries were very useful. Demo validate IAM policy showed practical approach to test policies. Detection \u0026amp; Continuous Monitoring Detection session covered comprehensive monitoring strategy. CloudTrail, GuardDuty, and Security Hub form a powerful security monitoring stack. Logging at every layer helped me understand defense in depth. Detection-as-Code approach was very innovative and practical. Network \u0026amp; Workload Security Infrastructure Protection session went deep into network security. Clear understanding of when to use Security Groups vs NACLs. Advanced protection with WAF, Shield, Network Firewall. Workload security for EC2, ECS, EKS provided practical guidance. Data Protection Data Protection session covered encryption and secrets management. KMS key management and rotation strategies are very important. Encryption at rest and in transit for all services. Secrets Manager patterns for automated rotation. Incident Response IR session provided practical playbooks for common scenarios. Learning about IR lifecycle and best practices. Automated response with Lambda and Step Functions is very powerful. Evidence collection procedures for forensics and compliance. Wrap-up and Q\u0026amp;A Wrap-up session summarized 5 pillars comprehensively. Common pitfalls helped me avoid frequent mistakes. Vietnamese enterprise reality provided local context. Learning roadmap for security certifications was very useful. Lessons Learned Security is foundation: Must design security from the start, not add-on later. Defense in Depth: Don\u0026rsquo;t rely on a single layer of protection. Automation is key: Automated detection and response reduce response time. Continuous improvement: Security is an ongoing process, not a one-time setup. Compliance matters: Understand regulatory requirements and best practices. Practice makes perfect: Need regular practice and review. Some event photos Overall, this workshop provided me with comprehensive knowledge about AWS Well-Architected Security Pillar. The combination of theory, best practices, and practical demos gave me a solid foundation to build secure cloud architecture. Particularly, the incident response and automation sections provided practical skills that can be applied immediately to work. This workshop is essential for anyone wanting to understand cloud security on AWS in depth.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.11-week11/","title":"Worklog Week 11","tags":[],"description":"","content":"Week 11 Objectives: Set up CI/CD Pipeline: Connect GitLab repository to AWS CodePipeline for automated deployments. Configure AWS CodeBuild for frontend and backend builds with automatic S3 upload and CloudFront invalidation. Implement SSH-less deployment for backend using AWS Systems Manager or CodeDeploy. Set up comprehensive monitoring with CloudWatch logs, metrics, and enhanced monitoring for EC2 and RDS. Configure AWS CloudTrail for audit logging and security compliance. Set up SNS Alerts with CloudWatch alarms for critical metrics (EC2 CPU, RDS connections, API 5xx errors). Perform end-to-end testing and create final project documentation with complete architecture diagram. Tasks for the Week: Day Task Start Date Completion Date Reference 19 - GitLab to CodePipeline Integration: + Create GitLab repository for the project (if not already created). + Set up AWS CodePipeline with source stage connected to GitLab repository. + Configure webhook or polling for automatic pipeline triggers on code commits. + Create S3 bucket for pipeline artifacts storage. + Test pipeline trigger by making a test commit to GitLab repository. + Verify CodePipeline can successfully connect to GitLab and retrieve source code. 16/11/2025 16/11/2025 CodePipeline documentation 20 - CodeBuild for Frontend: + Create CodeBuild project for frontend build process. + Configure buildspec.yml file for frontend build steps (install dependencies, build assets, optimize). + Set up CodeBuild environment with appropriate Docker image (Node.js, npm, etc.). + Configure build output to upload built files to S3 bucket (FE Bucket). + Set up automatic CloudFront invalidation after S3 upload (invalidate cache for updated files). + Test frontend build process and verify files are uploaded to S3 and CloudFront cache is invalidated. 17/11/2025 17/11/2025 CodeBuild documentation 21 - CodeBuild for Backend \u0026amp; SSH-less Deployment: + Create CodeBuild project for backend build process. + Configure buildspec.yml for backend build steps (compile, test, package artifacts). + Set up CodeBuild environment for backend (Java/Python/Node.js based on application). + Configure artifact upload to S3 or CodeDeploy. + Implement SSH-less deployment using AWS Systems Manager (SSM) or CodeDeploy: - Option 1: Use SSM Run Command to deploy to EC2 instances without SSH. - Option 2: Use CodeDeploy to deploy application to Auto Scaling Group. + Test backend build and deployment process end-to-end. 18/11/2025 18/11/2025 CodeDeploy / SSM documentation 22 - CloudWatch Logs \u0026amp; Metrics Setup: + Create CloudWatch log groups for EC2 application logs. + Configure CloudWatch agent on EC2 instances to send logs and custom metrics. + Set up CloudWatch metrics for EC2: CPU utilization, memory, disk I/O, network. + Enable RDS Enhanced Monitoring for detailed database metrics. + Configure API Gateway access logs to CloudWatch Logs. + Create CloudWatch dashboards for monitoring application health and performance. + Configure log retention policies for cost optimization. 19/11/2025 19/11/2025 CloudWatch documentation 23 - CloudTrail \u0026amp; Audit Dashboard: + Enable AWS CloudTrail for API call logging across all AWS services. + Create CloudTrail trail with S3 bucket for log storage. + Configure CloudTrail log file validation and encryption. + Set up CloudWatch Logs integration for CloudTrail events (optional). + Create CloudWatch dashboard for audit and security monitoring. + Review CloudTrail logs to verify API call logging is working correctly. + Document CloudTrail configuration and log retention policies. 20/11/2025 20/11/2025 CloudTrail documentation 24 - SNS Alerts \u0026amp; CloudWatch Alarms: + Create SNS topic for alarm notifications. + Subscribe email/SMS endpoints to SNS topic. + Create CloudWatch alarm for EC2 CPU utilization (threshold: \u0026gt;80% for 5 minutes). + Create CloudWatch alarm for RDS database connections (threshold: \u0026gt;80% of max connections). + Create CloudWatch alarm for API Gateway 5xx errors (threshold: \u0026gt;10 errors in 5 minutes). + Configure alarm actions to send notifications via SNS. + Test alarms by triggering conditions and verify email/SMS notifications are received. 21/11/2025 21/11/2025 CloudWatch Alarms \u0026amp; SNS 25 - End-to-End Testing \u0026amp; Final Documentation: + Perform comprehensive end-to-end testing: Users → Route 53 → CloudFront → WAF → API Gateway → EC2 → RDS. + Test CI/CD pipeline with code changes: verify automated frontend and backend deployments. + Test monitoring and alerting: trigger alarms and verify SNS notifications are received. + Test security: verify IAM permissions, Cognito authentication, Secrets Manager access, WAF protection. + Test scalability: verify Auto Scaling Group responds to load changes. + Create final architecture diagram with all components, data flows, and resource relationships. + Write comprehensive project documentation: deployment procedures, troubleshooting guides, runbooks, and architecture overview. + Prepare Worklog summary for all 4 weeks (Week 8-11). - Week 11 Summary: Complete AWS web application architecture deployed with CI/CD, monitoring, security, and automation. Project ready for production use. 22/11/2025 22/11/2025 Project documentation Achievements in Week 11: Successfully set up CI/CD Pipeline:\nConnected GitLab repository to AWS CodePipeline for automated deployments. Configured automatic pipeline triggers on code commits (webhook or polling). Created S3 bucket for pipeline artifacts storage. Verified end-to-end pipeline connectivity and source code retrieval. Configured CodeBuild for Frontend:\nCreated CodeBuild project with buildspec.yml for frontend build automation. Configured build environment with appropriate Docker image and dependencies. Set up automatic S3 upload of built frontend files. Implemented automatic CloudFront cache invalidation after deployments. Verified frontend build and deployment process works correctly. Implemented CodeBuild for Backend with SSH-less Deployment:\nCreated CodeBuild project for backend build automation. Configured buildspec.yml for backend compilation, testing, and packaging. Implemented SSH-less deployment using AWS Systems Manager (SSM) or CodeDeploy. Eliminated need for SSH keys and improved security posture. Verified backend build and deployment process works end-to-end. Set up comprehensive CloudWatch Monitoring:\nCreated CloudWatch log groups for EC2 application logs. Configured CloudWatch agent on EC2 instances for logs and custom metrics. Set up CloudWatch metrics for EC2 (CPU, memory, disk, network). Enabled RDS Enhanced Monitoring for detailed database insights. Configured API Gateway access logs to CloudWatch Logs. Created CloudWatch dashboards for real-time monitoring. Configured log retention policies for cost optimization. Configured CloudTrail for Audit and Compliance:\nEnabled CloudTrail for comprehensive API call logging. Created CloudTrail trail with S3 bucket for secure log storage. Configured log file validation and encryption. Set up CloudWatch dashboard for audit monitoring. Established audit trail for security and compliance requirements. Implemented SNS Alerts and CloudWatch Alarms:\nCreated SNS topic for alarm notifications with email/SMS subscriptions. Created CloudWatch alarm for EC2 CPU utilization monitoring. Created CloudWatch alarm for RDS database connection monitoring. Created CloudWatch alarm for API Gateway 5xx error detection. Configured alarm actions to send notifications via SNS. Tested alarms and verified notification delivery. Performed comprehensive end-to-end testing:\nVerified complete application flow: Users → Route 53 → CloudFront → WAF → API Gateway → EC2 → RDS. Tested CI/CD pipeline with code changes and verified automated deployments. Tested monitoring and alerting: triggered alarms and verified SNS notifications. Tested security: verified IAM permissions, Cognito authentication, Secrets Manager, WAF protection. Tested scalability: verified Auto Scaling Group responds to load changes. Created final project documentation:\nCreated comprehensive architecture diagram with all components, data flows, and resource relationships. Documented deployment procedures, troubleshooting guides, and runbooks. Prepared architecture overview and system design documentation. Completed Worklog summary for all 4 weeks (Week 8-11). After Week 11, the complete AWS web application architecture is fully deployed, monitored, secured, and automated:\nEdge Layer: Route 53, CloudFront, AWS WAF, ACM Certificate, S3 (Frontend). Networking Layer: VPC, public/private subnets, Internet Gateway, NAT Gateway, Security Groups, VPC Flow Logs. Compute \u0026amp; Database Layer: EC2 (with Auto Scaling), RDS, API Gateway, Cognito. CI/CD Pipeline: GitLab, CodePipeline, CodeBuild (Frontend \u0026amp; Backend), SSH-less deployment. Monitoring \u0026amp; Security: CloudWatch (Logs, Metrics, Dashboards, Alarms), CloudTrail, SNS Alerts, IAM, Secrets Manager. The project demonstrates a production-ready, scalable, secure, and well-monitored AWS architecture following best practices with complete automation and observability.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.10-week10/","title":"Worklog Week 10","tags":[],"description":"","content":"Week 10 Objectives: Deploy Backend Layer: EC2 instances in private subnet with application runtime and Auto Scaling configuration. Set up Amazon RDS database in private subnet with proper configuration and parameter groups. Deploy backend application and establish connectivity between EC2 and RDS using Secrets Manager. Configure API Gateway REST API with integration to EC2 backend. Integrate Amazon Cognito User Pool with API Gateway for authentication and authorization. Configure Auto Scaling Group for EC2 instances with Launch Template for scalability. Tasks for the Week: Day Task Start Date Completion Date Reference 13 - RDS Database Setup: + Create RDS subnet group spanning private subnet (10.0.2.0/24). + Launch RDS instance (MySQL/PostgreSQL) in private subnet with appropriate instance class. + Configure RDS parameter group with database-specific settings (character set, timezone, etc.). + Set up automated backups, encryption at rest, and Multi-AZ deployment (optional for cost optimization). + Configure RDS security group to allow connections only from EC2 Security Group. + Store initial database credentials in AWS Secrets Manager. 09/11/2025 09/11/2025 RDS documentation 14 - EC2 Backend Instance Setup: + Launch EC2 instance in private subnet (10.0.2.0/24) with appropriate instance type. + Install application runtime environment: Java/Python/Node.js based on application requirements. + Install and configure application dependencies and libraries. + Configure EC2 instance with IAM role (created in Week 9) for AWS service access. + Create base AMI from configured EC2 instance for Auto Scaling Group (to be used on Day 18). + Document EC2 configuration and application setup steps. 10/11/2025 10/11/2025 EC2 documentation 15 - Backend Application Deployment: + Deploy backend application code to EC2 instance (manual deployment for initial setup). + Configure application to connect to RDS database using credentials from Secrets Manager. + Test database connectivity from EC2 instance (verify connection string, credentials retrieval). + Configure application environment variables and configuration files. + Test basic application functionality and database operations (CRUD operations). + Document deployment process and application configuration. 11/11/2025 11/11/2025 Application deployment guide 16 - API Gateway REST API Configuration: + Create REST API in API Gateway with appropriate name and description. + Define API resources and methods (GET, POST, PUT, DELETE) based on application requirements. + Configure API Gateway integration with EC2 backend (HTTP/HTTPS integration or VPC Link for private resources). + Set up API Gateway VPC Link to connect to private subnet resources (EC2). + Enable CORS for frontend access (configure CORS headers: Access-Control-Allow-Origin, etc.). + Test API endpoints and verify integration with EC2 backend. 12/11/2025 12/11/2025 API Gateway documentation 17 - Amazon Cognito Integration: + Create Cognito User Pool for user authentication with appropriate name. + Configure user pool settings: password policies (minimum length, complexity), MFA (optional), email verification. + Create Cognito User Pool App Client for application integration. + Configure Cognito Authorizer in API Gateway for authenticated API access. + Test user registration flow: create test user in Cognito User Pool. + Test login flow: authenticate user and obtain JWT tokens. + Test authenticated API access: use JWT token to access protected API endpoints. 13/11/2025 13/11/2025 Cognito documentation 18 - Auto Scaling Group Configuration: + Create Launch Template based on base AMI created on Day 14. + Configure Launch Template with instance type, security groups, IAM role, and user data scripts. + Create Auto Scaling Group with Launch Template in private subnet. + Configure Auto Scaling policies: target tracking (CPU utilization, network in/out), step scaling, or scheduled scaling. + Set minimum, desired, and maximum capacity for Auto Scaling Group. + Test scale-out: trigger scaling by increasing load (or manually adjust desired capacity). + Test scale-in: reduce load and verify instances are terminated automatically. - Week 10 Summary: Backend and database layer complete, ready for CI/CD and monitoring setup in Week 11. 14/11/2025 14/11/2025 Auto Scaling documentation Achievements in Week 10: Successfully deployed Amazon RDS database:\nCreated RDS subnet group in private subnet for database isolation. Launched RDS instance (MySQL/PostgreSQL) with appropriate instance class and configuration. Configured RDS parameter group with database-specific settings. Set up automated backups, encryption at rest, and monitoring. Configured RDS security group to allow connections only from EC2 Security Group. Stored database credentials securely in AWS Secrets Manager. Set up EC2 backend infrastructure:\nLaunched EC2 instance in private subnet with appropriate instance type. Installed and configured application runtime environment (Java/Python/Node.js). Configured EC2 instance with IAM role for AWS service access. Created base AMI from configured EC2 instance for Auto Scaling Group. Documented EC2 configuration and application setup procedures. Deployed backend application:\nDeployed backend application code to EC2 instance. Configured application to connect to RDS using credentials from Secrets Manager. Tested database connectivity and verified connection functionality. Tested basic application functionality and database operations (CRUD). Documented deployment process and application configuration. Configured API Gateway REST API:\nCreated REST API with resources, methods, and integration points. Set up API Gateway VPC Link to connect to private subnet resources (EC2). Configured API Gateway integration with EC2 backend using HTTP/HTTPS. Enabled CORS for frontend access with proper headers. Tested API endpoints and verified integration with EC2 backend. Integrated Amazon Cognito for authentication:\nCreated Cognito User Pool with password policies, MFA, and email verification. Created Cognito User Pool App Client for application integration. Configured Cognito Authorizer in API Gateway for authenticated API access. Tested user registration, login, and authenticated API access flows. Established secure user authentication and authorization. Configured Auto Scaling Group for scalability:\nCreated Launch Template based on base AMI for consistent instance configuration. Created Auto Scaling Group with Launch Template in private subnet. Configured Auto Scaling policies (target tracking, step scaling) for automatic scaling. Set appropriate capacity limits (minimum, desired, maximum). Tested scale-out and scale-in functionality to verify automatic scaling. After Week 10, the backend and database layer is operational with scalable infrastructure. The application is ready for CI/CD automation and comprehensive monitoring in Week 11.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.9-week9/","title":"Worklog Week 9","tags":[],"description":"","content":"Week 9 Objectives: Build VPC and Networking Core: Create VPC with public and private subnets, Internet Gateway, and NAT Gateway. Establish secure network boundaries with proper routing and subnet segmentation. Configure Security Groups for EC2, RDS, and ALB following least-privilege principles. Set up IAM roles and policies for EC2 instances with custom permissions. Enable VPC Flow Logs for network traffic monitoring and auditing. Tasks for the Week: Day Task Start Date Completion Date Reference 7 - VPC Creation \u0026amp; Subnet Configuration: + Create VPC with CIDR block 10.0.0.0/16 in selected AWS region. + Create public subnet (10.0.1.0/24) in one Availability Zone with appropriate tags. + Create private subnet (10.0.2.0/24) in the same Availability Zone with appropriate tags. + Apply consistent tagging strategy (Name, Environment, Project, etc.) to all resources. + Document subnet allocation and IP addressing scheme. 02/11/2025 02/11/2025 AWS VPC documentation 8 - Internet Gateway Setup: + Create and attach Internet Gateway to VPC. + Configure public subnet route table to route internet-bound traffic (0.0.0.0/0) to Internet Gateway. + Verify public subnet route table configuration. + Test internet connectivity from public subnet (launch test EC2 instance if needed). + Document routing configuration and gateway associations. 03/11/2025 03/11/2025 Internet Gateway guide 9 - NAT Gateway Configuration: + Allocate Elastic IP address for NAT Gateway. + Create NAT Gateway in public subnet (10.0.1.0/24). + Configure private subnet route table to route internet-bound traffic (0.0.0.0/0) through NAT Gateway. + Verify private subnet route table configuration. + Test outbound internet connectivity from private subnet (launch test EC2 instance in private subnet). + Verify private subnet instances can reach internet while remaining isolated from inbound connections. 04/11/2025 04/11/2025 NAT Gateway documentation 10 - Security Groups Design \u0026amp; Implementation: + Create Security Group for EC2 instances: allow inbound from API Gateway/ALB, outbound to RDS and internet via NAT. + Create Security Group for RDS: allow inbound only from EC2 Security Group on database port (3306/5432). + Create Security Group for ALB (if used): allow inbound HTTP/HTTPS from internet, outbound to EC2 Security Group. + Apply least-privilege principle: grant minimum necessary permissions. + Document security group rules and relationships. 05/11/2025 05/11/2025 Security Groups guide 11 - IAM Roles \u0026amp; Policies for EC2: + Create IAM role for EC2 instances with descriptive name. + Create custom IAM policy for EC2 to access required AWS services (S3, Secrets Manager, CloudWatch, etc.). + Attach IAM role to EC2 instance profile. + Test IAM role permissions from EC2 instance (use AWS CLI or SDK). + Verify EC2 can access Secrets Manager to retrieve database credentials. + Document IAM roles and their permissions. 06/11/2025 06/11/2025 IAM best practices 12 - Network ACLs \u0026amp; VPC Flow Logs: + Review and configure Network ACLs (optional, default ACLs are usually sufficient). + Test Network ACL rules if custom rules are implemented. + Enable VPC Flow Logs to capture IP traffic flow information. + Configure Flow Logs destination (CloudWatch Logs or S3 bucket). + Review Flow Logs to audit network traffic patterns. + Audit and document all network configurations for security review. - Week 9 Summary: VPC and networking core complete, ready for backend and database deployment in Week 10. 07/11/2025 07/11/2025 VPC Flow Logs documentation Achievements in Week 9: Successfully created VPC and subnet infrastructure:\nCreated VPC with CIDR block 10.0.0.0/16 in selected AWS region. Configured public subnet (10.0.1.0/24) for internet-facing resources with proper tagging. Configured private subnet (10.0.2.0/24) for application servers with proper tagging. Applied consistent tagging strategy across all network resources for better management. Set up Internet Gateway for public subnet connectivity:\nCreated and attached Internet Gateway to VPC. Configured public subnet route table to route internet traffic (0.0.0.0/0) to Internet Gateway. Verified public subnet instances can access internet directly. Documented routing configuration and gateway associations. Configured NAT Gateway for private subnet outbound access:\nAllocated Elastic IP address and created NAT Gateway in public subnet. Configured private subnet route table to route internet traffic through NAT Gateway. Verified private subnet instances can reach internet for outbound connections (updates, downloads, API calls). Confirmed private subnet remains isolated from inbound internet connections (security best practice). Implemented Security Groups following least-privilege principles:\nCreated Security Group for EC2: allows inbound from API Gateway/ALB, outbound to RDS and internet. Created Security Group for RDS: allows inbound only from EC2 Security Group on database port. Created Security Group for ALB (if used): allows inbound HTTP/HTTPS, outbound to EC2. Applied least-privilege principle: granted minimum necessary permissions for each component. Documented security group rules and relationships for maintainability. Configured IAM roles and policies for EC2:\nCreated IAM role for EC2 instances with descriptive naming. Created custom IAM policy for EC2 to access required AWS services (S3, Secrets Manager, CloudWatch). Attached IAM role to EC2 instance profile. Tested IAM permissions from EC2 instance and verified access to Secrets Manager. Documented IAM roles and permissions for security audit. Enabled VPC Flow Logs for network monitoring:\nEnabled VPC Flow Logs to capture IP traffic flow information. Configured Flow Logs destination (CloudWatch Logs or S3 bucket). Reviewed Flow Logs to audit network traffic patterns and identify anomalies. Audited all network configurations for security compliance. After Week 9, the VPC and networking core infrastructure is complete with secure network boundaries, proper routing, and monitoring capabilities. The system is ready for backend and database deployment in Week 10.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.8-week8/","title":"Worklog Week 8","tags":[],"description":"","content":"Week 8 Objectives: Complete the Edge Layer and Frontend Storage: Route 53, S3, CloudFront, AWS WAF, and ACM Certificate. Set up DNS management with Route 53 hosted zone and domain configuration. Configure S3 bucket for static frontend hosting with proper access policies. Deploy CloudFront distribution for global content delivery with Origin Access Control. Implement AWS WAF protection with security rules (SQL injection, XSS, bot control). Set up ACM Certificate and enable HTTPS for secure content delivery. Tasks for the Week: Day Task Start Date Completion Date Reference 1 - System Requirements Analysis \u0026amp; Architecture Design: + Analyze system requirements and review the complete architecture diagram. + Identify all components: Route 53, S3, CloudFront, WAF, ACM, VPC, EC2, RDS, API Gateway. + Create High-Level Design (HLD) document with architecture overview. + Document data flow: Users → Route 53 → CloudFront → WAF → S3 (Frontend). + Plan IP addressing scheme and resource naming conventions. 26/10/2025 26/10/2025 Architecture diagram 2 - Route 53 Setup: + Create Route 53 hosted zone for domain management. + Create A record pointing to CloudFront distribution (planned for Day 4). + Create CNAME records for subdomains if needed. + Configure DNS settings and verify domain ownership. + Document DNS configuration and record types. 27/10/2025 27/10/2025 Route 53 documentation 3 - S3 Frontend Bucket Configuration: + Create S3 bucket for frontend static assets (FE Bucket) with appropriate naming. + Enable static website hosting on S3 bucket. + Configure public access policy for CloudFront access (block public access, allow CloudFront via OAC). + Upload test frontend files (HTML, CSS, JS, images) to S3 bucket. + Test static website hosting endpoint and verify file accessibility. 28/10/2025 28/10/2025 S3 documentation 4 - CloudFront Distribution Setup: + Create CloudFront distribution with S3 bucket as origin. + Configure Origin Access Control (OAC) for secure S3 access (replacing OAI). + Set up cache policies (CachingOptimized, CachingDisabled, etc.). + Configure default root object (index.html). + Map Route 53 domain to CloudFront distribution (update A record from Day 2). + Test CloudFront distribution and verify content delivery. 29/10/2025 29/10/2025 CloudFront documentation 5 - AWS WAF Integration: + Create AWS WAF WebACL for CloudFront protection. + Add managed rules: AWS Managed Rules for SQL injection protection. + Add managed rules: AWS Managed Rules for XSS (Cross-Site Scripting) protection. + Configure bot control rules to block common bots and scrapers. + Associate WAF WebACL with CloudFront distribution. + Test WAF rules by attempting common attack patterns and verify blocking. 30/10/2025 30/10/2025 AWS WAF documentation 6 - ACM Certificate \u0026amp; HTTPS Configuration: + Request ACM Certificate in us-east-1 region (required for CloudFront). + Validate certificate using DNS validation method (add CNAME records to Route 53). + Wait for certificate validation and issuance. + Bind ACM certificate to CloudFront distribution. + Configure CloudFront to use HTTPS only (redirect HTTP to HTTPS). + Test HTTPS connection and verify SSL/TLS certificate is working correctly. - Week 8 Summary: Edge layer and frontend storage complete, ready for VPC and networking setup in Week 9. 31/10/2025 31/10/2025 ACM documentation Achievements in Week 8: Successfully completed system analysis and architecture design:\nAnalyzed system requirements and reviewed complete architecture diagram. Created High-Level Design (HLD) document with architecture overview and component relationships. Documented data flow from users through edge services to frontend storage. Established resource naming conventions and planning documentation. Set up Route 53 DNS management:\nCreated Route 53 hosted zone for domain management. Configured A and CNAME records for domain routing. Established DNS foundation for connecting domain to CloudFront distribution. Configured S3 for static frontend hosting:\nCreated S3 bucket for frontend static assets with proper naming conventions. Enabled static website hosting on S3 bucket. Configured public access policies: blocked public access, allowed CloudFront access via Origin Access Control. Uploaded test frontend files and verified static website hosting functionality. Deployed CloudFront distribution:\nCreated CloudFront distribution with S3 bucket as origin. Configured Origin Access Control (OAC) for secure S3 access (modern replacement for OAI). Set up cache policies for optimized content delivery. Mapped Route 53 domain to CloudFront distribution. Verified content delivery through CloudFront CDN globally. Implemented AWS WAF protection:\nCreated AWS WAF WebACL with comprehensive security rules. Added AWS Managed Rules for SQL injection protection. Added AWS Managed Rules for XSS (Cross-Site Scripting) protection. Configured bot control rules to block malicious bots and scrapers. Associated WAF WebACL with CloudFront distribution. Tested WAF rules and verified protection against common attack patterns. Enabled HTTPS with ACM Certificate:\nRequested and validated ACM Certificate in us-east-1 region (required for CloudFront). Used DNS validation method with CNAME records in Route 53. Bound ACM certificate to CloudFront distribution. Configured CloudFront to enforce HTTPS (redirect HTTP to HTTPS). Verified SSL/TLS certificate is working correctly and secure connections are established. After Week 8, the edge layer and frontend storage are complete with secure, global content delivery. The system is ready for VPC and networking core setup in Week 9.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.7-week7/","title":"Worklog Week 7","tags":[],"description":"","content":"Week 7 Objectives: Understand Amazon DynamoDB as a fully managed NoSQL database service: key-value and document data models, partition keys, sort keys, global secondary indexes (GSI), and on-demand vs provisioned capacity modes. Learn how to build and manage Data Lakes on AWS using services like Amazon S3, AWS Glue, Amazon Athena, and Amazon QuickSight for analytics workloads. Explore AWS Analytics services: Amazon Athena for serverless SQL queries on S3, AWS Glue for ETL operations, and Amazon QuickSight for business intelligence and visualization. Practice data ingestion, transformation, and analysis workflows in the AWS cloud environment. Understand cost optimization and performance tuning strategies for analytics workloads on AWS. Tasks for the Week: Day Task Start Date Completion Date Reference 2 - Lab Practice: Data Lake on AWS. + Understand data lake architecture on AWS using S3 as the data lake storage layer. + Learn about data ingestion, cataloging, and querying patterns. + Explore integration between S3, Glue, and Athena for analytics. 20/10/2025 20/10/2025 https://000035.awsstudygroup.com/ 3 - Lab Practice: Amazon DynamoDB Immersion Day. + Deep dive into DynamoDB core concepts: tables, items, attributes, primary keys, and indexes. + Practice creating tables, inserting data, and querying with partition keys and sort keys. + Understand DynamoDB capacity modes (on-demand vs provisioned) and pricing models. 21/10/2025 21/10/2025 https://000039.awsstudygroup.com/ 4 - Lab Practice: Cost and performance analysis with AWS Glue and Amazon Athena. + Use AWS Glue to catalog data stored in S3 and create data catalogs. + Run SQL queries on S3 data using Amazon Athena. + Analyze cost implications and optimize query performance. + Understand partitioning strategies for cost-effective analytics. 22/10/2025 22/10/2025 https://000040.awsstudygroup.com/ 5 - Lab Practice: Work with Amazon DynamoDB. + Create DynamoDB tables with appropriate key schemas. + Perform CRUD operations (Create, Read, Update, Delete) on DynamoDB items. + Work with Global Secondary Indexes (GSI) and Local Secondary Indexes (LSI). + Practice querying and scanning operations, understanding the differences. 23/10/2025 23/10/2025 https://000060.awsstudygroup.com/ 6 - Lab Practice: Building a Datalake with Your Data. + Build a complete data lake solution using AWS services. + Implement data ingestion pipelines. + Set up data transformation workflows with AWS Glue. + Create analytics-ready datasets for downstream consumption. 24/10/2025 24/10/2025 https://000070.awsstudygroup.com/ 7 - Lab Practice: Analytics on AWS workshop. + Comprehensive workshop covering the full analytics stack on AWS. + Integrate multiple services: S3, Glue, Athena, and visualization tools. + Build end-to-end analytics solutions from raw data to insights. 25/10/2025 25/10/2025 https://000072.awsstudygroup.com/ 8 - Lab Practice: Get started with QuickSight. + Create visualizations and dashboards using Amazon QuickSight. + Connect QuickSight to various data sources (S3, Athena, RDS, etc.). + Build interactive reports and share insights with stakeholders. - Summarize and review all AWS Analytics and Data Lake Services covered in Week 7. 26/10/2025 26/10/2025 https://000073.awsstudygroup.com/ Achievements in Week 7: Gained comprehensive understanding of Amazon DynamoDB:\nDynamoDB as a fully managed, serverless NoSQL database service with single-digit millisecond latency. Key concepts: tables, items, attributes, primary keys (partition key + optional sort key), and data modeling best practices. Global Secondary Indexes (GSI) and Local Secondary Indexes (LSI) for flexible query patterns. Capacity modes: on-demand (pay-per-request) vs provisioned (reserved capacity) and when to use each. DynamoDB Streams for real-time data processing and change data capture. Built hands-on experience with Data Lake architecture on AWS:\nAmazon S3 as the foundation for data lake storage with lifecycle policies, versioning, and encryption. Data lake architecture patterns: raw zone, processed zone, and curated zone. Data ingestion strategies: batch uploads, streaming data, and integration with various data sources. Best practices for organizing data in S3: partitioning, naming conventions, and folder structures. Mastered AWS Analytics services:\nAWS Glue: Serverless ETL service for discovering, cataloging, and transforming data. Glue Data Catalog as a centralized metadata repository. Glue ETL jobs for data transformation using Apache Spark. Glue Crawlers for automatic schema discovery. Amazon Athena: Serverless interactive SQL query service for analyzing data in S3. Pay-per-query pricing model and cost optimization strategies. Integration with Glue Data Catalog for schema-on-read queries. Query performance optimization through partitioning and columnar formats (Parquet, ORC). Amazon QuickSight: Cloud-native business intelligence and visualization service. Creating dashboards, visualizations, and reports. Connecting to various data sources (S3, Athena, RDS, Redshift, etc.). Sharing insights with teams and embedding analytics in applications. Completed comprehensive lab practice sessions:\nData Lake on AWS (Lab 35): Built foundational understanding of data lake architecture and S3-based storage patterns. Amazon DynamoDB Immersion Day (Lab 39): Deep dive into DynamoDB operations, data modeling, and best practices. Cost and performance analysis with AWS Glue and Amazon Athena (Lab 40): Learned to optimize analytics workloads for cost and performance. Work with Amazon DynamoDB (Lab 60): Practiced CRUD operations, indexing strategies, and query patterns. Building a Datalake with Your Data (Lab 70): Implemented end-to-end data lake solution with ingestion and transformation pipelines. Analytics on AWS workshop (Lab 72): Comprehensive workshop integrating multiple analytics services. Get started with QuickSight (Lab 73): Created visualizations and dashboards for business intelligence. After Week 7, established a complete understanding of the AWS Analytics and Data Lake ecosystem:\nFrom NoSQL databases (DynamoDB) → Data Lake storage (S3) → ETL and cataloging (Glue) → Query and analysis (Athena) → Visualization (QuickSight), ready to design and implement modern analytics solutions on AWS. "},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.6-week6/","title":"Worklog Week 6","tags":[],"description":"","content":"Week 6 Objectives: Review fundamental Database Concepts: relational model, primary/foreign keys, ACID, normalization, OLTP vs OLAP. Understand Amazon RDS as a managed relational database service on AWS: engines, Multi-AZ, read replicas, backup, and scaling. Learn the benefits of Amazon Aurora compared to standard RDS engines: performance, high availability, automatic storage scaling, MySQL/PostgreSQL compatibility. Get familiar with Amazon Redshift as a petabyte-scale data warehouse for analytics, and distinguish it from RDS (OLTP workloads). Learn how Amazon ElastiCache (Redis / Memcached) provides an in-memory cache layer to reduce latency and offload backend databases. Practice Database Schema Conversion \u0026amp; Migration using AWS DMS and AWS Schema Conversion Tool (SCT) for moving databases to AWS. Tasks for the Week: Day Task Start Date Completion Date Reference 2 - Review Database Concepts (Module 06-01): relational model, ACID, transactions, indexing, normalization, OLTP vs OLAP. - Map traditional on-premises database concepts to AWS cloud services. 13/10/2025 13/10/2025 Class material – Module 06-01 3 - Study Amazon RDS \u0026amp; Amazon Aurora theory (Module 06-02). - Learn about supported engines, Multi-AZ, automated backups, snapshots, read replicas, and scaling. - Compare RDS vs Aurora in terms of performance, availability, and cost. 14/10/2025 14/10/2025 https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html, https://aws.amazon.com/rds/aurora/ 4 - Study Amazon Redshift \u0026amp; Amazon ElastiCache (Module 06-03). - Distinguish OLTP (RDS/Aurora) vs OLAP (Redshift) and in-memory cache layer (ElastiCache). - Explore common use cases: data warehouse \u0026amp; BI, caching sessions, leaderboard, rate limiting, etc. 15/10/2025 15/10/2025 https://aws.amazon.com/redshift/, https://aws.amazon.com/elasticache/ 5 - Lab Practice: Module 06-Lab 5 – Amazon Relational Database Service (Amazon RDS). + Create an RDS instance, configure security group, parameter group, backups. + Connect from a client, run queries, and test behavior (e.g., failover/Multi-AZ if available in the lab). 16/10/2025 16/10/2025 https://000005.awsstudygroup.com/ 6 - Lab Practice: Module 06-Lab 43 – Database Schema Conversion \u0026amp; Migration. + Use AWS Schema Conversion Tool (SCT) to analyze and convert schema from source DB to RDS/Aurora/Redshift target. + Use AWS Database Migration Service (DMS) to migrate data (full load and change data capture if supported in the lab). - Summarize and review all AWS Database Services covered in Week 6. 17/10/2025 17/10/2025 https://000043.awsstudygroup.com/ Achievements in Week 6: Consolidated understanding of core database concepts:\nRelational tables, primary/foreign keys, relational integrity, and basic indexing. ACID properties of transactions and why they matter in OLTP workloads. Clear distinction between OLTP vs OLAP and how this maps to AWS services. Gained hands-on familiarity with Amazon RDS:\nCreated and managed RDS instances via AWS Management Console. Reviewed supported engines (MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, Aurora) and their typical use cases. Practiced configuring Multi-AZ, automated backups, snapshots, monitoring, and basic scaling options. Understood the strengths of Amazon Aurora:\nAurora as a cloud-native, MySQL/PostgreSQL-compatible database with significantly improved performance over standard engines. Aurora DB cluster architecture, with a distributed storage layer across multiple AZs. Reader and writer endpoints, automatic storage scaling, and high availability design. Built a big-picture view of Amazon Redshift \u0026amp; ElastiCache:\nRedshift as a columnar, petabyte-scale data warehouse optimized for analytics and BI workloads. How Redshift differs from RDS/Aurora: optimized for complex queries over large datasets rather than transactional workloads. ElastiCache (Redis/Memcached) as a fully managed, low-latency in-memory cache layer to increase throughput and reduce load on backend databases. Completed the main labs of the week:\nModule 06-Lab 5 – Amazon RDS: Deployed an RDS instance, connected from a client, executed basic SQL queries. Observed the impact of configuration changes (instance class, storage, backup settings) on behavior and management. Module 06-Lab 43 – Database Schema Conversion \u0026amp; Migration: Used AWS Schema Conversion Tool (SCT) to assess and convert schemas, identifying what can be auto-converted vs. what needs manual adjustment. Used AWS Database Migration Service (DMS) to migrate data from a source database into RDS/Aurora/Redshift targets according to the lab scenario. After Week 6, established a clear mental model of the AWS database ecosystem:\nFrom traditional database concepts → RDS/Aurora for OLTP → Redshift for OLAP → ElastiCache for caching → DMS/SCT for migration, ready to apply in real-world architectures. "},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.5-week5/","title":"Worklog Week 5","tags":[],"description":"","content":"Week 5 Objectives This week, my main goal was to master the concepts and security services in AWS, including the shared responsibility model, access management, encryption, and resource protection.\nI also familiarized myself with various AWS tools and services to apply them in hands-on practice.\nSpecific objectives include:\nUnderstand the Shared Responsibility Model of AWS. Master key AWS security services: IAM, Cognito, Security Hub, KMS, Identity Center. Improve resource management and security through IAM Permissions Boundaries, Resource Tags, and encryption techniques. Weekly Tasks Day Task Start Date Completion Date Reference Source 2 - Study the theory of the Shared Responsibility Model and AWS security principles. - Review documentation for AWS security services: + Amazon IAM + Amazon Cognito + AWS Identity Center + AWS KMS + AWS Security Hub 04/10/2025 04/10/2025 AWS Study Group 3 - Hands-on: + Configure and use AWS Security Hub to monitor and detect security issues. + Create and manage IAM Users, Roles, and Policies for AWS accounts. + Create IAM Groups and manage access permissions for user groups. 05/10/2025 05/10/2025 AWS Study Group 4 - Hands-on: + Optimize EC2 costs using Lambda for automated start/stop of EC2 instances. + Manage EC2 access via Resource Tags using IAM. + Configure IAM Permission Boundaries to limit user privileges. + Encrypt data using AWS KMS. 06/10/2025 06/10/2025 AWS Study Group 5 - Advanced Practice: + Learn and apply security methods in AWS Organizations for multi-account management. + Enhance proficiency in AWS Identity Center for managing and synchronizing users and groups across AWS services. 07/10/2025 07/10/2025 AWS Study Group Results Achieved in Week 5 During this week, I achieved significant progress in understanding and applying AWS security services, effectively bridging theory with practice. Specifically:\nUnderstanding and Applying the AWS Shared Responsibility Model\nI fully grasped that AWS is responsible for the security of the cloud infrastructure, while users are responsible for securing their own data and applications. This clarified my role in ensuring compliance and protection when deploying services on AWS. Theoretical Knowledge of AWS Core Security Services\nAmazon IAM: Learned how to create and manage Users, Roles, and Policies to control user and group access. Amazon Cognito: Studied user management and authentication for AWS applications. AWS Identity Center: Understood how to link and manage user access across multiple AWS services. AWS Security Hub: Configured and utilized it to monitor and detect security threats. AWS KMS: Practiced encrypting data at rest and securing sensitive data using encryption keys. Practical Implementation of AWS Security Services\nSuccessfully installed and configured AWS Security Hub for continuous monitoring and vulnerability detection. Configured IAM Permissions Boundaries to restrict user privileges and prevent unauthorized access. EC2 Cost Optimization with Lambda: Automated the shutdown of unused EC2 instances to minimize operational costs. EC2 Access Control via IAM \u0026amp; Resource Tags: Applied IAM Policies that use Tags to precisely define access scope. Enhanced AWS Resource Management Skills\nCreated and managed IAM Groups and Policies, improving group-based access control. Learned how to manage multiple AWS accounts using AWS Organizations, ensuring consistent security policies across the organization. LAB PRACTICE Table of Contents Lab 18 Lab 22 VPC EC2 Slack Lambda + EventBridge Test Results Lab 27 Lab 28 Regions \u0026amp; EC2 Tags Lab 30 Lab 33 Lab 18 Illustration:\nLab 22 VPC VPC configuration illustrations:\nEC2 EC2 configuration illustrations:\nSlack Note (New UI): Re-select the channel during setup to get the correct Webhook URL.\nLambda + EventBridge Lambda and EventBridge configuration illustrations:\nTest Results Lab 27 Illustrations:\nLab 28 Illustrations:\nRegions \u0026amp; EC2 EC2 in ap-northeast-1 (Tokyo)\nEC2 in us-east-1 (North Virginia)\nTags Sample key/value pairs used:\nKey Value Name Example Team Beta Team Alpha Team TEST Tag interface examples:\nName = Example, Team = Beta\nName = Example, Team = Alpha\nTeam = TEST\nLab 30 Policies: IAM: Check Permission: Lab 33 Policies Role User KMS S3 CloudTrail Athena Test after applying ACLs Conclusion During Week 5, I significantly improved my ability to use AWS security and access management tools.\nThese knowledge and skills form a solid foundation for implementing advanced security solutions and cost optimization in future AWS projects.\nThe lab exercises reinforced theoretical understanding and enhanced my practical skills for working effectively in real AWS environments.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.4-week4/","title":"Worklog Week 4","tags":[],"description":"","content":"Week 4 Objectives Gain an in-depth understanding of AWS’s core storage service Amazon S3. Master the key concepts: bucket, object, storage class, access point, static website hosting, and CORS. Study hybrid storage and data migration solutions such as AWS Storage Gateway and AWS Snow Family. Get familiar with Amazon FSx for Windows File Server and the automated backup service AWS Backup. Practice deploying, managing, and integrating AWS storage services in a real-world environment. Tasks to Be Completed in Week 4 Day Task Start Date Completion Date References 2 - Study the theory of AWS Storage Service (S3) – Module 04-01.\n- Get familiar with the concepts of Bucket, Object, and the storage mechanism. 29/09/2025 29/09/2025 https://docs.aws.amazon.com/s3/ 3 - Learn about Access Point and Storage Class in S3 – Module 04-02.\n- Distinguish between storage classes: Standard, IA, Glacier, Deep Archive. 30/09/2025 30/09/2025 https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html 4 - Explore S3 Static Website \u0026amp; CORS, Access Control, Object Key, Performance, and Glacier – Module 04-03. 01/10/2025 01/10/2025 https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html 5 - Hands-on: Module 04-Lab13 – Deploy AWS Backup to the System.\n- Hands-on: Module 04-Lab14 – VM Import/Export. 02/10/2025 02/10/2025 Lab13, Lab14 6 - Hands-on: Module 04-Lab24 – Using File Storage Gateway.\n- Hands-on: Module 04-Lab25 – Amazon FSx for Windows File Server.\n- Review and consolidate all AWS storage services. 03/10/2025 03/10/2025 Lab24, Lab25 Results Achieved in Week 4 Gained a clear understanding of the architecture and operating principles of Amazon S3, including:\nHow to create and manage Buckets, Objects, and Access Policies. Different Storage Classes and strategies for optimizing storage costs. How to configure S3 Static Website Hosting and handle CORS for web applications. Became familiar with S3 Glacier – a cold storage service that helps save costs for infrequently accessed data.\nGained a solid understanding of Hybrid Storage \u0026amp; Data Migration through:\nAWS Snow Family (Snowcone, Snowball, Snowmobile). AWS Storage Gateway – a solution to connect on-premises systems with AWS Cloud. Successfully completed the following labs:\nLab 13 – AWS Backup Goal: Configure and deploy resource backups with AWS Backup. Step 1:\nStep 2:\nStep 3:\nStep 4:\nSuccess:\nLab 14 – VM Import/Export Goal: Perform VM Import/Export – migrate virtual machines between the local environment and AWS. Step 1:\nSuccess:\nStep 2:\nStep 3:\nStep 4:\nStep 5:\nStep 6 (successfully uploaded the VM to EC2 (AMIs)):\nStep 7:\nStep 8 (Internet connectivity test):\nStep 9:\nStep 10 (Done):\nLab 24 – File Storage Gateway Goal: Configure File Storage Gateway – create and link file storage between on-premises systems and AWS. Note: The account must be upgraded.\nStep 1 – After creating the S3 bucket, create the File Storage Gateway (FSG):\nStep 2 – EC2 settings:\nStep 3:\nLab 25 – Amazon FSx for Windows File Server Goal: Deploy a file storage system for Windows using Amazon FSx for Windows File Server. Step 1 (Lambda error – Node.js version):\nStep 2:\nCompleted the entire Module 04 – AWS Storage Services, building a solid foundation to move on to compute, database, and security services in the following weeks. "},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.3-week3/","title":"Worklog Week 3","tags":[],"description":"","content":"Week 3 Objectives: Gain a solid understanding of Amazon EC2 and its ecosystem (AMI, Backup, Key Pair, EBS, Instance Store, User Data, Metadata). Learn about EC2 Auto Scaling and its role in elasticity and cost optimization. Explore related compute services including EFS, FSx, Lightsail, and AWS MGN. Strengthen AWS storage knowledge with hands-on labs covering S3, AWS Backup, and Storage Gateway. Develop practical skills in configuring, managing, and scaling EC2 workloads. Tasks for this week: Day Task Start Date Completion Date Reference 1 - Theory: + EC2 overview: AMI, Backup, Key Pair 22/09/2025 22/09/2025 AWS EC2 Documentation 2 - Theory: + EBS (Elastic Block Store) + Instance Store 23/09/2025 23/09/2025 AWS EBS Documentation 3 - Theory: + EC2 User Data + EC2 Metadata 24/09/2025 24/09/2025 AWS EC2 User Guide 4 - Theory: + EC2 Auto Scaling + Related services: EFS, FSx, Lightsail, MGN 25/09/2025 25/09/2025 AWS Auto Scaling 5 - Hands-on: + Lab 57: Start with Amazon S3 26/09/2025 26/09/2025 AWS Study Group - Lab57 6 - Hands-on: + Lab 13: Deploy AWS Backup to the System 27/09/2025 27/09/2025 AWS Study Group - Lab13 7 - Hands-on: + Lab 24: Using AWS Storage Gateway 28/09/2025 28/09/2025 AWS Study Group - Lab24 Week 3 Achievements: Built strong theoretical knowledge of Amazon EC2, including:\nAMI and backup strategies for resilience. Key Pair usage for secure SSH authentication. Differences between EBS (persistent storage) and Instance Store (ephemeral storage). How User Data and Metadata scripts automate instance initialization and provide dynamic configuration. The role of EC2 Auto Scaling in maintaining performance and cost efficiency. Expanded understanding of related services:\nAmazon EFS and FSx for shared and high-performance file storage. Amazon Lightsail as a simplified alternative for small-scale workloads. AWS MGN for migrating workloads into AWS. Completed practical labs:\nLaunched and managed an S3 bucket (Lab57). Implemented AWS Backup to protect workloads (Lab13). Integrated on-premises systems with AWS using Storage Gateway (Lab24). Key skills acquired:\nConfidently distinguish storage types (EBS vs Instance Store vs EFS vs FSx). Automate EC2 lifecycle with User Data and Auto Scaling. Combine backup and hybrid storage solutions to create more resilient architectures. Direction:\nThis week provided a deeper dive into compute and storage fundamentals. By combining theoretical concepts with hands-on practice, I strengthened my ability to not only launch and manage EC2 instances but also design scalable, reliable, and cost-efficient architectures that integrate with AWS’s broader storage and migration services.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Get familiar with the First Cloud Journey (FCJ) team. Understand AWS core services and learn how to interact with them via Console \u0026amp; CLI. Tasks completed this week: Day Task Start Date Completion Date Reference 2 - Meet and get to know FCJ members.\n- Read and understand internship rules and guidelines. 2025-09-02 2025-09-02 3 - Study AWS and its main service groups:\n+ Compute + Storage + Networking + Database + \u0026hellip; 2025-09-03 2025-09-03 https://000001.awsstudygroup.com/en/ 4 - Register AWS Free Tier account.\n- Explore AWS Console \u0026amp; CLI.\n- Practice:\n+ Create AWS account + Install \u0026amp; configure AWS CLI + Basic CLI tasks 2025-09-04 2025-09-04 https://000001.awsstudygroup.com/en/ 5 - Configure basic security:\n+ Setup Virtual MFA Device + Create admin group \u0026amp; admin user + Account authentication support + Create Budget 2025-09-05 2025-09-06 https://000007.awsstudygroup.com/en/ 6 - Practice cost management:\n+ Create Cost Budget + Create Usage Budget + Reservation Instance (RI) + Savings Plans Budget 2025-09-06 2025-09-06 https://000007.awsstudygroup.com/en/ 7 - Submit AWS support request and manage responses.\n- Write worklog \u0026amp; self-assess AWS fundamentals.\n- Prepare for Week 2 goals. 2025-09-07 2025-09-07 https://000009.awsstudygroup.com/en/ Week 1 Outcomes: Onboarding completed:\nConnected with FCJ members. Understood internship guidelines and basic rules. AWS foundation knowledge:\nLearned what AWS is and its main service categories: Compute Storage Networking Database \u0026hellip; Account setup \u0026amp; configuration:\nSuccessfully registered AWS Free Tier account. Configured basic security: enabled MFA, created admin group \u0026amp; admin user. Created budgets to monitor costs: Cost Budget, Usage Budget, RI, Savings Plans. Management tools:\nPracticed AWS Management Console: navigating and using services via GUI. Installed and configured AWS CLI with: Access Key, Secret Key, default Region. Hands-on with AWS CLI:\nChecked account and configuration info. Listed regions. Viewed EC2 information. Created and managed key pairs. Monitored running services. Console \u0026amp; CLI integration:\nManaged AWS resources in parallel using Console and CLI. Compared approaches and gained insights on when to use each tool. Personal reflection:\nCompleted Week 1 worklog (submitted late in Week 2). Assessed current understanding and set targets for the upcoming week. "},{"uri":"https://Fatnotfatt.github.io/learning-aws/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Accelerating Enterprise ML Experimentation with Amazon SageMaker AI and Comet By: Vikesh Pandey, Naufal Mir and Sarah Ostermeier \u0026ndash; Date: September 22, 2025\nTopics: Amazon SageMaker AI, SageMaker Unified Studio, Partner solutions, Sarah Ostermeier\nAs organizations scale their machine learning (ML) operations from proof-of-concept to production, managing experiments, tracking model lineage, and ensuring reproducibility become significantly more complex. The main reason is that data scientists and ML engineers often experiment with numerous combinations of hyperparameters, model architectures, and dataset versions, generating large amounts of metadata that need to be tracked to ensure reproducibility and regulatory compliance. As ML models scale across multiple teams and regulatory requirements increase, experiment tracking becomes mandatory rather than just a \u0026ldquo;best practice\u0026rdquo;.\nAmazon SageMaker AI provides managed infrastructure for enterprises to scale ML workloads, handling compute provisioning, distributed training, and deployment without worrying about infrastructure. However, teams still need a robust experiment tracking system, model comparison capabilities, and collaboration beyond basic logging.\nComet is a comprehensive ML experiment management platform that automatically tracks, compares, and optimizes ML experiments throughout the model lifecycle. It provides data scientists and ML engineers with powerful tools for tracking, model monitoring, hyperparameter optimization, and collaborative model development. It also includes Opik — Comet\u0026rsquo;s open-source platform for LLM (large language model) observability and development.\nComet is available in SageMaker AI as a Partner AI App, as a fully managed capability for experimentation, with enterprise-grade security, seamless workflow integration, and a simple procurement process through AWS Marketplace.\nThis combination addresses the needs of end-to-end enterprise ML workflows: SageMaker AI handles infrastructure and compute, while Comet provides the experiment management, model registry, and production monitoring capabilities that teams need for regulatory compliance and operational efficiency. In this article, we demonstrate a complete fraud detection workflow using SageMaker AI + Comet, showcasing the reproducibility and audit-ready logging that modern enterprises require.\nMaking Comet \u0026ldquo;Enterprise-ready\u0026rdquo; on SageMaker AI Before diving into the deployment guide, organizations need to define their operating model and determine how to deploy Comet. AWS recommends setting up Comet following a federated operating model: Comet is centrally managed in a shared services account, and each ML data team has its own autonomous environment. Each operating model has its own pros and cons. (Refer to SageMaker Studio Administration Best Practices for details).\nIn this architecture, there are typically two roles:\nAdministrator \u0026ndash; responsible for setting up shared infrastructure and environments for use-case teams\nUser \u0026ndash; ML practitioners from use-case teams who use the established environment to solve business problems\nComet works well with both SageMaker AI and Amazon SageMaker (SageMaker AI uses the integrated environment in SageMaker Studio IDE; SageMaker uses Unified Studio IDE). Here, we use SageMaker Studio in the example.\nAdministrator Journey When a team wants to deploy a fraud detection use-case, the admin performs:\nComplete the prerequisite steps to set up Partner AI Apps \u0026mdash; grant permissions so Comet can assume the user\u0026rsquo;s SageMaker AI role and manage Comet subscription through AWS Marketplace.\\\nIn the SageMaker AI console, go to Applications and IDEs → Partner AI Apps → Comet to view details.\nDisplays subscription details, pricing model, and estimated Comet infrastructure costs. Select Go to Marketplace to subscribe to Comet from AWS Marketplace.\nSelect \u0026ldquo;View purchase options\u0026rdquo; and fill in the subscription information.\nAfter subscription is complete, the admin begins configuring Comet.\nWhen deploying Comet, add the fraud detection team\u0026rsquo;s project lead as an admin to manage the Comet dashboard. The Comet deployment process takes a few minutes. (Refer to the Partner AI App provisioning guide for details).\nSet up the SageMaker AI domain following the Use custom setup for Amazon SageMaker AI guide. As a best practice, provide a pre-signed domain URL so the use-case team can access the Comet UI without logging into the SageMaker console.\nAdd team members to the domain and enable Comet access when configuring the domain.\nAfter these steps, the SageMaker AI domain is ready for users to log in and begin working.\nUser Journey (ML Practitioner) When the environment is ready, the user performs:\nLog into the SageMaker AI domain via the pre-signed URL.\nAutomatically redirects to SageMaker Studio IDE, with username and IAM execution role pre-configured by the admin. Create a JupyterLab Space following the JupyterLab user guide.\nBegin the fraud detection use-case by launching a notebook.\nThe admin has already granted data access through the necessary S3 buckets. To use Comet\u0026rsquo;s API, install the comet_ml package and configure environment variables following the Set up Partner AI Apps SDKs guide.\nIn SageMaker Studio, select Partner AI Apps → Open Comet to access the Comet UI.\nBegin the experiment workflow. Solution Overview This use-case highlights common challenges in enterprises:\nImbalanced datasets (e.g., only ~0.17% of transactions are fraudulent)\nMultiple iteration rounds\nRequirements for complete reproducibility and audit compliance\nData \u0026amp; model lineage must be recorded in detail\nUsing the Credit Card Fraud Detection dataset, with binary labels \u0026mdash; 1 for fraud, 0 for legitimate. The following steps illustrate the key parts of the implementation (complete code is available in Comet\u0026rsquo;s GitHub repo).\nPrerequisites Configure the imports and Comet + SageMaker environment variables:\n# Comet ML for experiment tracking\nimport comet_ml\nfrom comet_ml import Experiment, API, Artifact\nfrom comet_ml.integration.sagemaker import log_sagemaker_training_job_v1\nAWS_PARTNER_APP_AUTH = True\nAWS_PARTNER_APP_ARN = \u0026lt;Your_AWS_PARTNER_APP_ARN\u0026gt;\nCOMET_API_KEY = \u0026lt;Your_Comet_API_Key\u0026gt;\nCOMET_WORKSPACE = \u0026lsquo;\u0026lt;your-comet-workspace-name\u0026gt;\u0026rsquo;\nCOMET_PROJECT_NAME = \u0026lsquo;\u0026lt;your-comet-project-name\u0026gt;\u0026rsquo;\nThe AWS_PARTNER_APP_ARN and COMET_API_KEY variables are obtained from the Comet details page in SageMaker.\nCOMET_WORKSPACE and COMET_PROJECT_NAME are the workspace and project names you will use to group experiments.\nPreparing the Dataset A key feature of Comet is automatic dataset versioning \u0026amp; lineage tracking. This enables full audit trails of which data was used to train each model \u0026mdash; crucial in regulated environments.\nExample:\n# Create dataset Artifact to track the original dataset\ndataset_artifact = Artifact(\nname=\u0026ldquo;fraud-dataset\u0026rdquo;,\nartifact_type=\u0026ldquo;dataset\u0026rdquo;,\naliases=[\u0026ldquo;raw\u0026rdquo;]\n)\ndataset_artifact.add_remote(s3_data_path, metadata={\n\u0026ldquo;dataset_stage\u0026rdquo;: \u0026ldquo;raw\u0026rdquo;,\n\u0026ldquo;dataset_split\u0026rdquo;: \u0026ldquo;not_split\u0026rdquo;,\n\u0026ldquo;preprocessing\u0026rdquo;: \u0026ldquo;none\u0026rdquo;\n})\nArtifact allows tagging of dataset files and associated metadata\nRaw data is added to the artifact for Comet to track the dataset source\nStarting a Comet Experiment After the artifact has been logged, you start an experiment and Comet will automatically record background metadata, environment, libraries, code, etc.\nexperiment_1 = comet_ml.Experiment(\nproject_name=COMET_PROJECT_NAME,\nworkspace=COMET_WORKSPACE,\n)\nexperiment_1.log_artifact(dataset_artifact)\nExperiment automatically begins recording information\nlog_artifact logs the dataset artifact to the experiment for traceability\nData Preprocessing Preprocessing steps include:\nRemoving duplicate records\nDropping unnecessary columns\nSplitting data into train/validation/test sets\nFeature standardization using scikit-learn\u0026rsquo;s StandardScaler\nThe preprocessing code is written in the preprocess.py file and run as a SageMaker Processing Job:\nprocessor = SKLearnProcessor(\nframework_version=\u0026lsquo;1.0-1\u0026rsquo;,\nrole=sagemaker.get_execution_role(),\ninstance_count=1,\ninstance_type=\u0026lsquo;ml.t3.medium\u0026rsquo;\n)\nprocessor.run(\ncode=\u0026lsquo;preprocess.py\u0026rsquo;,\ninputs=[ProcessingInput(source=s3_data_path, destination=\u0026rsquo;/opt/ml/processing/input\u0026rsquo;)],\noutputs=[ProcessingOutput(source=\u0026rsquo;/opt/ml/processing/output\u0026rsquo;, destination=f\u0026rsquo;s3://{bucket_name}/{processed_data_prefix}\u0026rsquo;)]\n)\nWhen the job starts, SageMaker AI creates an instance, processes the data, and then releases the resource.\nPreprocessing results are saved to S3.\nAfter completion, you create a new version of the dataset artifact to track the processed data:\npreprocessed_dataset_artifact = Artifact(\nname=\u0026ldquo;fraud-dataset\u0026rdquo;,\nartifact_type=\u0026ldquo;dataset\u0026rdquo;,\naliases=[\u0026ldquo;preprocessed\u0026rdquo;],\nmetadata={\n\u0026ldquo;description\u0026rdquo;: \u0026ldquo;Credit card fraud detection dataset\u0026rdquo;,\n\u0026ldquo;fraud_percentage\u0026rdquo;: f\u0026quot;{fraud_percentage:.3f}%\u0026quot;,\n\u0026ldquo;dataset_stage\u0026rdquo;: \u0026ldquo;preprocessed\u0026rdquo;,\n\u0026ldquo;preprocessing\u0026rdquo;: \u0026ldquo;StandardScaler + train/val/test split\u0026rdquo;,\n}\n)\npreprocessed_dataset_artifact.add_remote(\nuri=f\u0026rsquo;s3://{bucket_name}/{processed_data_prefix}',\nlogical_path=\u0026lsquo;split_data\u0026rsquo;\n)\nexperiment_1.log_artifact(preprocessed_dataset_artifact)\nArtifacts with the same name but different aliases allow Comet to manage versioning\nAdditional metadata helps document what was done (split, preprocessing\u0026hellip;)\nComet + SageMaker AI Experiment Workflow To accelerate rapid experimentation, you should organize the workflow into utility functions that can be called multiple times with different hyperparameters while ensuring consistent logging and evaluation.\nKey functions:\ntrain() \u0026mdash; creates an XGBoost training job on SageMaker:\nestimator = Estimator(\nimage_uri=xgboost_image,\nrole=execution_role,\ninstance_count=1,\ninstance_type=\u0026lsquo;ml.m5.large\u0026rsquo;,\noutput_path=model_output_path,\nsagemaker_session=sagemaker_session_obj,\nhyperparameters=hyperparameters_dict,\nmax_run=1800 # maximum time (seconds)\n)\nestimator.fit({\n\u0026rsquo;train\u0026rsquo;: train_channel,\n\u0026lsquo;validation\u0026rsquo;: val_channel\n})\nlog_training_job() \u0026mdash; logs training metadata to Comet and links the model: log_sagemaker_training_job_v1(\nestimator=training_estimator,\nexperiment=api_experiment\n)\nlog_model_to_comet() \u0026mdash; logs model artifact to Comet: experiment.log_remote_model(\nmodel_name=model_name,\nuri=model_artifact_path,\nmetadata=metadata\n)\ndeploy_and_evaluate_model() \u0026mdash; deploys endpoint and evaluates, logs metrics: predictor = estimator.deploy(initial_instance_count=1, instance_type=\u0026ldquo;ml.m5.xlarge\u0026rdquo;)\nexperiment.log_metrics(metrics)\nexperiment.log_confusion_matrix(matrix=cm, labels=[\u0026lsquo;Normal\u0026rsquo;, \u0026lsquo;Fraud\u0026rsquo;])\nfpr, tpr, _ = roc_curve(y_test, y_pred_prob_as_np_array)\nexperiment.log_curve(\u0026ldquo;roc_curve\u0026rdquo;, x=fpr, y=tpr)\nComplete prediction and evaluation code is available in the GitHub repo. Running the Experiments You can try multiple experiments by calling utility functions with different hyperparameter configurations and comparing results to select the optimal configuration for your use-case.\nFor example, the first experiment (baseline):\nhyperparameters_v1 = {\n\u0026lsquo;objective\u0026rsquo;: \u0026lsquo;binary:logistic\u0026rsquo;,\n\u0026rsquo;num_round\u0026rsquo;: 100,\n\u0026rsquo;eval_metric\u0026rsquo;: \u0026lsquo;auc\u0026rsquo;,\n\u0026rsquo;learning_rate\u0026rsquo;: 0.15,\n\u0026lsquo;booster\u0026rsquo;: \u0026lsquo;gbtree\u0026rsquo;\n}\nestimator_1 = train(\nmodel_output_path=f\u0026quot;s3://{bucket_name}/{model_output_prefix}/1\u0026quot;,\nexecution_role=role,\nsagemaker_session_obj=sagemaker_session,\nhyperparameters_dict=hyperparameters_v1,\ntrain_channel_loc=train_channel_location,\nval_channel_loc=validation_channel_location\n)\nlog_training_job(experiment_key = experiment_1.get_key(), training_estimator=estimator_1)\nlog_model_to_comet(\nexperiment = experiment_1,\nmodel_name=\u0026ldquo;fraud-detection-xgb-v1\u0026rdquo;,\nmodel_artifact_path=estimator_1.model_data,\nmetadata=metadata\n)\ndeploy_and_evaluate_model(\nexperiment=experiment_1,\nestimator=estimator_1,\nX_test_scaled=X_test_scaled,\ny_test=y_test\n)\nWhen running a Comet experiment from a Jupyter notebook, you need to call experiment_1.end() to ensure all information is recorded and saved to the Comet server.\nAfter the baseline experiment completes, you can launch the next experiment with different hyperparameters and compare the two experiments in the Comet UI.\nViewing Experiments in Comet UI To access the UI, you can get the URL from SageMaker Studio IDE or print it from the notebook using experiment_2.url.\nScreenshots of the Comet interface show experiments being compared \u0026mdash; these details are for illustration purposes and do not represent actual experiments.\n(Note: insert Comet UI screenshot here)\nClean Up Due to the ephemeral nature of SageMaker infrastructure (processing, training) \u0026mdash; it automatically shuts down after the job completes. However, you still need to:\nShut down JupyterLab Space when not in use (following the Idle shutdown guide).\nUnsubscribe from Comet if not continuing to use it (to avoid charges) \u0026mdash; the subscription will auto-renew if not cancelled.\nBenefits of SageMaker + Comet Integration Streamlined Model Development The SageMaker-Comet combination reduces manual burden when running experiments. While SageMaker handles infrastructure provisioning, Comet automatically logs hyperparameters, metrics, code, libraries, system information \u0026mdash; no additional configuration needed.\nComet supports visualization beyond simple metric charts: integrated charts enable quick experiment comparison; custom Python panels help you debug model behavior, optimize hyperparameters, or create custom visuals when default tools don\u0026rsquo;t meet needs.\nEnterprise Collaboration \u0026amp; Governance In enterprise environments, this combination creates a strong foundation for scaling ML projects in heavily regulated environments. SageMaker ensures consistent, secure ML environments; Comet supports collaboration with complete artifact flows and lineage. This helps avoid errors when teams cannot reproduce previous results.\nComplete ML Lifecycle Integration Unlike fragmented solutions that only support training or monitoring, SageMaker + Comet supports the entire ML lifecycle.\nModels can be registered in Comet\u0026rsquo;s model registry with versioning and management.\nSageMaker handles deployment.\nComet maintains lineage and approval workflow for promotion.\nComet monitors model performance and tracks data drift after deployment \u0026mdash; creating a feedback loop where information from production influences subsequent experiments.\nConclusion In this article, we presented how to integrate SageMaker and Comet to create a fully managed ML environment supporting reproducibility and experiment tracking. To complement your SageMaker workflow, you can deploy Comet directly within the SageMaker environment through AWS Marketplace.\nAbout the Authors "},{"uri":"https://Fatnotfatt.github.io/learning-aws/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Using Apache Airflow workflows to orchestrate data processing on Amazon SageMaker Unified Studio by Vinod Jayendra, Kamen Sharlandjiev, Sean Bjurstrom, and Suba Palanisamy\nSEPTEMBER 22, 2025\nOrchestrating machine learning pipelines is a complex task, especially when data processing, model training, and deployment are performed across multiple services and tools. In this article, we will walk through a practical \u0026ldquo;end-to-end\u0026rdquo; example — building, testing, and running an ML pipeline using SageMaker workflows through the SageMaker Unified Studio interface. These workflows are powered by Amazon Managed Workflows for Apache Airflow (Amazon MWAA).\nAlthough SageMaker Unified Studio has a visual builder (low-code) to create workflows, this article focuses on the code-based approach: writing and managing workflows as DAGs (Directed Acyclic Graphs) using Python in Apache Airflow.\nWe will examine a pipeline example that includes the following steps: ingesting weather data and taxi data, transforming \u0026amp; merging the data, then using ML to predict taxi fares — all orchestrated through SageMaker Unified Studio workflows.\nSolution overview This solution demonstrates how to use workflows in SageMaker Unified Studio to orchestrate a pipeline from data to ML model in a centralized environment. The pipeline consists of the following tasks:\nIngest \u0026amp; preprocess weather data\nUse a notebook in SageMaker Unified Studio to ingest simulated weather data, processing attributes such as time, temperature, rainfall, humidity, and wind speed.\nIngest, process, and merge taxi data\nUse a second notebook to ingest NYC taxi data (including pickup time, drop-off time, distance, passenger count, fare amount). Then process and join the taxi \u0026amp; weather data, saving the results to Amazon S3 for use in the next step.\nTrain and predict ML model\nA third notebook applies regression techniques to build a model that predicts taxi fares based on the merged data. The model is then used to make predictions on new data.\nThrough this approach, ETL (extract, transform, load) and ML steps are orchestrated within the same workflow, with full traceability of the data process and ensuring reproducibility through workflow management in SageMaker Unified Studio.\nPrerequisites Before building the workflow, you need to:\nCreate a SageMaker Unified Studio domain — follow AWS guidelines.\n(Create an Amazon SageMaker Unified Studio domain – quick setup)\nLog in to SageMaker Unified Studio domain — use the domain you created.\n(Access Amazon SageMaker Unified Studio)\nCreate a project in SageMaker Unified Studio — when creating the project, select the \u0026ldquo;All capabilities\u0026rdquo; profile to support full workflow functionality.\n(project creation guide)\nSet up workflow environment You can use workflows in SageMaker Unified Studio to set up and run a series of tasks such as notebooks, querybooks, and jobs. Workflows are written in Python code (Airflow DAGs), then you can access the Airflow UI from SageMaker for monitoring.\nSpecific steps:\nIn your project, go to Compute → Workflow environment.\nSelect Create environment to set up a new workflow environment.\nBy default, SageMaker Unified Studio will use the mw1.micro environment type — suitable for small testing.\nIf needed, you can override the default configuration (e.g., increase resources) when creating the project or adjust in blueprint deployment settings.\nDevelop workflows Workflows allow you to orchestrate notebooks, querybooks, etc. within the project. You can write Python DAGs, test, and share them with other members.\nExample:\nDownload 3 sample notebooks: Weather Data Ingestion, Taxi Ingest \u0026amp; Join, Prediction to your machine.\nIn SageMaker Unified Studio, go to Build → JupyterLab, upload the 3 notebooks.\nConfigure space: stop the current space → change instance type (e.g., ml.m5.8xlarge) → restart space.\nGo to Build → Orchestration → Workflows, select \u0026ldquo;Create new workflow\u0026rdquo; → select \u0026ldquo;Create in code editor\u0026rdquo;.\nIn the editor, create a new Python file multinotebook_dag.py in the src/workflows/dags folder. Paste the following example DAG code (modify \u0026lt;REPLACE-OWNER\u0026gt; and notebook paths accordingly):\nfrom airflow.decorators import dag\nfrom airflow.utils.dates import days_ago\nfrom workflows.airflow.providers.amazon.aws.operators.sagemaker_workflows import NotebookOperator\nWORKFLOW_SCHEDULE = \u0026lsquo;@daily\u0026rsquo;\nNOTEBOOK_PATHS = [\n\u0026lsquo;\u0026lt;FULL_PATH/Weather_Data_Ingestion.ipynb\u0026gt;\u0026rsquo;,\n\u0026lsquo;\u0026lt;FULL_PATH/Taxi_Weather_Data_Collection.ipynb\u0026gt;\u0026rsquo;,\n\u0026lsquo;\u0026lt;FULL_PATH/Prediction.ipynb\u0026gt;\u0026rsquo;\n]\ndefault_args = {\n\u0026lsquo;owner\u0026rsquo;: \u0026lsquo;\u0026lt;REPLACE-OWNER\u0026gt;\u0026rsquo;,\n}\n@dag(\ndag_id=\u0026lsquo;workflow-multinotebooks\u0026rsquo;,\ndefault_args=default_args,\nschedule_interval=WORKFLOW_SCHEDULE,\nstart_date=days_ago(2),\nis_paused_upon_creation=False,\ntags=[\u0026lsquo;MLPipeline\u0026rsquo;],\ncatchup=False\n)\ndef multi_notebook():\nprevious_task = None\nfor idx, notebook_path in enumerate(NOTEBOOK_PATHS, 1):\ncurrent_task = NotebookOperator(\ntask_id=f\u0026quot;Notebook{idx}task\u0026quot;,\ninput_config={\u0026lsquo;input_path\u0026rsquo;: notebook_path, \u0026lsquo;input_params\u0026rsquo;: {}},\noutput_config={\u0026lsquo;output_formats\u0026rsquo;: [\u0026lsquo;NOTEBOOK\u0026rsquo;]},\nwait_for_completion=True,\npoll_interval=5\n)\nif previous_task: previous_task \u0026gt;\u0026gt; current_task previous_task = current_task multi_notebook()\nNotebookOperator is used to run each notebook, with dependencies to ensure execution order.\nYou can customize WORKFLOW_SCHEDULE (e.g., @daily, @hourly, or cron expression).\nAfter the workflow environment is created and the DAG file is synced to the project, project members can view and run the shared workflow.\nTest and monitor workflow Go to Build → Orchestration → Workflows, you will see the workflow running on schedule or triggered.\nWhen the workflow completes, the status changes to \u0026ldquo;success\u0026rdquo;.\nYou can view each execution for details, logs of each task.\nAccess the Airflow UI from SageMaker to view DAGs, run history, detailed logs.\ny\nResults \u0026amp; outputs The model results are written to the results directory on Amazon S3. You need to check:\nPrediction accuracy\nConsistency in relationships between variables\nIf there are anomalous results, review the data processing steps, pipeline, and model assumptions.\nClean up To avoid unnecessary costs, you should delete the created resources:\nSageMaker Unified Studio domain\nS3 buckets related to the domain\nWorkflow environments, projects if no longer in use\nConclusion In this article, we demonstrated how you can use SageMaker Unified Studio to build an integrated ML workflow, including:\nCreating a SageMaker Unified Studio project\nUsing multi-compute notebooks to process data\nBuilding a DAG workflow in Python to orchestrate the entire pipeline\nRunning and monitoring workflows in SageMaker Unified Studio\nSageMaker provides a comprehensive toolset to execute steps from data preparation, model training to deployment. When used through SageMaker Unified Studio, these tools are consolidated in a single working environment, helping eliminate friction between disparate tools.\nAs organizations build complex data applications, teams can use SageMaker + Unified Studio to collaborate effectively and operate AI/ML with high reliability. You can discover data, build models, and orchestrate workflows in a managed and controlled environment.\nAbout the authors "},{"uri":"https://Fatnotfatt.github.io/learning-aws/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Migrating full-text search from SQL Server to Amazon Aurora PostgreSQL-Compatible Edition or Amazon RDS for PostgreSQL Authors: Sivaprasad Appana, Surya Nallu, and Saumitra Das\nPublished: August 19, 2024\nIn today\u0026rsquo;s data world, the ability to find and retrieve information from large datasets is crucial. While some database management systems (both commercial and open source) excel at handling structured data, PostgreSQL also provides powerful tools for searching unstructured or semi-structured data. PostgreSQL has built-in full-text search (FTS) capabilities, and also supports extensions like pg_trgm and pg_bigm for text search.\nTraditional queries using the LIKE, ILIKE operators or regular expressions are well-suited for finding exact strings or structured data, but have limitations when searching within large text fields such as documents, articles, or product descriptions.\nWhen migrating from a commercial database like SQL Server to PostgreSQL (such as Amazon Aurora PostgreSQL-Compatible or Amazon RDS for PostgreSQL), migrating full-text search requires modifying queries and schema structure, as the FTS implementation differs between the two systems. The AWS Schema Conversion Tool (AWS SCT) does not automatically convert full-text search related code.\nSQL Server FTS is designed to find words, phrases, or word forms (stemming) in unstructured text data. It supports fast searching, ranking, and text indexing, helping applications efficiently process large amounts of text information.\nIn this article, we will guide you on how to migrate full-text search from SQL Server to Amazon Aurora PostgreSQL by using the tsvector and tsquery data types. We will also show how to implement FTS using the pg_trgm and pg_bigm extensions.\nPrerequisites In this article, we use the AdventureWorks2019 sample database to illustrate how to migrate FTS from SQL Server 2019 Standard to PostgreSQL.\nThe main steps to set up FTS in SQL Server:\nEnable full-text search for the AdventureWorks2019 database: USE [AdventureWorks2019]\nGO\nEXEC sp_fulltext_database \u0026rsquo;enable\u0026rsquo;\nGO\nCreate a full-text catalog: CREATE FULLTEXT CATALOG DescFTSCatalog;\nGO\nA full-text catalog is a logical component for managing full-text indexes, defining word breakers and stemmers by language.\nDefine a full-text index for columns containing text data that you want to search: CREATE FULLTEXT INDEX\nON\n[AdventureWorks2019].[Production].ProductDescription\nKEY INDEX [PK_ProductDescription_ProductDescriptionID]\nON DescFTSCatalog\nGO\nUse AWS SCT and AWS Database Migration Service (AWS DMS) to migrate the AdventureWorks 2019 database from SQL Server to Amazon Aurora PostgreSQL. In this article, we migrate the Product Description table. PostgreSQL has several options for searching text: exact matching, pattern matching, regular expressions, and full-text search. In the following sections, we will guide you on how to use FTS in PostgreSQL on the migrated database to achieve similar results.\nFull-text search in PostgreSQL The LIKE, ILIKE operators and regular expressions are used in the WHERE clause for pattern matching. However, LIKE/ILIKE do not support ranking and typically ignore words such as \u0026ldquo;the\u0026rdquo;, \u0026ldquo;is\u0026rdquo;, etc. PostgreSQL provides FTS by using tsvector and tsquery, along with related functions, operators, and parameters.\ntsvector: a data type representing the processed version of text (word tokenization, removal of stop words, reduction to lexemes), optimized for fast text searching. The to_tsvector function converts text to tsvector.\ntsquery: contains one or more lexemes used for searching. Lexemes can be combined with operators to create complex search conditions. The to_tsquery or plainto_tsquery function converts search queries to tsquery.\nExample: \u0026ldquo;He is running in the park\u0026rdquo; → the words \u0026ldquo;he\u0026rdquo;, \u0026ldquo;run\u0026rdquo;, \u0026ldquo;park\u0026rdquo; after removing stop words and stemming.\nCONTAINS predicate with AND operator A simple FTS query in SQL Server uses the CONTAINS predicate. The CONTAINS predicate in Transact-SQL provides a flexible way to perform advanced FTS in SQL Server databases. It supports multiple search conditions, fuzzy searching, wildcards, and thesaurus features, allowing you to customize queries to meet specific requirements.\nIn the following sample query, the CONTAINS predicate checks for the words \u0026ldquo;entry\u0026rdquo; and \u0026ldquo;level\u0026rdquo; in the Description column:\nSELECT ProductDescriptionID,Description\nFROM [AdventureWorks2019].[Production].[ProductDescription]\nWHERE CONTAINS([Description], \u0026rsquo;entry \u0026amp; level\u0026rsquo;);\nCONTAINS predicate with OR operator This is similar to the previous use case using the CONTAINS predicate, except the check is performed using the OR operator. In the following sample query, the predicate checks for \u0026ldquo;entry\u0026rdquo;, \u0026ldquo;level\u0026rdquo;, or both:\nSELECT ProductDescriptionID,Description\nFROM [AdventureWorks2019].[Production].[ProductDescription]\nWHERE CONTAINS([Description], \u0026rsquo;entry | level\u0026rsquo;);\nYou can rewrite the query in PostgreSQL by using the to_tsvector and to_tsquery functions as follows and using the default built-in text search dictionary value of pg_catalog.simple.\nFREETEXT predicate The FREETEXT predicate in Transact-SQL (T-SQL) is used to perform full-text search in SQL Server databases. Unlike the CONTAINS function, which requires specific terms and conditions, the FREETEXT predicate allows more flexible and natural language-based searching.\nIn the following sample queries, FREETEXT checks for the words \u0026ldquo;entry\u0026rdquo; or \u0026ldquo;level\u0026rdquo; and their forms (using inflection) in the Description column:\nSELECT ProductDescriptionID, Description\nFROM [AdventureWorks2019].[Production].[Product description]\nWHERE FREETEXT([Description], \u0026rsquo;entry level\u0026rsquo;);\nYou can rewrite the query in PostgreSQL using the to_tsvector and to_tsquery functions as follows with the pg_catalog.english configuration value. This configuration uses english_stem and a simple dictionary to convert tokens to lexemes. Therefore, a lexeme represents a normalized form of a word or token that can be indexed and used for search operations.\nFREETEXTTABLE function with RANK FTS in SQL Server can generate an optional score (or rank value) that represents the relevance of data returned by the full-text query. This rank value is calculated per row and can be used as a sorting criterion to order the query result set by relevance. The rank value only shows the relative relevance order of rows in the result set. The actual value is not important and often differs each time you run the query. The rank value has no meaning between queries.\nIn the following sample queries, FREETEXTTABLE checks for the words \u0026ldquo;entry\u0026rdquo; or \u0026ldquo;level\u0026rdquo; and their forms (using inflection) in the Description column and also retrieves RANK information:\nSELECT FT_TBL.[ProductDescriptionID],FT_TBL.[Description], KEY_TBL.[RANK]\nFROM [AdventureWorks2019].[Production].[ProductDescription] FT_TBL\nINNER JOIN FREETEXTTABLE([AdventureWorks2019].[Production].[ProductDescription], [Description], \u0026rsquo;entry OR level\u0026rsquo;,1033) AS KEY_TBL\nON FT\\_TBL.[ProductDescriptionID] \\=KEY\\_TBL.[KEY] ORDER BY KEY_TBL.[RANK] DESC,FT_TBL.[ProductDescriptionID];\nIn PostgreSQL, the ts_rank function is used to calculate the relevance rank of search results based on how well they match a specific query. The rank is calculated using a numeric value indicating how well the document matches the search terms in the query.\nThe ts_headline function is used to create a text summary version of the document, highlighting the most relevant portions that match the specific search query. This function is useful for creating excerpts or search result headlines, providing context to users about why a particular document matches their search. The following screenshot shows the results of the PostgreSQL query headline column created using the ts_headline function.\n![][image5]\nCONTAINSTABLE and FORMSOF functions with RANK The FORMSOF function in SQL Server is used to perform inflectional search. Inflectional search involves searching for different forms of a word, such as plural forms, verb tenses, or related word forms. This can help you find relevant documents even when they contain variations of the search term, thus improving search accuracy.\nIn the following sample queries, CONTAINSTABLE checks for the word \u0026ldquo;gear\u0026rdquo; and its forms (using INFLECTIONAL) in the Description column and also retrieves RANK information:\nSELECT FT_TBL.[ProductDescriptionID],FT_TBL.[Description], KEY_TBL.[RANK]\nFROM [AdventureWorks2019].[Production].[ProductDescription] FT_TBL\n**INNER JOIN CONTAINSTABLE([AdventureWorks2019].[Production].[ProductDescription],** **[Description], 'FORMSOF(INFLECTIONAL,''gear'')',1033)** AS KEY_TBL\nON FT_TBL.[ProductDescriptionID] = KEY_TBL.[KEY]\nORDER BY KEY_TBL.[RANK] DESC,FT_TBL.[ProductDescriptionID];\nIn PostgreSQL queries, phrases are first broken down into words or tokens, and these words are normalized and classified into root words (lexemes) using the pg_catalog.english FTS configuration. These lexemes will be the same for different forms (inflections) of a word. Therefore, this feature automatically handles inflectional searches.\nSQL Server SELECT FT_TBL.[ProductDescriptionID],FT_TBL.[Description], KEY_TBL.[RANK] FROM [AdventureWorks2019].[Production].[ProductDescription] FT_TBL INNER JOIN CONTAINSTABLE([AdventureWorks2019].[Production].[ProductDescription], [Description], \u0026lsquo;FORMSOF(INFLECTIONAL,\u0026lsquo;\u0026lsquo;gear\u0026rsquo;\u0026rsquo;)\u0026rsquo;,1033) AS KEY_TBL ON FT_TBL.[ProductDescriptionID] = KEY_TBL.[KEY] ORDER BY KEY_TBL.[RANK] DESC,FT_TBL.[ProductDescriptionID]; ![][image6] PostgreSQL | SELECT p.productdescriptionid ,p.description, ts_rank(to_tsvector(\u0026lsquo;pg_catalog.english\u0026rsquo;,p.Description), query) AS rank, ts_headline(\u0026lsquo;pg_catalog.english\u0026rsquo;,p.Description,query) headline FROM production.productdescription p, to_tsquery(\u0026lsquo;pg_catalog.english\u0026rsquo;,\u0026lsquo;gear\u0026rsquo;) query WHERE query @@ to_tsvector(\u0026lsquo;pg_catalog.english\u0026rsquo;,p.Description) ORDER BY rank desc,p.productdescriptionid; Improving query performance in PostgreSQL For the sample PostgreSQL queries shown previously, the to_tsvector function retrieves tsvector values from the Description column in the productdescription table. In the following sections, we will introduce you to various options to improve query performance.\nSolution 1: Using GIN index A GIN (Generalized Inverted Index) in PostgreSQL is a popular indexing method used to efficiently speed up searching for complex data types such as JSON and full-text search. Standard database indexes, a B-tree, are designed to check for equality, while GIN is designed for search patterns that operate on nested or aggregate data structures, allowing more expressive search patterns. By indexing the components of complex data types separately, GIN indexes enable faster queries on arrays, JSON data, and text search operations. This makes GIN indexes a useful tool for improving the performance of queries related to complex data structures in PostgreSQL databases.\nIn this approach, you create a GIN index based on an expression on the column of interest in the product description table.\nRun the following command: CREATE INDEX productdescription_gin_idx ON production.productdescription\nUSING GIN (to_tsvector(\u0026lsquo;pg_catalog.english\u0026rsquo;, Description)); If the table has millions of rows, you can increase the maintenance_work_mem configuration parameter at the session level to speed up index creation time. maintenance_work_mem specifies the maximum amount of memory in MB that will be used for maintenance operations such as creating INDEX—by default (PostgreSQL), it is 64 MB. Run the following EXPLAIN ANALYZE query: EXPLAIN ANALYZE\nSELECT * FROM production.productdescription\nWHERE to_tsvector(\u0026lsquo;pg_catalog.english\u0026rsquo;, Description) @@ to_tsquery(\u0026lsquo;pg_catalog.english\u0026rsquo;,\u0026rsquo;entry | level\u0026rsquo;)\nORDER BY productdescriptionid DESC; The output shows that a bitmap index scan is being performed on productdescription_gin_index, improving query performance. The following screenshot shows the explain plan before creating the index.\nThe following screenshot shows the explain plan after creating the index.\nIn this case, we can see that query performance is improved when using the GIN index. Although generally, using GIN indexes for full-text search in PostgreSQL can help improve performance, you need to be aware of other performance trade-offs, including the time required to build the index and the additional storage space that the index requires.\nSolution 2: Using stored generated column In this approach, you create a computed column description_tsv containing tsvector values from the description column in the table followed by a GIN index on the computed column.\nRun the following commands: ALTER TABLE production.productdescription\nADD COLUMN description_tsv tsvector\nGENERATED ALWAYS AS (to_tsvector(\u0026lsquo;pg_catalog.english\u0026rsquo;,Description )) STORED;\nCREATE INDEX productdescription_gin_idx ON production.productdescription USING GIN(description_tsv); Run the following sample EXPLAIN ANALYZE query: EXPLAIN ANALYZE\nSELECT *\nFROM production.productdescription\nWHERE description_tsv @@ to_tsquery(\u0026lsquo;pg_catalog.english\u0026rsquo;,\u0026rsquo;entry | level\u0026rsquo;)\nORDER BY productdescriptionid DESC; The output shows that a bitmap index scan is being performed on productdescription_gin_index, in this case demonstrating improved query performance:\nFull-text search in PostgreSQL using the pg_trgm extension In PostgreSQL, the pg_trgm extension is implemented for text search functions using trigrams. A trigram is essentially a set of three consecutive characters extracted from a given string. By using trigrams, users can identify similarity or matches in text patterns within strings by comparing the number of matching trigrams between strings, along with a predefined similarity threshold parameter set before performing the search.\nThe pg_trgm extension provides operators that can be used to create trigram indexes on text columns in tables that need to be searched. This index enables efficient similarity operations on indexed columns. The extension provides three similarity operators: similarity (%), word_similarity (\u0026lt;%), and strict_word_similarity (\u0026lt;\u0026lt;%). The threshold parameters for the respective operators are pg_trgm.similarity_threshold, pg_trgm.word_similarity_threshold, and pg_trgm.strict_word_similarity_threshold, which can be set to values from 0 (no similarity) to 1 (perfect match). The similarity(), word_similarity(), and strict_word_similarity() functions are used to calculate similarity scores. You can use pg_trgm as in the following code snippet:\nRun the following command to create the pg_trgm extension:\nCREATE EXTENSION pg_trgm; Run the following command to create a GIN index on the productdescription column: CREATE INDEX productdescription_trgm_idx ON production.productdescription USING GIN (Description gin_trgm_ops); Run the following command to set the similarity_threshold configuration value to 0.2. The similarity feature will check for common trigrams between two strings and return a value from 0–1. SET pg_trgm.similarity_threshold = 0.2;\nSET enable_seqscan = off;\nSELECT productdescriptionid, Description, similarity(Description, \u0026rsquo;entry level\u0026rsquo;) AS sml\nFROM production.product description\nWHERE Description % \u0026rsquo;entry level\u0026rsquo;\nORDER BY sml DESC, Description; Run the following command to set the word_similarity_threshold configuration value to 0.6. word_similarity checks for common trigrams between strings at the word level. SET pg_trgm.word_similarity_threshold = 0.6;\nSET enable_seqscan = off;\nSELECT productdescriptionid, Description, word_similarity(\u0026rsquo;entry level\u0026rsquo;, Description) AS sml\nFROM production.productdescription\nWHERE \u0026rsquo;entry level\u0026rsquo; \u0026lt;% Description\nORDER BY sml DESC, Description; Run the following command to set the strict_word_similarity_threshold configuration value to 0.6. strict_word_similarity is similar to word_similarity but it only considers common trigrams when both words are identical. SET pg_trgm.strict_word_similarity_threshold = 0.6;\nSET enable_seqscan = off;\nSELECT productdescriptionid, Description, strict_word_similarity(\u0026lsquo;aluminum cups and hollow axle\u0026rsquo;, Description) AS sml\nFROM production.productdescription\nWHERE \u0026rsquo;entry level\u0026rsquo; \u0026lt;\u0026lt;% Description\nORDER BY sml DESC, Description; Run the following command to drop the index and enable sequential scan: DROP INDEX production.productdescription_trgm_idx; Full-text search in PostgreSQL using the pg_bigm extension The pg_bigm extension in PostgreSQL enhances full-text search capabilities, especially for languages with complex character sets such as Asian languages.\nA bigram is a group of two consecutive characters taken from a string. This extension uses a bigram indexing method, which involves dividing text into consecutive character pairs and building an index based on these bigrams. The pg_bigm extension provides the bigm_similarity() function, the bigm similarity operator = %, and the pg_bigm.similarity_limit threshold parameter. You can use pg_bigm as follows:\nRun the following command to create the pg_bigm extension. For instructions on creating extensions in Amazon RDS for PostgreSQL, refer to the article Using PostgreSQL extensions with Amazon RDS for PostgreSQL . CREATE EXTENSION pg_bigm; Run the following command to create a GIN index on the productdescription column: CREATE INDEX productdescription_bigm_idx ON production.productdescription USING gin (Description gin_bigm_ops); Run the following command to set the similarity_limit configuration value to 0.15. The similarity check finds common bigrams between two strings and returns a value from 0–1. SET pg_bigm.similarity_limit TO 0.15;\nSELECT *,bigm_similarity(Description, \u0026lsquo;%entry level%\u0026rsquo;) rank1\nFROM production.productdescription\nWHERE Description =% \u0026lsquo;%entry level%\u0026rsquo;\nORDER BY rank1 DESC; Run the following command to drop the index and enable sequential scan: DROP INDEX production.productdescription_bigm_idx;\nSET enable_seqscan = on; Conclusion In this article, we have guided you on how to migrate FTS from SQL Server to PostgreSQL and compared some common use cases. Migrating full-text search from SQL Server to PostgreSQL requires manual code rewriting. To learn more, please refer to the limitations of text search features in PostgreSQL . We have also guided you on how to use the pg_trgm and pg_bigm extensions in PostgreSQL to implement FTS.\nAbout the authors\n![][image15]\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AI-Driven Development Life Cycle: Reimagining Software Engineering\u0026rdquo; Event Objectives Explore the transformative impact of generative AI on software development Understand how to integrate AI into the software development lifecycle (SDLC) Demonstrate AI tools for automating development tasks: Amazon Q Developer and Kiro Learn how to leverage AI to increase productivity and focus on high-value work Speakers \u0026amp; Organizers Instructors:\nToan Huynh – AI-Driven Development Life Cycle overview and Amazon Q Developer demonstration My Nguyen – Kiro demonstration Coordinators:\nDiem My Dai Truong Dinh Nguyen Event Details Date: Friday, October 3, 2025 Time: 2:00 PM – 4:30 PM Location: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City Duration: 2.5 hours Agenda 2:00 PM - 2:15 PM: Welcoming 2:15 PM - 3:30 PM: AI-Driven Development Life Cycle overview and Amazon Q Developer demonstration (by Toan Huynh) 3:30 PM - 3:45 PM: Break 3:45 PM - 4:30 PM: Kiro demonstration (by My Nguyen) Key Highlights The Rise of Generative AI in Software Development Transformative shift: Generative AI reimagines how developers and organizations learn, plan, create, deploy, and manage applications securely SDLC integration: AI can be integrated across the entire software development lifecycle: architecture, development, testing, deployment, and maintenance Automation benefits: Automates undifferentiated heavy lifting tasks, allowing developers to focus on higher-value, creative work Productivity gains: Increases productivity while enabling developers to concentrate on innovative problem-solving AI-Driven Development Life Cycle End-to-end coverage: From initial architecture planning through development, testing, deployment, and ongoing maintenance Workflow transformation: How AI tools reshape traditional development workflows Best practices: Guidelines for effectively integrating AI into existing development processes Amazon Q Developer SDLC support: Comprehensive AI tool that supports the entire software development lifecycle Key capabilities: Code generation, debugging, testing, documentation, and refactoring assistance Integration: Seamless integration with popular IDEs and development environments Practical demonstration: Real-world examples of using Amazon Q Developer to accelerate development tasks Kiro AI-powered development: Introduction to Kiro as an AI development assistant Use cases: Specific scenarios where Kiro enhances developer productivity Features: Key features and capabilities demonstrated in the session Hands-on experience: Practical demonstration of Kiro in action Key Takeaways AI Integration Strategy Gradual adoption: Start with specific use cases and gradually expand AI tool integration Quality assurance: AI tools assist but human oversight and review remain critical Learning curve: Understanding AI tools requires time and practice to maximize benefits Team collaboration: AI enhances team productivity but requires clear workflows and guidelines Development Workflow Enhancement Automated tasks: Identify repetitive, low-value tasks that can be automated with AI Code quality: Use AI for code review, testing, and documentation to maintain high standards Acceleration: Leverage AI to speed up development cycles without sacrificing quality Continuous learning: AI tools evolve rapidly—stay updated with new features and best practices Productivity and Focus Value creation: Free developers from routine tasks to focus on complex problem-solving and innovation Time savings: Significant time savings in code writing, debugging, and documentation Knowledge augmentation: AI tools help bridge knowledge gaps and provide contextual assistance Scalability: AI enables teams to handle larger projects with the same resources Applying to Work Integrate Amazon Q Developer: Start using it in daily development tasks for code generation and assistance Explore Kiro: Evaluate Kiro for specific use cases in your development workflow Establish AI workflows: Define guidelines for when and how to use AI tools in team projects Measure productivity: Track improvements in development speed and code quality after AI tool adoption Share learnings: Document best practices and share experiences with team members Stay updated: Follow updates to AI development tools and incorporate new features as they become available Outcomes \u0026amp; Value Gained Attending this event provided significant value through new knowledge, skills, and practical insights that can be directly applied to current and future projects.\nNew Knowledge Acquired AI-Driven Development Concepts:\nDeep understanding of how generative AI transforms the software development lifecycle from planning to maintenance Comprehensive knowledge of integrating AI tools across different stages: architecture design, code generation, testing, deployment, and ongoing monitoring Insights into best practices for AI-assisted development, including when to leverage AI and when human judgment is critical Amazon Q Developer Expertise:\nPractical knowledge of using Amazon Q Developer for code generation, debugging, documentation, and refactoring Understanding of how to integrate Amazon Q Developer with existing IDEs and development environments Learning about specific capabilities: intelligent code suggestions, automated testing assistance, and code quality improvements Kiro Platform Understanding:\nExploration of Kiro as an AI development assistant and its unique features Knowledge of specific use cases where Kiro can enhance developer productivity Understanding of how Kiro complements other AI development tools in the workflow New Skills Developed AI Tool Integration:\nSkill: Ability to identify opportunities for AI automation in development workflows Skill: Proficiency in integrating AI tools like Amazon Q Developer into daily development tasks Skill: Capability to evaluate and select appropriate AI tools for specific project needs Development Workflow Enhancement:\nSkill: Improved ability to automate repetitive coding tasks while maintaining code quality Skill: Enhanced code review capabilities using AI-assisted analysis Skill: Better documentation practices through AI-powered documentation generation AI-Assisted Problem Solving:\nSkill: Leveraging AI for debugging and troubleshooting complex issues Skill: Using AI suggestions to improve code efficiency and best practices Skill: Balancing AI assistance with critical human review and judgment Lessons Learned Practical Insights:\nAI tools are powerful productivity multipliers but require proper understanding and workflow integration Gradual adoption strategy is more effective than attempting to integrate all AI tools at once Human oversight remains essential—AI assists but doesn\u0026rsquo;t replace developer expertise and judgment Measuring productivity gains and code quality improvements helps justify AI tool adoption Strategic Understanding:\nSuccessful AI integration requires team alignment and clear guidelines on usage AI tools evolve rapidly—staying updated with new features maximizes long-term value Identifying the right use cases is crucial—not all development tasks benefit equally from AI assistance Code quality can actually improve with AI tools when used thoughtfully and reviewed properly Contribution to Team/Projects Knowledge Sharing:\nDocumentation: Created notes and documentation of best practices from the event to share with team members Presentation: Prepared to share insights about Amazon Q Developer and Kiro with the development team Guidelines: Developed preliminary guidelines for integrating AI tools into team workflows Practical Application:\nPilot Projects: Identified specific projects where AI tools can be piloted for immediate productivity gains Workflow Improvement: Proposed integration of Amazon Q Developer for code review and documentation tasks Training: Planning to conduct internal sessions on AI-driven development practices for the team Long-term Value:\nCompetitive Advantage: Gained knowledge that positions the team to leverage cutting-edge AI development tools Efficiency Gains: Anticipated 20-30% improvement in development speed for repetitive tasks using AI tools Quality Enhancement: Potential for improved code quality through AI-assisted review and best practice suggestions Innovation: New capabilities enable the team to tackle more complex problems by offloading routine work to AI Personal Growth Technical Growth:\nExpanded understanding of modern software development practices with AI integration Developed a forward-thinking perspective on the evolution of software engineering Enhanced ability to evaluate and adopt new technologies effectively Professional Development:\nIncreased confidence in working with AI-powered development tools Improved ability to communicate technical concepts to both technical and non-technical stakeholders Strengthened network connections with other developers interested in AI-driven development Event Experience Attending the \u0026ldquo;AI-Driven Development Life Cycle: Reimagining Software Engineering\u0026rdquo; session was an eye-opening experience that provided deep insights into how generative AI is transforming software development. Key experiences included:\nUnderstanding the AI transformation Learned how generative AI marks a transformative shift in software development practices. Gained insights into integrating AI across the entire SDLC: architecture, development, testing, deployment, and maintenance. Understood how AI automation enables developers to focus on higher-value, creative tasks. Hands-on tool demonstrations Witnessed Amazon Q Developer in action, seeing how it can assist with code generation, debugging, and documentation. Explored Kiro as an AI development assistant and learned about its specific capabilities and use cases. Saw practical examples of how these tools accelerate development while maintaining code quality. Practical learning Learned about best practices for integrating AI tools into existing development workflows. Understood the importance of human oversight and quality assurance when using AI tools. Gained insights into identifying opportunities for AI automation in development processes. Networking and discussions Connected with other developers interested in AI-driven development. Exchanged ideas about practical applications of AI tools in real-world projects. Discussed challenges and opportunities in adopting AI development tools. Lessons learned AI tools are powerful assistants but human judgment and review remain essential for quality code. Successful AI integration requires gradual adoption and team training. The right AI tools can significantly boost productivity and free developers for more creative work. Staying updated with AI tool evolution is crucial for maximizing benefits. Some event photos Overall, this event opened my eyes to the potential of AI-driven development and provided practical guidance on how to leverage these powerful tools to enhance productivity, improve code quality, and focus on high-value work in software engineering.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; Event Objectives Provide comprehensive overview of AWS AI/ML services and capabilities Introduce Amazon SageMaker as an end-to-end ML platform Explore Generative AI with Amazon Bedrock Demonstrate practical applications through live demos Share best practices for AI/ML implementation in Vietnam Event Details Date: Saturday, November 15, 2025 Time: 8:30 AM – 12:00 PM Location: AWS Vietnam Office Duration: 3.5 hours (excluding lunch break) Agenda 8:30 – 9:00 AM | Welcome \u0026amp; Introduction Participant registration and networking Workshop overview and learning objectives Ice-breaker activity Overview of the AI/ML landscape in Vietnam 9:00 – 10:30 AM | AWS AI/ML Services Overview Amazon SageMaker – End-to-end ML platform Data preparation and labeling Model training, tuning, and deployment Integrated MLOps capabilities Live Demo: SageMaker Studio walkthrough 10:30 – 10:45 AM | Coffee Break 10:45 AM – 12:00 PM | Generative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan – comparison \u0026amp; selection guide Prompt Engineering: Techniques, Chain-of-Thought reasoning, Few-shot learning Retrieval-Augmented Generation (RAG): Architecture \u0026amp; Knowledge Base integration Bedrock Agents: Multi-step workflows and tool integrations Guardrails: Safety and content filtering Live Demo: Building a Generative AI chatbot using Bedrock Key Highlights Amazon SageMaker Platform Comprehensive ML Platform: Complete solution for building, training, and deploying machine learning models Data Preparation: Tools for data labeling, feature engineering, and data validation Model Training: Support for various ML frameworks and algorithms with distributed training capabilities Model Deployment: Flexible deployment options including real-time inference, batch processing, and serverless inference MLOps Integration: Built-in capabilities for model monitoring, versioning, and automated workflows Generative AI with Amazon Bedrock Foundation Model Selection: Understanding differences between Claude, Llama, and Titan models Claude: Strong reasoning and conversation capabilities Llama: Open-source models with good performance Titan: AWS-developed models optimized for specific use cases Prompt Engineering Techniques: Chain-of-Thought reasoning for complex problem solving Few-shot learning with examples Context management and prompt optimization RAG Architecture: Combining retrieval with generation for accurate, context-aware responses Knowledge base integration Vector embeddings and similarity search Document chunking strategies Bedrock Agents: Autonomous agents that can perform multi-step tasks Tool integrations and API calling Workflow orchestration Decision-making capabilities Guardrails for AI Safety: Content filtering and safety controls Harmful content detection Custom policy configurations Compliance and governance AI/ML Landscape in Vietnam Current adoption trends and opportunities Use cases specific to Vietnamese market Challenges and solutions for local businesses Success stories and case studies Key Takeaways Machine Learning Best Practices End-to-end Platform Approach: Use SageMaker for complete ML lifecycle management Data Quality First: Invest in data preparation and labeling for better model performance MLOps Integration: Implement monitoring and automated workflows from the start Model Selection Strategy: Choose the right model based on use case, not just performance metrics Generative AI Implementation Foundation Model Selection: Understand strengths of each model (Claude, Llama, Titan) for different scenarios Prompt Engineering Mastery: Chain-of-Thought and Few-shot learning significantly improve results RAG for Accuracy: Use RAG architecture when factual accuracy is critical Agent Design: Build agents that can handle multi-step workflows with proper tool integration Safety First: Always implement guardrails for content filtering and compliance Production Readiness Start Small, Scale Gradually: Begin with pilot projects before full deployment Cost Optimization: Monitor and optimize inference costs with serverless options Security \u0026amp; Compliance: Implement proper access controls and data privacy measures Continuous Improvement: Monitor model performance and iterate based on real-world feedback Applying to Work Explore SageMaker: Start with SageMaker Studio for ML experimentation and model development Implement RAG Solutions: Build knowledge bases for domain-specific applications using RAG architecture Develop Bedrock Agents: Create autonomous agents for customer service or workflow automation Prompt Engineering Practice: Apply Chain-of-Thought and Few-shot techniques to improve AI responses Deploy Guardrails: Implement content filtering and safety controls for production GenAI applications MLOps Setup: Establish model monitoring and automated deployment pipelines using SageMaker capabilities Event Experience Attending the \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; workshop was an exceptional learning experience that provided comprehensive insights into AWS\u0026rsquo;s AI and machine learning capabilities. The event combined theoretical knowledge with practical demonstrations, giving me a clear understanding of how to implement AI/ML solutions on AWS.\nLearning from comprehensive agenda The structured agenda covered everything from foundational ML concepts to advanced Generative AI implementations. Starting with SageMaker platform overview helped me understand the complete ML lifecycle before diving into GenAI specifics. The progression from traditional ML to Generative AI showed the evolution and complementary nature of these technologies. Hands-on technical exposure The SageMaker Studio walkthrough demonstrated the practical workflow of building ML models, from data preparation to deployment. I learned about data labeling tools and how they can significantly improve model accuracy with proper data quality. The MLOps capabilities showed me how to implement continuous integration and monitoring for ML models in production. Generative AI deep dive The Amazon Bedrock session was eye-opening, showing me how to leverage foundation models without training them from scratch. Prompt Engineering techniques like Chain-of-Thought reasoning and Few-shot learning were demonstrated with practical examples. Learning about RAG architecture helped me understand how to build accurate AI applications that combine retrieval with generation. The Bedrock Agents demo showed how to build autonomous AI systems that can perform complex multi-step tasks. Practical demonstrations The live demo of building a Generative AI chatbot using Bedrock gave me a complete picture of implementation from start to finish. Seeing Guardrails in action demonstrated the importance of safety and content filtering in production GenAI applications. The comparison between Claude, Llama, and Titan models helped me understand when to use each model. Networking and discussions The workshop provided excellent networking opportunities with other AI/ML enthusiasts and practitioners in Vietnam. Discussing AI/ML landscape in Vietnam gave me context-specific insights into local market opportunities and challenges. Sharing experiences with peers helped me understand real-world implementation challenges and solutions. Lessons learned SageMaker provides a complete platform that simplifies the entire ML lifecycle, from data preparation to deployment. Foundation models in Bedrock eliminate the need to train large models from scratch, significantly reducing time and cost. RAG architecture is crucial for building accurate GenAI applications that need to reference specific knowledge bases. Prompt engineering is a skill that requires practice and understanding of different techniques for optimal results. Guardrails are essential for production GenAI applications to ensure safety and compliance. Some event photos Overall, this workshop provided me with both foundational knowledge and practical skills needed to implement AI/ML and Generative AI solutions on AWS. The combination of comprehensive platform overview, detailed GenAI capabilities, and hands-on demonstrations gave me confidence to start building AI-powered applications.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: \u0026ldquo;DevOps on AWS\u0026rdquo; Event Objectives Introduce DevOps culture, principles, and key metrics Demonstrate AWS DevOps services for CI/CD pipeline automation Explore Infrastructure as Code (IaC) with CloudFormation and CDK Cover container services and microservices deployment strategies Provide monitoring and observability best practices Share real-world DevOps case studies and best practices Event Details Date: Monday, November 17, 2025 Time: 8:30 AM – 5:00 PM Location: Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh Duration: Full day (8.5 hours with breaks) Agenda Morning Session (8:30 AM – 12:00 PM) 8:30 – 9:00 AM | Welcome \u0026amp; DevOps Mindset\nRecap of AI/ML session DevOps culture and principles Benefits and key metrics (DORA, MTTR, deployment frequency) 9:00 – 10:30 AM | AWS DevOps Services – CI/CD Pipeline\nSource Control: AWS CodeCommit, Git strategies (GitFlow, Trunk-based) Build \u0026amp; Test: CodeBuild configuration, testing pipelines Deployment: CodeDeploy with Blue/Green, Canary, and Rolling updates Orchestration: CodePipeline automation Demo: Full CI/CD pipeline walkthrough 10:30 – 10:45 AM | Break\n10:45 AM – 12:00 PM | Infrastructure as Code (IaC)\nAWS CloudFormation: Templates, stacks, and drift detection AWS CDK (Cloud Development Kit): Constructs, reusable patterns, and language support Demo: Deploying with CloudFormation and CDK Discussion: Choosing between IaC tools Lunch Break (12:00 – 1:00 PM) Afternoon Session (1:00 – 5:00 PM) 1:00 – 2:30 PM | Container Services on AWS\nDocker Fundamentals: Microservices and containerization Amazon ECR: Image storage, scanning, lifecycle policies Amazon ECS \u0026amp; EKS: Deployment strategies, scaling, and orchestration AWS App Runner: Simplified container deployment Demo \u0026amp; Case Study: Microservices deployment comparison 2:30 – 2:45 PM | Break\n2:45 – 4:00 PM | Monitoring \u0026amp; Observability\nCloudWatch: Metrics, logs, alarms, and dashboards AWS X-Ray: Distributed tracing and performance insights Demo: Full-stack observability setup Best Practices: Alerting, dashboards, and on-call processes 4:00 – 4:45 PM | DevOps Best Practices \u0026amp; Case Studies\nDeployment strategies: Feature flags, A/B testing Automated testing and CI/CD integration Incident management and postmortems Case Studies: Startups and enterprise DevOps transformations 4:45 – 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up\nDevOps career pathways AWS certification roadmap Key Highlights DevOps Culture and Principles DevOps Mindset: Collaboration between development and operations teams Cultural Transformation: Breaking down silos and fostering shared responsibility Key Metrics (DORA): Measuring DevOps performance Deployment Frequency: How often deployments occur Lead Time: Time from code commit to production MTTR (Mean Time To Recovery): Time to recover from failures Change Failure Rate: Percentage of deployments causing failures Benefits: Faster delivery, improved reliability, better collaboration AWS CI/CD Pipeline Services AWS CodeCommit:\nFully managed source control service Git-based version control Integration with other AWS services Git strategies: GitFlow, Trunk-based development, feature branches AWS CodeBuild:\nFully managed build service Scalable build environments Supports multiple programming languages and build tools Build artifacts and test reports Integration with testing frameworks AWS CodeDeploy:\nAutomated deployment service Deployment strategies: Blue/Green: Zero-downtime deployments with instant rollback Canary: Gradual rollout with automatic rollback on errors Rolling: Rolling updates with configurable batch sizes Application deployment across EC2, Lambda, and on-premises AWS CodePipeline:\nFully managed continuous delivery service Visual workflow builder Integration with third-party tools Automated pipeline orchestration Approval gates and manual intervention points Infrastructure as Code (IaC) AWS CloudFormation:\nDeclarative IaC service JSON/YAML template syntax Stack management and resource provisioning Drift detection and stack updates Change sets for previewing changes Nested stacks for modular infrastructure AWS CDK (Cloud Development Kit):\nProgrammatic IaC using familiar programming languages TypeScript, Python, Java, C#, and Go support Constructs for reusable infrastructure patterns Higher-level abstractions and best practices Integration with CloudFormation CLI tools for deployment and management Choosing Between IaC Tools:\nCloudFormation: Declarative, template-based, AWS-native CDK: Programmatic, type-safe, developer-friendly Use cases and when to choose each approach Container Services on AWS Docker Fundamentals:\nContainerization benefits and use cases Microservices architecture with containers Docker image creation and optimization Multi-stage builds and best practices Amazon ECR (Elastic Container Registry):\nFully managed Docker container registry Image storage and versioning Image scanning for vulnerabilities Lifecycle policies for automated cleanup Integration with ECS and EKS Amazon ECS (Elastic Container Service):\nFully managed container orchestration Fargate (serverless) and EC2 launch types Task definitions and service configurations Auto-scaling and load balancing Service discovery and networking Amazon EKS (Elastic Kubernetes Service):\nManaged Kubernetes service Kubernetes-native orchestration Worker nodes management Add-ons and ecosystem integration Multi-tenant and namespace isolation AWS App Runner:\nSimplified container deployment Automatic scaling and load balancing Source code or container image deployment Built-in CI/CD integration Pay-per-use pricing model Monitoring \u0026amp; Observability Amazon CloudWatch:\nMetrics: Application and infrastructure metrics Logs: Centralized log management and analysis Alarms: Automated alerting and notifications Dashboards: Custom visualization of metrics and logs Insights: Automated anomaly detection Composite Alarms: Complex alerting logic AWS X-Ray:\nDistributed tracing for microservices Request flow visualization Performance bottleneck identification Service map generation Integration with Lambda, ECS, and API Gateway Trace analysis and filtering Best Practices:\nSetting up effective alerting strategies Creating meaningful dashboards On-call processes and incident response Log aggregation and analysis Metric collection and retention policies DevOps Best Practices Deployment Strategies:\nFeature Flags: Gradual feature rollouts A/B Testing: Comparing different versions Canary Deployments: Risk mitigation through gradual rollout Blue/Green Deployments: Zero-downtime updates Automated Testing:\nUnit, integration, and end-to-end testing Test automation in CI/CD pipelines Quality gates and test coverage Performance and load testing Incident Management:\nRunbook creation and maintenance Incident response procedures Postmortem analysis and learning Continuous improvement processes Key Takeaways DevOps Culture Transformation Cultural Change is Fundamental: Tools alone don\u0026rsquo;t make DevOps—culture and collaboration are key Measure What Matters: Use DORA metrics to track DevOps maturity Continuous Improvement: DevOps is a journey, not a destination Automation First: Automate repetitive tasks to focus on high-value work CI/CD Best Practices Start Simple, Scale Gradually: Begin with basic pipelines and add complexity over time Git Strategy Matters: Choose GitFlow or Trunk-based based on team size and release cadence Testing is Critical: Integrate automated testing at every stage Deployment Strategies: Use appropriate deployment strategy based on risk tolerance Infrastructure as Code: Always use IaC for reproducible and version-controlled infrastructure Container Orchestration Choose Wisely: ECS for simplicity, EKS for Kubernetes ecosystem Start with Serverless: Fargate eliminates node management overhead Optimize Images: Smaller images mean faster deployments and lower costs Security First: Scan images and use least-privilege IAM policies Observability Strategy Implement Full-Stack Observability: Metrics, logs, and traces together Proactive Monitoring: Set up alarms before incidents occur Meaningful Dashboards: Create dashboards that provide actionable insights Distributed Tracing: Essential for debugging microservices architectures Applying to Work Implement CI/CD Pipelines: Set up CodePipeline for automated deployments Adopt Infrastructure as Code: Use CloudFormation or CDK for all infrastructure Containerize Applications: Start containerizing applications for better portability Set Up Monitoring: Implement CloudWatch and X-Ray for observability Establish DevOps Practices: Create runbooks, incident response procedures, and postmortem templates Measure DevOps Metrics: Track DORA metrics to measure improvement Event Experience Attending the \u0026ldquo;DevOps on AWS\u0026rdquo; full-day workshop was an intensive and comprehensive learning experience that covered the entire DevOps spectrum from culture to implementation. The event provided both theoretical knowledge and practical demonstrations, giving me a complete understanding of implementing DevOps practices on AWS.\nLearning DevOps fundamentals The session started with DevOps mindset and culture, emphasizing that DevOps is more than just tools—it\u0026rsquo;s about collaboration and shared responsibility. I learned about DORA metrics (Deployment Frequency, Lead Time, MTTR, Change Failure Rate) and how to measure DevOps maturity. Understanding the benefits of DevOps helped me see the bigger picture beyond technical implementation. AWS CI/CD pipeline deep dive The CodeCommit, CodeBuild, CodeDeploy, and CodePipeline walkthrough showed me how to build a complete CI/CD pipeline. Learning about different Git strategies (GitFlow vs Trunk-based) helped me understand when to use each approach. The deployment strategies (Blue/Green, Canary, Rolling) demo was eye-opening, showing how to minimize risk and downtime. The live CI/CD pipeline demo demonstrated the entire workflow from code commit to production deployment. Infrastructure as Code mastery CloudFormation demonstrated how to manage infrastructure declaratively with templates. AWS CDK showed me how to write infrastructure code in familiar programming languages, making it more maintainable. The comparison between CloudFormation and CDK helped me understand when to use each tool. Learning about drift detection and change sets gave me confidence in managing infrastructure safely. Container services exploration Docker fundamentals refreshed my understanding of containerization and its benefits. Amazon ECR showed how to manage container images securely with scanning and lifecycle policies. Comparing ECS and EKS helped me understand the trade-offs between managed services and Kubernetes flexibility. AWS App Runner introduced a simpler way to deploy containers without managing infrastructure. The microservices deployment case study provided real-world insights into choosing the right container service. Monitoring and observability setup CloudWatch comprehensive coverage showed me how to collect metrics, logs, and set up alarms. AWS X-Ray distributed tracing demonstrated how to debug complex microservices architectures. The full-stack observability demo showed how to connect all monitoring pieces together. Learning about alerting best practices and on-call processes provided practical operational knowledge. Best practices and case studies Deployment strategies like feature flags and A/B testing showed advanced techniques for safe deployments. Automated testing integration demonstrated how to build quality gates into CI/CD pipelines. Incident management practices and postmortem templates provided structure for handling production issues. Case studies from startups and enterprises showed real-world DevOps transformations and lessons learned. Career and certification guidance The DevOps career pathways discussion helped me understand different roles and skill requirements. The AWS certification roadmap provided clear guidance on certifications relevant to DevOps. Understanding the career progression gave me a roadmap for professional development. Practical demonstrations Every session included live demos that showed real implementations, not just slides. The full CI/CD pipeline walkthrough demonstrated end-to-end automation. CloudFormation and CDK demos showed both approaches to infrastructure management. Container deployment comparison helped me visualize different approaches side by side. Networking and discussions The full-day format allowed for extended networking with other DevOps practitioners. Q\u0026amp;A sessions provided opportunities to get answers to specific questions. Discussing real-world challenges with peers helped me understand common pitfalls and solutions. Lessons learned DevOps is a cultural transformation that requires buy-in from both development and operations teams. Automation is essential but must be implemented thoughtfully to avoid creating technical debt. Infrastructure as Code is non-negotiable for modern DevOps practices. Monitoring and observability are crucial for maintaining production systems. Start simple and iterate rather than trying to implement everything at once. Measure everything using DORA metrics to track improvement over time. Some event photos Overall, this full-day workshop provided me with comprehensive knowledge of AWS DevOps services and best practices. The combination of cultural transformation principles, practical tool demonstrations, and real-world case studies gave me confidence to implement DevOps practices in my projects. The depth and breadth of content covered everything from CI/CD pipelines to container orchestration and observability, providing a complete foundation for building DevOps capabilities on AWS.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Võ Trường Thành Phát\nPhone Number: 0707712750\nEmail: phatvttse171823@fpt.edu.vn\nUniversity: FPT University\nMajor: Software Engineer\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Overview This worklog documents my journey through the AWS Cloud Journey internship program, where I completed a comprehensive 12-week learning and hands-on project experience. The program was structured to progressively build knowledge from fundamental AWS concepts to deploying a complete, production-ready web application architecture on AWS.\nDuration: 12 weeks (approximately 3 months)\nCompletion Date: November 2025\nFinal Project: Production-ready AWS web application with CI/CD, monitoring, and security\nWeekly Progress Week 1: Getting familiar with AWS and basic AWS services - Introduction to AWS ecosystem, console navigation, and fundamental services.\nWeek 2: Basic AWS services exploration - Deep dive into core AWS services and their use cases.\nWeek 3: Advanced AWS concepts - Exploring advanced features and service integrations.\nWeek 4: Hands-on labs and practice - Practical exercises and lab sessions to reinforce learning.\nWeek 5: Advanced labs and workshops - Complex scenarios and multi-service integrations.\nWeek 6: Database Services on AWS - Amazon RDS, Aurora, Redshift, ElastiCache, and database migration tools (DMS, SCT).\nWeek 7: Analytics and Data Lake Services - Amazon DynamoDB, AWS Glue, Amazon Athena, Amazon QuickSight, and building data lakes on AWS.\nWeek 8: Edge Layer and Frontend Infrastructure - Route 53, S3 static hosting, CloudFront CDN, AWS WAF, and ACM Certificate setup.\nWeek 9: VPC and Networking Core - VPC creation, subnets, Internet Gateway, NAT Gateway, Security Groups, IAM roles, and VPC Flow Logs.\nWeek 10: Backend and Database Deployment - EC2 backend setup, RDS database configuration, API Gateway, Cognito authentication, and Auto Scaling Group.\nWeek 11: CI/CD Pipeline and Monitoring - GitLab integration, CodePipeline, CodeBuild, SSH-less deployment, CloudWatch monitoring, CloudTrail, and SNS alerts.\nWeek 12: Project finalization and documentation - Final testing, documentation, and project handover.\nChallenges and Solutions Throughout the 12-week program, I encountered several technical challenges that required problem-solving and deeper understanding:\nChallenge 1: CloudFront Origin Access Control (OAC) Configuration Issue: Initially confused between Origin Access Identity (OAI) and the newer Origin Access Control (OAC). The OAI method was deprecated, and I needed to use OAC for S3 bucket access.\nSolution: Researched AWS documentation and learned that OAC is the recommended approach. Updated S3 bucket policies to work with OAC and configured CloudFront distribution accordingly. This required understanding the difference in permission models between OAI and OAC.\nChallenge 2: API Gateway VPC Link Setup for Private Resources Issue: Connecting API Gateway to EC2 instances in private subnet was challenging. Initially tried direct HTTP integration, but private subnet resources are not directly accessible.\nSolution: Implemented API Gateway VPC Link to establish a connection between API Gateway and the VPC. This required creating a Network Load Balancer (NLB) in the private subnet and configuring the VPC Link to point to the NLB. Learned about the importance of VPC endpoints and private connectivity patterns.\nChallenge 3: RDS Connection from EC2 in Private Subnet Issue: EC2 instance in private subnet couldn\u0026rsquo;t connect to RDS database initially. Security group rules were not properly configured, and I wasn\u0026rsquo;t using the correct RDS endpoint.\nSolution:\nVerified Security Group rules: RDS Security Group must allow inbound from EC2 Security Group on database port (3306/5432). Used AWS Secrets Manager to securely retrieve database credentials instead of hardcoding. Tested connectivity using AWS Systems Manager Session Manager to access EC2 without SSH. Challenge 4: CodeBuild and CloudFront Cache Invalidation Issue: After deploying frontend updates via CodeBuild to S3, changes weren\u0026rsquo;t reflected immediately due to CloudFront caching. Manual cache invalidation was time-consuming.\nSolution: Automated CloudFront cache invalidation in the CodeBuild buildspec.yml file. Added AWS CLI command to create invalidation after S3 upload, ensuring users see updated content immediately after deployment.\nChallenge 5: SSH-less Deployment for Backend Issue: Traditional SSH-based deployment was not secure and didn\u0026rsquo;t work well with Auto Scaling Group (instances are ephemeral). Needed a better approach for automated backend deployments.\nSolution: Implemented SSH-less deployment using AWS Systems Manager (SSM) Run Command. This allowed CodeBuild to execute deployment scripts on EC2 instances without SSH keys. Alternative approach using CodeDeploy was also explored for more complex deployment scenarios.\nChallenge 6: CloudWatch Alarms Not Triggering Issue: Created CloudWatch alarms but notifications weren\u0026rsquo;t being received. Initially, the alarm threshold was too high, and SNS topic subscriptions weren\u0026rsquo;t properly configured.\nSolution:\nAdjusted alarm thresholds based on actual application metrics (CPU \u0026gt;80% for 5 minutes, RDS connections \u0026gt;80% of max). Verified SNS topic subscriptions (email confirmation was required). Tested alarms by manually triggering conditions to ensure the notification flow worked correctly. Challenge 7: Cognito JWT Token Validation in API Gateway Issue: After setting up Cognito User Pool and Authorizer in API Gateway, API calls with JWT tokens were being rejected with 401 Unauthorized errors.\nSolution:\nVerified JWT token format and expiration. Checked Cognito User Pool App Client settings (allowed OAuth flows, callback URLs). Ensured API Gateway Authorizer was correctly configured with the Cognito User Pool ARN. Tested token generation and validation flow step-by-step. Challenge 8: Auto Scaling Group Launch Template Issues Issue: Auto Scaling Group failed to launch instances. The Launch Template referenced an AMI that wasn\u0026rsquo;t available in the target Availability Zone.\nSolution:\nCreated base AMI in the same region and Availability Zone as the Auto Scaling Group. Verified Launch Template configuration (instance type, security groups, IAM role, user data). Tested Launch Template manually before using it in Auto Scaling Group. Ensured all required resources (Security Groups, IAM roles) existed before ASG creation. Challenge 9: VPC Flow Logs Cost Optimization Issue: VPC Flow Logs were generating large amounts of data, leading to high CloudWatch Logs costs.\nSolution:\nConfigured log retention policies (7 days for detailed logs, 30 days for aggregated logs). Used S3 as destination for long-term log storage (more cost-effective than CloudWatch Logs). Implemented log filtering to capture only relevant traffic patterns. Set up lifecycle policies on S3 to transition logs to cheaper storage classes. Challenge 10: End-to-End Testing Complexity Issue: Testing the complete flow from Route 53 → CloudFront → WAF → API Gateway → EC2 → RDS was complex, and issues were hard to isolate.\nSolution:\nImplemented comprehensive logging at each layer (CloudFront access logs, API Gateway logs, EC2 application logs, RDS slow query logs). Used CloudWatch dashboards to visualize the entire request flow. Created test scripts to validate each component independently before end-to-end testing. Documented troubleshooting procedures for common issues at each layer. Key Learnings Infrastructure as Code (IaC): Learned the importance of using CloudFormation for reproducible infrastructure deployments.\nSecurity Best Practices: Implemented least-privilege IAM policies, network segmentation, and secure credential management with Secrets Manager.\nMonitoring and Observability: Established comprehensive monitoring with CloudWatch, CloudTrail, and VPC Flow Logs for security and performance insights.\nCI/CD Automation: Automated deployment pipelines reduced manual errors and improved deployment speed.\nCost Optimization: Learned to balance performance, security, and cost through proper resource sizing, caching strategies, and log retention policies.\nConclusion This 12-week journey provided hands-on experience with a wide range of AWS services and best practices. The final project demonstrates a production-ready architecture with proper security, monitoring, automation, and scalability. The challenges encountered and resolved during this period have significantly strengthened my understanding of cloud architecture and AWS services.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.2-week2/","title":"Worklog Week 2","tags":[],"description":"","content":"Week 2 Objectives: Understand the concept and structure of VPC (CIDR, Subnet, Route Table, ENI). Learn how to configure firewalls in VPC (NACL, Security Group). Get familiar with networking services: VPN, Direct Connect. Study and practice Load Balancer. Practice creating and configuring core components: VPC, Subnet, Route Table, IGW, EBS, Elastic IP. Learn how to connect and remote into EC2 via SSH. Get hands-on with Hybrid DNS using Route 53 Resolver. Practice connecting multiple VPCs using VPC Peering. Deploy AWS Transit Gateway to manage inter-VPC connections. Tasks for this week: Day Task Start Date Completion Date Reference 2 - Learn theory\n- What is VPC and how to optimize cloud service usage 15/09/2025 15/09/2025 AWS VPC Documentation 3 - Learn about VPC\n+ Subnet, CIDR + Route table + ENI (Elastic Network Interface) 16/09/2025 16/09/2025 YouTube - AWS VPC 4 - Configure VPC firewalls: NACL, Security Group\n- VPN, Direct Connect\n- Load Balancer\n- Extra Resources 17/09/2025 17/09/2025 YouTube - AWS Security 5 - Hands-on: + VPC + Subnet\n+ Route Table\n+ IGW\n+ EBS\n+ \u0026hellip;\n- Remote SSH into EC2\n- Learn Elastic IP 18/09/2025 18/09/2025 AWS Study Group - 000003 6 - Hands-on: + Set up Hybrid DNS with Route 53 Resolver 19/09/2025 19/09/2025 AWS Study Group - 000010 7 - Hands-on: + Set up VPC Peering 19/09/2025 19/09/2025 AWS Study Group - 000019 8 - Hands-on: + Set up AWS Transit Gateway 19/09/2025 19/09/2025 AWS Study Group - 000020 Week 2 Goals: Build a strong understanding of Amazon VPC, focusing on its key components: CIDR blocks, subnets, route tables, and ENIs. Learn how to secure VPCs using both Security Groups and Network ACLs, and understand their differences in scope and use cases. Explore AWS networking services such as VPN and Direct Connect to understand options for hybrid and enterprise connectivity. Study Elastic Load Balancing and its role in distributing traffic for high availability. Gain hands-on practice with VPC essentials: creating subnets, configuring route tables, attaching internet gateways, working with EBS, and managing Elastic IPs. Strengthen skills in accessing and managing EC2 instances securely via SSH. Implement advanced networking scenarios, including Hybrid DNS resolution with Route 53 Resolver. Practice connecting multiple VPCs through VPC Peering. Deploy AWS Transit Gateway to design and manage scalable multi-VPC architectures. Direction: The goal of Week 2 was not only to expand theoretical knowledge of AWS networking but also to translate that knowledge into practical experience, laying a foundation for building and managing complex cloud infrastructures.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/4-eventparticipated/4.6-event6/","title":"Event 6","tags":[],"description":"","content":"Summary Report: \u0026ldquo;Kick-off AWS First Cloud Journey Workforce OJT FALL 2025\u0026rdquo; Event Objectives Welcome and kick-off the AWS First Cloud Journey Workforce OJT FALL 2025 program Introduce the program and future direction in Cloud Computing Share experiences from alumni and industry experts Connect students with AWS community and partner enterprises Inspire and motivate for the learning journey and career development Event Details Date: Saturday, September 6, 2025 Time: 8:30 AM – 12:00 PM Location: 26th Floor, Bitexco Financial Tower, 2 Hải Triều, P. Bến Nghé, Quận 1, TP.HCM Duration: 3.5 hours (including tea break and networking) About AWS First Cloud Journey Workforce Program Launched in 2021, the program has accompanied over 2,000 students across the country.\nMore than 150 students have received intensive training and are currently working at leading technology companies in Vietnam and internationally.\nMain Objectives:\nBuild a high-quality generation of AWS Builders for Vietnam Equip practical skills in Cloud, DevOps, AI/ML, Security, Data \u0026amp; Analytics Connect students with AWS Study Group community of 47,000+ members and AWS partner enterprises The program is not just technology training, but also an important bridge between knowledge – technology – career, helping students confidently integrate into the modern technology world and international integration.\nAgenda 8:30 – 9:00 AM | Welcome \u0026amp; Check-in Networking \u0026amp; commemorative photos Registration and document collection Meeting fellow students 9:00 – 9:15 AM | Opening \u0026amp; Welcome School Representative: Mr. Nguyễn Trần Phước Bảo – Head of Enterprise Relations (QHDN)\nOpening speech for the program Introduction to AWS First Cloud Journey Workforce Orientation and expectations for the course Attended by 2–3 staff members from QHDN Department Keynote \u0026amp; Industry Sharing 9:15 – 9:40 AM | AWS First Cloud Journey \u0026amp; Future Direction (25 minutes) Nguyễn Gia Hưng – Head of Solutions Architect, AWS Vietnam\nIntroduction to AWS First Cloud Journey Vision and future direction of the program Career opportunities in Cloud Computing Career development path with AWS Q\u0026amp;A 9:40 – 10:05 AM | DevOps \u0026amp; Future Career (25 minutes) Đỗ Huy Thắng – DevOps Lead, VNG\nWhat is DevOps and why it matters Career in DevOps field Required skills and how to develop them Real-world experience from VNG Q\u0026amp;A 10:05 – 10:20 AM | Tea Break \u0026amp; Networking (15 minutes) Break time Networking with speakers and participants Commemorative photos Alumni \u0026amp; Career Sharing 10:20 – 10:40 AM | From First Cloud Journey to GenAI Engineer (20 minutes) Danh Hoàng Hiếu Nghị – GenAI Engineer, Renova\nJourney from First Cloud Journey to GenAI Engineer Learning and development experience Opportunities in AI/ML field Advice for new students Q\u0026amp;A 10:40 – 11:00 AM | She in Tech \u0026amp; Journey with First Cloud Journey (20 minutes) Bùi Hồ Linh Nhi – AI Engineer, SoftwareOne\nWomen\u0026rsquo;s journey in technology Experience participating in First Cloud Journey Challenges and opportunities Advice for women wanting to pursue tech career Q\u0026amp;A 11:00 – 11:20 AM | A Day as Cloud Engineer (20 minutes) Phạm Nguyễn Hải Anh – Cloud Engineer, G-Asia Pacific\nA day in the life of a Cloud Engineer Real work and responsibilities Skills and tools used daily Challenges and solutions Q\u0026amp;A 11:20 – 11:40 AM | Journey to First Cloud Journey (20 minutes) Nguyễn Đồng Thanh Hiệp – Principal Cloud Engineer, G-Asia Pacific\nPersonal journey to First Cloud Journey Valuable lessons and experiences Development path from junior to principal engineer Advice for beginners Q\u0026amp;A 11:40 AM – 12:00 PM | Q\u0026amp;A \u0026amp; Wrap-up (20 minutes) Answering questions from speakers \u0026amp; mentors Summary of main event content Information about next steps in the program Commemorative photos 📸 Meet Our Speakers Nguyễn Gia Hưng Head of Solutions Architect, AWS Vietnam\nLeading expert in AWS architecture and solutions Extensive experience in consulting and implementing cloud solutions Leader of AWS First Cloud Journey program in Vietnam Đỗ Huy Thắng DevOps Lead, VNG\nExpert in DevOps practices and automation Experience in building and operating large-scale systems Mentor for many generations of DevOps engineers Danh Hoàng Hiếu Nghị GenAI Engineer, Renova\nAlumni of First Cloud Journey program Expert in Generative AI and Machine Learning Experience in developing AI solutions Bùi Hồ Linh Nhi AI Engineer, SoftwareOne\nAlumni of First Cloud Journey program Expert in AI/ML engineering Inspiration for women in technology Phạm Nguyễn Hải Anh Cloud Engineer, G-Asia Pacific\nExpert in cloud infrastructure and operations Real-world experience in managing and operating cloud systems Mentor for new cloud engineers Nguyễn Đồng Thanh Hiệp Principal Cloud Engineer, G-Asia Pacific\nAlumni of First Cloud Journey program Expert in cloud architecture and best practices Experience from junior to principal level Key Highlights AWS First Cloud Journey Program Program Introduction:\nOn-the-job training (OJT) program on AWS Cloud Computing Has accompanied over 2,000 students since 2021 More than 150 students have received intensive training and work at leading companies Objectives:\nBuild a high-quality generation of AWS Builders Equip practical skills in Cloud, DevOps, AI/ML, Security, Data \u0026amp; Analytics Connect with AWS Study Group community of 47,000+ members Benefits:\nIntensive training with AWS experts Opportunity to practice with real AWS environments Connection with partner enterprises Career development support Career Pathways in Cloud Computing DevOps Career:\nWhat DevOps is and its role in organizations Required skills: CI/CD, Infrastructure as Code, Monitoring Development path: Junior → Mid → Senior → Lead Career opportunities and salary ranges Cloud Engineer Career:\nDaily work of a Cloud Engineer Technical and soft skills Challenges and solutions Growth opportunities AI/ML Engineer Career:\nFrom Cloud Engineer to AI/ML Engineer Required skills for AI/ML Opportunities in Generative AI field Future trends and opportunities Alumni Success Stories From First Cloud Journey to Success:\nJourneys of alumni Valuable lessons and experiences Challenges and how to overcome them Advice for newcomers Diversity in Tech:\nWomen\u0026rsquo;s journey in technology Challenges and opportunities Support and resources available Inspiration and motivation Key Takeaways Understanding AWS First Cloud Journey What the program is: OJT program on AWS Cloud Computing Objectives and benefits: Build practical skills and connect with community Learning path: Stages and milestones in the program Career opportunities: Connection with partner enterprises Career Pathways DevOps: Understanding DevOps career and required skills Cloud Engineer: Real work and responsibilities AI/ML Engineer: Development path in AI/ML field Career progression: From junior to senior and principal level Real-world Insights Alumni experiences: Journeys and lessons from alumni Day-to-day work: Real work of different roles Challenges: Challenges and solutions Best practices: Advice and tips from experts Networking and Community AWS Study Group: Community of 47,000+ members Mentorship: Opportunity to be mentored by experts Peer learning: Learning from fellow students Industry connections: Connection with enterprises Applying to Work Set clear goals: Identify the career path you want to pursue Build skills: Focus on skills needed for your chosen career path Practice regularly: Utilize AWS environment to practice Connect and network: Join community and connect with mentors Learn from alumni: Apply lessons learned from successful alumni Develop soft skills: Not just technical skills but also communication, teamwork Event Experience Attending the Kick-off AWS First Cloud Journey Workforce OJT FALL 2025 event was an inspiring and motivating experience. The event not only provided information about the program but also inspired the learning journey and career development in Cloud Computing.\nOpening and Welcome Opening speech by Mr. Nguyễn Trần Phước Bảo created a formal and professional atmosphere. I understood clearly about the program\u0026rsquo;s objectives and expectations. Introduction to First Cloud Journey helped me visualize the upcoming journey. Keynote from AWS Session by Nguyễn Gia Hưng provided an overall vision of AWS First Cloud Journey. Understanding future direction and career opportunities in Cloud Computing. Career development path gave me a clear roadmap to follow. Industry Insights DevOps session by Đỗ Huy Thắng helped me understand DevOps career. Learning about required skills and how to develop in this field. Real-world examples from VNG helped me visualize actual work. Alumni Success Stories Journey of Danh Hoàng Hiếu Nghị from First Cloud Journey to GenAI Engineer was very inspiring. I learned about persistence and continuous learning. Session by Bùi Hồ Linh Nhi about She in Tech was very empowering. Understanding challenges and opportunities for women in tech. Day-to-day Work Insights Session by Phạm Nguyễn Hải Anh about a day as Cloud Engineer was very practical. I understood the actual work and responsibilities of a Cloud Engineer. Journey of Nguyễn Đồng Thanh Hiệp from junior to principal was very motivating. Learning about career progression and growth mindset. Networking and Connections Networking sessions allowed me to connect with speakers and participants. Sharing experiences and learnings with fellow students. Q\u0026amp;A sessions provided opportunities to ask specific questions. Meeting alumni and receiving advice about career development. Lessons Learned First Cloud Journey is a great opportunity: The program provides a solid foundation for cloud career. Diverse career paths: There are many development paths in cloud computing. Continuous learning is key: Need to keep learning to develop. Networking matters: Connecting with community and mentors is very important. Diversity and inclusion: Tech industry is becoming more inclusive. Set clear goals: Need clear goals and plan to achieve them. Some event photos Overall, this Kick-off event was an excellent start to the AWS First Cloud Journey. The combination of program information, career insights, and alumni success stories gave me motivation and clear direction. Particularly, networking with speakers, alumni, and participants provided valuable connections and support for my learning journey and career development in Cloud Computing.\nClosing Remarks Today\u0026rsquo;s Kick-off event is the starting point for the AWS Builders journey – where students not only access the most advanced cloud computing technology but also get inspired, connect with experts, and expand career opportunities.\nOnce again, congratulations to all students who have officially become part of AWS First Cloud Journey Workforce OJT FALL 2025. Let\u0026rsquo;s start this new journey together – a journey of learning, building, and developing, to take cloud computing technology in Vietnam far! 🚀\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.2-prerequisite/","title":"Prerequisites &amp; Preparation","tags":[],"description":"","content":"System Requirements 1. AWS Account Active AWS account Administrator access or the following permissions: CloudFormation: Full access EC2: Full access VPC: Full access RDS: Full access S3: Full access CloudFront: Full access IAM: Create roles and policies CloudWatch: Full access 2. AWS CLI Install and configure AWS CLI:\nWindows:\n# Download and install from: https://aws.amazon.com/cli/ # Or use chocolatey: choco install awscli # Verify installation aws --version Linux/Mac:\n# Using pip pip install awscli # Or use package manager # Ubuntu/Debian sudo apt-get install awscli # MacOS brew install awscli # Verify installation aws --version Configure AWS CLI:\naws configure # AWS Access Key ID: \u0026lt;your-access-key\u0026gt; # AWS Secret Access Key: \u0026lt;your-secret-key\u0026gt; # Default region name: ap-southeast-1 # Default output format: json 3. EC2 Key Pair Create an EC2 Key Pair for SSH access:\nVia AWS Console:\nOpen EC2 Console Select region ap-southeast-1 (Singapore) Go to Network \u0026amp; Security → Key Pairs Click Create key pair Name: workshop-aws-key Key pair type: RSA Private key file format: .pem (Linux/Mac) or .ppk (Windows/PuTTY) Click Create key pair Save the .pem file securely Via AWS CLI:\n# Create key pair aws ec2 create-key-pair \\ --key-name workshop-aws-key \\ --query \u0026#39;KeyMaterial\u0026#39; \\ --output text \\ --region ap-southeast-1 \u0026gt; workshop-aws-key.pem # Set permissions (Linux/Mac only) chmod 400 workshop-aws-key.pem 4. Development Tools Java Development Kit (JDK) 17:\n# Windows (chocolatey) choco install openjdk17 # Linux (Ubuntu/Debian) sudo apt-get install openjdk-17-jdk # MacOS brew install openjdk@17 # Verify java -version Maven:\n# Windows choco install maven # Linux sudo apt-get install maven # MacOS brew install maven # Verify mvn -version Node.js and npm:\n# Windows choco install nodejs # Linux curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash - sudo apt-get install -y nodejs # MacOS brew install node # Verify node --version npm --version Prepare Project Files 1. Clone or Download Project # If you have Git repository git clone \u0026lt;your-repo-url\u0026gt; cd aws_project # Or download and extract ZIP file 2. Project Structure aws_project/\r├── aws/\r│ ├── infrastructure.yaml # Main CloudFormation template\r│ ├── cicd-pipeline.yaml # CI/CD pipeline (optional)\r│ ├── parameters.json # Stack parameters\r│ ├── deploy.bat # Deploy script (Windows)\r│ ├── deploy.sh # Deploy script (Linux/Mac)\r│ └── README.md # Detailed instructions\r├── BE/\r│ └── workshop_BE/\r│ ├── src/ # Backend source code\r│ ├── pom.xml # Maven configuration\r│ └── README.md\r└── FE/\r├── src/ # Frontend source code\r├── package.json # npm dependencies\r└── README.md 3. Configure Parameters Open aws/parameters.json and update values:\n[ { \u0026#34;ParameterKey\u0026#34;: \u0026#34;ProjectName\u0026#34;, \u0026#34;ParameterValue\u0026#34;: \u0026#34;workshop-aws\u0026#34; }, { \u0026#34;ParameterKey\u0026#34;: \u0026#34;Environment\u0026#34;, \u0026#34;ParameterValue\u0026#34;: \u0026#34;dev\u0026#34; }, { \u0026#34;ParameterKey\u0026#34;: \u0026#34;KeyPairName\u0026#34;, \u0026#34;ParameterValue\u0026#34;: \u0026#34;workshop-aws-key\u0026#34; // ⚠️ Replace with your key pair name }, { \u0026#34;ParameterKey\u0026#34;: \u0026#34;DBPassword\u0026#34;, \u0026#34;ParameterValue\u0026#34;: \u0026#34;YourStrongPassword123!\u0026#34; // ⚠️ Use a strong password }, { \u0026#34;ParameterKey\u0026#34;: \u0026#34;InstanceType\u0026#34;, \u0026#34;ParameterValue\u0026#34;: \u0026#34;t3.micro\u0026#34; }, { \u0026#34;ParameterKey\u0026#34;: \u0026#34;DBInstanceClass\u0026#34;, \u0026#34;ParameterValue\u0026#34;: \u0026#34;db.t3.micro\u0026#34; } ] Important notes:\nKeyPairName: Must match your created key pair name DBPassword: Minimum 8 characters with uppercase, lowercase, numbers, and special characters Don\u0026rsquo;t commit this file with real passwords to Git 4. Update AMI ID The CloudFormation template uses a default AMI ID. Update it for your region:\nFind AMI ID:\n# Find Amazon Linux 2023 AMI for ap-southeast-1 aws ec2 describe-images \\ --owners amazon \\ --filters \u0026#34;Name=name,Values=al2023-ami-*-x86_64\u0026#34; \\ --query \u0026#39;Images | sort_by(@, \u0026amp;CreationDate) | [-1].[ImageId,Name,CreationDate]\u0026#39; \\ --region ap-southeast-1 \\ --output table Update in infrastructure.yaml:\nFind line ~530:\nLaunchTemplate: Properties: LaunchTemplateData: ImageId: ami-0c55b159cbfafe1f0 # ⚠️ Replace with your AMI ID Preparation Checklist Ensure you have completed all the following steps:\nAWS account configured AWS CLI installed and configured (aws configure) EC2 Key Pair created Java 17 installed Maven installed Node.js and npm installed Project files downloaded parameters.json file updated AMI ID updated in infrastructure.yaml Validate AWS Credentials # Check AWS credentials aws sts get-caller-identity # Expected output: # { # \u0026#34;UserId\u0026#34;: \u0026#34;AIDAXXXXXXXXXXXXXXXXX\u0026#34;, # \u0026#34;Account\u0026#34;: \u0026#34;123456789012\u0026#34;, # \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/your-username\u0026#34; # } Validate CloudFormation Template cd aws aws cloudformation validate-template \\ --template-body file://infrastructure.yaml \\ --region ap-southeast-1 If successful, you\u0026rsquo;ll see output with template parameters and outputs information.\nCost Estimation Before deployment, understand the costs:\nService Instance Type Cost/month (USD) EC2 t3.nano $3.50 RDS MySQL db.t3.micro $2.80 API Gateway - $0.50 S3 + CloudFront - $0.80 Route 53 - $0.50 Cognito - $0.10 CloudWatch - $0.30 CI/CD (CodePipeline) - $0.40 Total $8.90 For workshop (2-3 hours): ~$0.50-1.00\nNote:\nCosts apply to ap-southeast-1 region Use AWS Free Tier if account is eligible NAT Gateway (~$32/month) can be disabled to save costs Next Steps After completing all preparation steps, you\u0026rsquo;re ready to:\n➡️ Deploy Infrastructure with CloudFormation\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.12-week12/","title":"Worklog Week 12","tags":[],"description":"","content":"Week 12 Objectives: Present the team\u0026rsquo;s project to mentors and supervisors.\nCollect feedback for improvement on architecture, implementation, and presentation quality.\nIdentify areas that need adjustment before the final submission.\nDocument all comments and action items for the next sprint.\nTasks to be carried out this week: Day Tasks Start Date Completion Date Resources Used 2 - Prepare final slide deck - Review speaking roles for each member 24/11/2025 24/11/2025 Slide deck 3 - Conduct internal rehearsal session - Adjust timing and transitions between presenters 25/11/2025 25/11/2025 Internal notes 4 - Official project presentation in front of mentors - Present system architecture, CI/CD pipeline, cost estimation, and demo 26/11/2025 26/11/2025 Presentation materials 5 - Receive mentor feedback on technical design, security considerations, and deployment approach - Record all comments for follow-up 27/11/2025 27/11/2025 Mentor feedback 6 - Analyze received feedback - Identify improvements required for architecture, diagrams, and slide clarity 28/11/2025 28/11/2025 Consolidated feedback 7 - Update documents and adjust the slide deck according to mentor comments - Prepare follow-up action plan for next week 29–30/11/2025 30/11/2025 Proposal, Slide deck Week 12 Achievements: Successfully presented the project to mentors and received detailed feedback on architecture, implementation, and presentation quality.\nDocumented all comments related to:\nSystem architecture improvements\nClarification of data flow and networking layers\nCI/CD pipeline explanation\nCost optimization suggestions\nSecurity considerations\nImproved the slide deck and project documentation based on mentor recommendations.\nIdentified key action items to refine before the final submission.\nCompleted rehearsal and delivery of the mid-stage presentation as planned.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/2-proposal/","title":"Proposal","tags":[],"description":"","content":"In this section, you need to summarize the contents of the workshop that you plan to conduct.\nBlood Donation Support System AWS Solution for Blood Donation Support Software 1. Executive Summary Blood Donation Support System (BDSS) is a web platform that supports the management and connection of blood donors with medical facilities. The project was developed by a group of students in Ho Chi Minh City to optimize the blood donation process, reduce the burden of searching for donors and improve the efficiency of medical communication.\nThe system is built on AWS Cloud architecture, using Amazon EC2, Amazon RDS, API Gateway, Cognito and CI/CD Pipeline (GitLab + CodePipeline) for automatic deployment. BDSS supports four user groups (Guest, Member, Staff, Admin), providing features for searching, registering for blood donation, managing blood banks, tracking blood donation processes and visual reporting.\n2. Problem Statement What’s the Problem? Healthcare facilities currently manage blood donation processes manually or through disparate tools. Finding donors who match blood type or region is difficult, especially in emergency situations. In addition, data storage systems are not synchronized, making it difficult to analyze, report and optimize blood donation campaigns.\nThe Solution Developed a comprehensive blood donation support platform on AWS Cloud, with functions for blood donation management, searching for donors and blood needers by blood type or geographic location, integrating user authentication via Amazon Cognito, and data governance on Amazon RDS. The frontend was deployed via Route 53 + CloudFront, the backend via API Gateway – EC2, a MySQL database on Amazon RDS, and an automated CI/CD pipeline using GitLab – CodePipeline.\nBenefits and Return on Investment Reduce the time it takes to find a matching donor by 60–70%. Increase the accuracy of blood type and location information. Optimize operating costs with a flexible, pay-as-you-go cloud architecture. Improve response to blood emergencies\n3. Solution Architecture The platform employs a comprehensive AWS cloud architecture to support blood donation management, connecting donors with medical facilities efficiently. The system integrates multiple AWS services to provide a scalable, secure, and cost-effective solution. The architecture is detailed below:\nThe system is divided into 4 main layers:\nEdge Networking Layer: Route 53 manages domain and DNS routing. CloudFront increases page loading speed and delivers static content. AWS WAF protects against web attacks (SQL injection, DDoS).\nApplication \u0026amp; Data Layer: Amazon EC2: Deploys backend API and handles main business. Amazon RDS (MySQL): Stores blood donor data, blood types, donation history. API Gateway: Communicates between frontend and backend. Elastic Load Balancer (ELB): Distributes load to EC2 instances. NAT Gateway \u0026amp; Internet Gateway: Supports secure Internet connection.\nCI/CD \u0026amp; DevOps Layer: GitLab: Source code management. AWS CodePipeline, CodeBuild: Deploy and update automatically.\nMonitoring \u0026amp; Security Layer: Amazon Cognito: Authentication and authorization (Guest, Member, Staff, Admin). CloudWatch, CloudTrail, IAM, Secrets Manager: Monitoring, security, system alerts. SNS: Send notifications when there is an event (blood emergency, suitable donor).\n4. Technical Implementation Implementation Phases\nAnalysis \u0026amp; Design (January) Gather requirements, define use cases, design ERD and AWS architecture. Infrastructure \u0026amp; Pipeline Setup (February) Configure Route 53, CloudFront, EC2, RDS and CI/CD on AWS. Development \u0026amp; Testing (March-April) Build main modules: blood donation registration, search, blood bank management. Integrate Cognito and SNS alert system. Deployment \u0026amp; Operation (May) Deploy the official product and monitor with CloudWatch. Key Technical Requirements: Frontend: React/Next.js or Angular (deploy via S3/CloudFront). Backend: Spring Boot on EC2, communicate via REST API Gateway. Database: Amazon RDS MySQL, optimize queries and periodic backups. CI/CD: GitLab → CodeBuild → CodePipeline → EC2. Auth: Cognito (4 roles: Guest, Member, Staff, Admin). Alert \u0026amp; Logs: SNS + CloudWatch + CloudTrail.\n5. Timeline \u0026amp; Milestones Timeline Phase Key Results Month 1 Requirements analysis \u0026amp; design AWS architecture + use case diagram Month 2 Infrastructure \u0026amp; pipeline setup EC2, RDS, API Gateway operational Month 3–4 Development \u0026amp; testing Key modules finalized Month 5 Live deployment System stable, with Dashboard reporting 6. Budget Estimation Services Estimated Cost/Month (USD) Notes EC2 (t3.nano) 3.50 Backend REST API Amazon RDS (MySQL) 2.80 20 GB storage API Gateway 0.50 5,000 requests CloudFront + S3 0.80 Website + CDN Route 53 0.50 Domain \u0026amp; DNS Cognito 0.10 \u0026lt;100 users CloudWatch + Logs 0.30 Monitoring \u0026amp; Alerting CI/CD (CodePipeline, CodeBuild) 0.40 Automated Deployment Total 8.9 USD/month ~106.8 USD/year Total costs may vary based on AWS Free Tier or spot instance usage.\n7. Risk Assessment Risk Impact Probability Mitigation Internet Outage Medium Medium Redundancy on EC2 Backup DDoS Attack High Low AWS WAF + CloudFront User Data Corruption High Low RDS Backup + IAM Restricted Access Cost Overrun Medium Low AWS Budget Alert CI/CD Deployment Disruption Low Medium Pipeline Testing Before Merging 8. Expected Outcomes Technology: Cloud-native system, automatic CI/CD, multi-user support and high security. Application: Helps medical facilities manage blood donations effectively, minimizing manual processes. Expansion: Can be replicated for many other hospitals, integrating AI to analyze blood group needs or predict upcoming blood donations.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nSteps:\nTick 2 acknowledgement boxes Choose Create stack Wait for CloudFormation deployment (about 15 minutes) Resources created:\n2 VPCs 3 EC2 instances "},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.3-deploy-infrastructure/","title":"Deploy Infrastructure with CloudFormation","tags":[],"description":"","content":"Overview In this step, you will deploy the complete AWS infrastructure using CloudFormation template. The template will create VPC, subnets, EC2 instances, RDS database, Load Balancer, S3 buckets, CloudFront distribution, and all necessary resources.\nValidate Template Before deployment, validate the template to ensure no syntax errors:\ncd aws aws cloudformation validate-template \\ --template-body file://infrastructure.yaml \\ --region ap-southeast-1 Expected result: Information about parameters, outputs, and template description.\nDeploy Stack Method 1: Using Deploy Script (Recommended) Windows:\ncd aws deploy.bat create Linux/Mac:\ncd aws chmod +x deploy.sh ./deploy.sh create The script will automatically:\nValidate template Create CloudFormation stack Monitor deployment progress Display outputs when complete Method 2: Using AWS CLI aws cloudformation create-stack \\ --stack-name workshop-aws-dev \\ --template-body file://infrastructure.yaml \\ --parameters file://parameters.json \\ --capabilities CAPABILITY_NAMED_IAM \\ --region ap-southeast-1 Monitor Progress Via AWS Console Open CloudFormation Console Select stack workshop-aws-dev Events tab: View resources being created Resources tab: View resource list Outputs tab: View outputs (after completion) Via AWS CLI # Check status aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].StackStatus\u0026#39; # View events aws cloudformation describe-stack-events \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --max-items 10 Deployment Time Stack creation takes approximately 15-20 minutes:\nVPC and Networking: 2-3 minutes NAT Gateway: 2-3 minutes RDS Database: 5-7 minutes EC2 Auto Scaling Group: 3-5 minutes Load Balancer: 2-3 minutes CloudFront Distribution: 5-10 minutes VPC Endpoints: 2-3 minutes View Outputs After stack creation succeeds (Status: CREATE_COMPLETE), get outputs:\naws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs\u0026#39; \\ --output table Important Outputs:\nOutput Key Description Example VPCId VPC ID vpc-0123456789abcdef0 FrontendBucketName S3 bucket for frontend workshop-aws-dev-frontend-123456789012-ap-southeast-1 CloudFrontDomainName CloudFront URL d1234567890abc.cloudfront.net ALBDNSName Load Balancer DNS workshop-aws-dev-alb-123456789.ap-southeast-1.elb.amazonaws.com RDSEndpoint Database endpoint workshop-aws-dev-db.xxxxx.ap-southeast-1.rds.amazonaws.com APIGatewayURL API Gateway URL https://xxxxx.execute-api.ap-southeast-1.amazonaws.com/dev CognitoUserPoolId Cognito User Pool ID ap-southeast-1_xxxxxxxxx Save these values - you\u0026rsquo;ll need them for next steps!\nVerify Created Resources 1. VPC and Networking # Get VPC ID VPC_ID=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`VPCId`].OutputValue\u0026#39; \\ --output text) # View VPC details aws ec2 describe-vpcs --vpc-ids $VPC_ID --region ap-southeast-1 # View Subnets aws ec2 describe-subnets \\ --filters \u0026#34;Name=vpc-id,Values=$VPC_ID\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;Subnets[*].[SubnetId,CidrBlock,AvailabilityZone,Tags[?Key==`Name`].Value|[0]]\u0026#39; \\ --output table 2. EC2 Instances # View EC2 instances in Auto Scaling Group aws ec2 describe-instances \\ --filters \u0026#34;Name=tag:aws:cloudformation:stack-name,Values=workshop-aws-dev\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;Reservations[*].Instances[*].[InstanceId,State.Name,PrivateIpAddress,PublicIpAddress]\u0026#39; \\ --output table 3. RDS Database # View RDS instance aws rds describe-db-instances \\ --db-instance-identifier workshop-aws-dev-db \\ --region ap-southeast-1 \\ --query \u0026#39;DBInstances[0].[DBInstanceIdentifier,DBInstanceStatus,Endpoint.Address,Endpoint.Port]\u0026#39; \\ --output table Troubleshooting Stack Creation Failed If stack creation fails:\nView Events to find error: aws cloudformation describe-stack-events \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;StackEvents[?ResourceStatus==`CREATE_FAILED`].[LogicalResourceId,ResourceStatusReason]\u0026#39; \\ --output table Common errors: Error: \u0026ldquo;Key pair does not exist\u0026rdquo;\nCheck key pair name in parameters.json Ensure key pair exists in ap-southeast-1 region Error: \u0026ldquo;Invalid AMI ID\u0026rdquo;\nUpdate AMI ID in infrastructure.yaml Use AMI ID appropriate for your region Error: \u0026ldquo;Insufficient permissions\u0026rdquo;\nCheck IAM permissions of user Need CloudFormation, EC2, VPC, RDS, S3, IAM permissions Rollback and retry: # Delete failed stack aws cloudformation delete-stack \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 # Wait for stack deletion aws cloudformation wait stack-delete-complete \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 # Try creating again aws cloudformation create-stack \\ --stack-name workshop-aws-dev \\ --template-body file://infrastructure.yaml \\ --parameters file://parameters.json \\ --capabilities CAPABILITY_NAMED_IAM \\ --region ap-southeast-1 Confirm Successful Deployment Checklist to confirm infrastructure is ready:\nStack status is CREATE_COMPLETE VPC and subnets created EC2 instances running (State: running) RDS database status is available Load Balancer status is active S3 buckets created CloudFront distribution status is Deployed All outputs have values Next Steps After infrastructure is ready, you can:\n➡️ Configure and Deploy Backend Application\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Accelerating Enterprise ML Experimentation with Amazon SageMaker AI and Comet This blog introduces how to integrate Amazon SageMaker AI with Comet to accelerate enterprise machine learning experimentation. You will learn how to manage ML experiments, track model lineage, and ensure reproducibility in production environments. The article demonstrates a complete fraud detection workflow using SageMaker AI + Comet, showcasing experiment tracking, model comparison, and audit-ready logging that modern enterprises require. It covers both administrator and user journeys, from setting up the Comet Partner AI App to running experiments and comparing results in the Comet UI.\nBlog 2 - Using Apache Airflow workflows to orchestrate data processing on Amazon SageMaker Unified Studio This blog demonstrates how to use Apache Airflow workflows to orchestrate data processing pipelines on Amazon SageMaker Unified Studio. You will learn how to build, test, and run end-to-end ML pipelines using SageMaker workflows through the Unified Studio interface. The article walks through a practical example that includes ingesting weather and taxi data, transforming and merging datasets, then using ML to predict taxi fares - all orchestrated through SageMaker Unified Studio workflows powered by Amazon Managed Workflows for Apache Airflow (Amazon MWAA). It focuses on the code-based approach using Python DAGs for workflow management.\nBlog 3 - Migrating full-text search from SQL Server to Amazon Aurora PostgreSQL-Compatible Edition or Amazon RDS for PostgreSQL This blog guides you on how to migrate full-text search from SQL Server to Amazon Aurora PostgreSQL or Amazon RDS for PostgreSQL. You will learn how to migrate FTS queries and schema structures, as the implementation differs between the two systems. The article demonstrates how to use PostgreSQL\u0026rsquo;s tsvector and tsquery data types to achieve similar FTS functionality, and also shows how to implement FTS using the pg_trgm and pg_bigm extensions. It covers various use cases including CONTAINS predicates, FREETEXT queries, ranking with RANK, and performance optimization techniques using GIN indexes and stored generated columns.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.4-deploy-backend/","title":"Deploy Backend Application","tags":[],"description":"","content":"Overview In this step, you will build and deploy the Spring Boot backend application to EC2 instances. The backend provides RESTful API for DNA analysis, user authentication, and data management.\nStep 1: Configure Database Connection Get RDS endpoint from CloudFormation outputs:\nRDS_ENDPOINT=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`RDSEndpoint`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;RDS Endpoint: $RDS_ENDPOINT\u0026#34; Update file BE/workshop_BE/src/main/resources/application.properties:\n# Database Configuration spring.datasource.url=jdbc:mysql://${RDS_ENDPOINT}:3306/workshop_aws?useSSL=true\u0026amp;requireSSL=false\u0026amp;allowPublicKeyRetrieval=true\u0026amp;serverTimezone=Asia/Ho_Chi_Minh spring.datasource.username=admin spring.datasource.password=YourStrongPassword123! spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver # Connection Pool spring.datasource.hikari.maximum-pool-size=10 spring.datasource.hikari.minimum-idle=5 spring.datasource.hikari.connection-timeout=20000 # JPA Configuration spring.jpa.hibernate.ddl-auto=update spring.jpa.show-sql=false spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL8Dialect # Server Configuration server.port=8080 server.servlet.context-path=/dna_service # JWT Configuration jwt.signerKey=2VJ50pdhYm96e4VECp/vsZGVmkSl9xp1rSYAZKsZL7n9Ti1pZYle3k9mheQEKt6+ jwt.expiration=86400000 # CORS Configuration cors.allowed.origins=* # Logging logging.level.root=INFO logging.level.aws_project.workshop=DEBUG logging.file.name=/opt/workshop/application.log Step 2: Build Backend JAR cd BE/workshop_BE # Clean and build mvn clean package -DskipTests # Or use Maven Wrapper ./mvnw clean package -DskipTests # Verify JAR file ls -lh target/workshop-0.0.1-SNAPSHOT.jar Expected result: JAR file approximately 50-80MB in target/ directory\nStep 3: Upload JAR to S3 Create S3 bucket for backend artifacts (if not exists):\nACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) BACKEND_BUCKET=\u0026#34;workshop-aws-dev-backend-${ACCOUNT_ID}-ap-southeast-1\u0026#34; # Create bucket aws s3 mb s3://${BACKEND_BUCKET} --region ap-southeast-1 # Upload JAR aws s3 cp target/workshop-0.0.1-SNAPSHOT.jar \\ s3://${BACKEND_BUCKET}/jars/ \\ --region ap-southeast-1 # Verify upload aws s3 ls s3://${BACKEND_BUCKET}/jars/ Step 4: Deploy to EC2 Get EC2 Instance ID INSTANCE_ID=$(aws ec2 describe-instances \\ --filters \u0026#34;Name=tag:aws:cloudformation:stack-name,Values=workshop-aws-dev\u0026#34; \\ \u0026#34;Name=instance-state-name,Values=running\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;Reservations[0].Instances[0].InstanceId\u0026#39; \\ --output text) echo \u0026#34;Instance ID: $INSTANCE_ID\u0026#34; Connect to EC2 via Session Manager aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 On EC2 Instance, run the following commands: # Switch to ec2-user sudo su - ec2-user # Navigate to application directory cd /opt/workshop # Download JAR from S3 ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) BACKEND_BUCKET=\u0026#34;workshop-aws-dev-backend-${ACCOUNT_ID}-ap-southeast-1\u0026#34; aws s3 cp s3://${BACKEND_BUCKET}/jars/workshop-0.0.1-SNAPSHOT.jar . \\ --region ap-southeast-1 # Verify file ls -lh workshop-0.0.1-SNAPSHOT.jar Create Application Properties cat \u0026gt; application.properties \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; spring.application.name=workshop-aws # Database Configuration spring.datasource.url=jdbc:mysql://REPLACE_WITH_RDS_ENDPOINT:3306/workshop_aws?useSSL=true\u0026amp;requireSSL=false\u0026amp;allowPublicKeyRetrieval=true\u0026amp;serverTimezone=Asia/Ho_Chi_Minh spring.datasource.username=admin spring.datasource.password=YourStrongPassword123! spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver # Connection Pool spring.datasource.hikari.maximum-pool-size=10 spring.datasource.hikari.minimum-idle=5 # JPA Configuration spring.jpa.hibernate.ddl-auto=update spring.jpa.show-sql=false # Server Configuration server.port=8080 server.servlet.context-path=/dna_service # JWT Configuration jwt.signerKey=2VJ50pdhYm96e4VECp/vsZGVmkSl9xp1rSYAZKsZL7n9Ti1pZYle3k9mheQEKt6+ # Logging logging.level.root=INFO logging.level.aws_project.workshop=DEBUG logging.file.name=/opt/workshop/application.log EOF # Replace RDS endpoint RDS_ENDPOINT=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`RDSEndpoint`].OutputValue\u0026#39; \\ --output text) sed -i \u0026#34;s/REPLACE_WITH_RDS_ENDPOINT/${RDS_ENDPOINT}/g\u0026#34; application.properties Start Application # Stop old application (if any) sudo systemctl stop workshop.service 2\u0026gt;/dev/null || true pkill -f workshop-0.0.1-SNAPSHOT.jar 2\u0026gt;/dev/null || true # Start application nohup java -jar workshop-0.0.1-SNAPSHOT.jar \\ --spring.config.location=file:/opt/workshop/application.properties \\ \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp; # Save PID echo $! \u0026gt; application.pid # Wait for application to start sleep 10 # Check process ps aux | grep java Step 5: Verify Application Test Health Endpoint # On EC2 curl http://localhost:8080/dna_service/actuator/health # Expected result: # {\u0026#34;status\u0026#34;:\u0026#34;UP\u0026#34;} Test via Load Balancer # On local machine ALB_DNS=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ALBDNSName`].OutputValue\u0026#39; \\ --output text) curl http://${ALB_DNS}/dna_service/actuator/health Test via API Gateway API_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`APIGatewayURL`].OutputValue\u0026#39; \\ --output text) curl ${API_URL}/dna_service/actuator/health Step 6: Configure Systemd Service To automatically start application when EC2 restarts:\n# On EC2 sudo tee /etc/systemd/system/workshop.service \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; [Unit] Description=Workshop DNA Analysis Backend After=network.target [Service] Type=simple User=ec2-user Group=ec2-user WorkingDirectory=/opt/workshop ExecStart=/usr/bin/java -jar /opt/workshop/workshop-0.0.1-SNAPSHOT.jar --spring.config.location=file:/opt/workshop/application.properties Restart=always RestartSec=10 StandardOutput=append:/opt/workshop/application.log StandardError=append:/opt/workshop/application.log [Install] WantedBy=multi-user.target EOF # Reload systemd sudo systemctl daemon-reload # Enable service sudo systemctl enable workshop.service # Start service sudo systemctl start workshop.service # Check status sudo systemctl status workshop.service Step 7: View Logs # View application logs tail -f /opt/workshop/application.log # View systemd logs sudo journalctl -u workshop.service -f # View CloudWatch Logs (on local machine) aws logs tail /aws/workshop-aws/dev/application \\ --follow \\ --region ap-southeast-1 Troubleshooting Application Won\u0026rsquo;t Start Check logs:\ntail -100 /opt/workshop/application.log Common errors:\nDatabase connection failed\nCheck RDS endpoint in application.properties Verify Security Group allows EC2 to connect to RDS Verify database credentials Port 8080 already in use\n# Kill process using port 8080 sudo lsof -ti:8080 | xargs kill -9 Out of memory\n# Increase heap size java -Xmx512m -jar workshop-0.0.1-SNAPSHOT.jar Health Check Failed # Check application is running ps aux | grep java # Check port listening sudo netstat -tulpn | grep 8080 # Test locally curl -v http://localhost:8080/dna_service/actuator/health Load Balancer Health Check Failed # Check Target Group health aws elbv2 describe-target-health \\ --target-group-arn \u0026lt;target-group-arn\u0026gt; \\ --region ap-southeast-1 # Check Security Group # EC2 SG must allow traffic from ALB SG on port 8080 Confirm Successful Deployment Checklist:\nJAR file built successfully JAR uploaded to S3 Application running on EC2 Health endpoint returns {\u0026quot;status\u0026quot;:\u0026quot;UP\u0026quot;} Accessible via Load Balancer Accessible via API Gateway Systemd service enabled Logs being written to CloudWatch Next Steps After backend is ready:\n➡️ Deploy Frontend to S3 and CloudFront\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00, October 3, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I gained a deep understanding of how generative AI is changing software development. The event showed me how AI can help throughout the entire development lifecycle, from planning to maintenance. I learned that while AI tools are powerful, human judgment is still crucial. I also understood that we should adopt AI tools gradually and ensure the whole team is aligned. New Skills: I learned how to use Amazon Q Developer for code generation, debugging, and documentation. I also explored Kiro and discovered how it can enhance my productivity. I developed skills in identifying which tasks can benefit from AI automation and learned how to balance AI assistance with human review to maintain code quality. Contribution to Team/Project: I documented the key takeaways from the event and shared them with my team. I identified specific projects where we could pilot AI tools, which could improve our development speed by 20-30% for repetitive tasks. I created guidelines for using AI tools in our workflow and planned to organize internal training sessions. The knowledge I gained helps our team stay competitive by using cutting-edge AI development tools. Event 2 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about the complete AWS AI/ML ecosystem, from Amazon SageMaker for traditional ML to Amazon Bedrock for Generative AI. I gained deep understanding of prompt engineering techniques like Chain-of-Thought reasoning and Few-shot learning. I also learned about RAG (Retrieval-Augmented Generation) architecture and how it\u0026rsquo;s crucial for building accurate GenAI applications. The importance of guardrails for AI safety and content filtering in production applications was also emphasized. New Skills: I developed skills in using Amazon SageMaker Studio for ML model development and deployment. I learned how to implement RAG architecture for knowledge base integration. I gained practical knowledge of prompt engineering and how to build Bedrock Agents for multi-step workflows. I also learned about different foundation models (Claude, Llama, Titan) and when to use each one. Contribution to Team/Project: I shared comprehensive notes about SageMaker capabilities and Bedrock features with my team. I identified opportunities to implement RAG solutions for our domain-specific applications. I proposed pilot projects using Bedrock Agents for customer service automation. I also created guidelines for prompt engineering best practices and guardrail implementation for our GenAI projects. Event 3 Event Name: DevOps on AWS\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about DevOps culture and principles, including DORA metrics (Deployment Frequency, Lead Time, MTTR, Change Failure Rate) for measuring DevOps maturity. I gained deep understanding of AWS CI/CD services (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) and different deployment strategies (Blue/Green, Canary, Rolling). I also learned about Infrastructure as Code with CloudFormation and CDK, and when to use each approach. New Skills: I developed skills in building complete CI/CD pipelines using AWS DevOps services. I learned how to implement Infrastructure as Code with both CloudFormation and CDK. I gained practical knowledge of container services (ECR, ECS, EKS, App Runner) and when to use each one. I also learned how to set up monitoring and observability using CloudWatch and X-Ray. Contribution to Team/Project: I shared comprehensive notes about AWS DevOps services and best practices with my team. I proposed implementing CI/CD pipelines using CodePipeline for automated deployments. I suggested adopting Infrastructure as Code for all our infrastructure using CloudFormation or CDK. I also created guidelines for containerization strategies and monitoring best practices. The knowledge gained helps our team implement modern DevOps practices and improve deployment frequency and reliability. Event 4 Event Name: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about the AWS Well-Architected Framework Security Pillar and its five core pillars: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response. I gained deep understanding of core security principles including Least Privilege, Zero Trust, and Defense in Depth. I also learned about the Shared Responsibility Model and top security threats in cloud environments in Vietnam. The importance of building security into architecture from the start, not as an afterthought, was emphasized. New Skills: I developed skills in modern IAM architecture using IAM Identity Center, Service Control Policies, and permission boundaries. I learned how to implement comprehensive detection and monitoring using CloudTrail, GuardDuty, and Security Hub. I gained practical knowledge of network security with VPC segmentation, Security Groups, NACLs, WAF, and Shield. I also learned about encryption at rest and in transit, KMS key management, and secrets management with Secrets Manager and Parameter Store. Contribution to Team/Project: I shared comprehensive security best practices and the five pillars framework with my team. I proposed implementing modern IAM patterns with IAM Identity Center for SSO. I suggested setting up comprehensive monitoring using CloudTrail, GuardDuty, and Security Hub. I created incident response playbooks for common scenarios like compromised IAM keys, S3 public exposure, and EC2 malware detection. I also developed guidelines for encryption strategies and secrets management. The knowledge gained helps our team build secure cloud architectures following AWS Well-Architected best practices. Event 5 Event Name: Building Agentic AI: Context Optimization with Amazon Bedrock\nDate \u0026amp; Time: 09:00, December 5, 2025\nLocation: 26th Floor, Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about Building Agentic AI and Context Optimization with Amazon Bedrock. I gained deep understanding of how to build autonomous AI agents with Amazon Bedrock through hands-on techniques. I learned about agentic orchestration patterns and advanced context optimization techniques. I also understood the importance of context optimization in reducing costs and improving performance. The workshop emphasized that agentic AI is the future of AI applications and context optimization is key to scaling effectively. New Skills: I developed skills in building Bedrock Agents from scratch with guidance from experts. I learned about context optimization techniques such as compression, summarization, and relevant information extraction. I gained practical knowledge of agentic orchestration patterns and how to coordinate multiple agents. I also learned about the CloudThinker platform and how it simplifies building agentic systems. The hands-on workshop gave me opportunities to practice with real AWS environments. Contribution to Team/Project: I shared comprehensive notes about Building Agentic AI and Context Optimization with my team. I proposed pilot projects using Bedrock Agents for automation tasks. I created guidelines for context optimization best practices to reduce costs and improve performance. I also documented CloudThinker platform capabilities and integration patterns. The knowledge gained helps our team explore agentic AI solutions and optimize costs in AI/ML projects. Event 6 Event Name: Kick-off AWS First Cloud Journey Workforce OJT FALL 2025\nDate \u0026amp; Time: 08:30, September 6, 2025\nLocation: 26th Floor, Bitexco Financial Tower, 2 Hải Triều, P. Bến Nghé, Quận 1, TP.HCM\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about the AWS First Cloud Journey Workforce program, which has trained over 2,000 students since 2021, with more than 150 graduates working at leading technology companies. I gained insights into various career pathways in cloud computing including DevOps, Cloud Engineer, and AI/ML Engineer roles. I learned about the importance of continuous learning, networking, and setting clear career goals. The event emphasized that cloud computing offers diverse career paths and that security, DevOps, and AI/ML are all viable and rewarding directions. New Skills: I developed a better understanding of career development in cloud computing and learned about the skills required for different roles. I gained insights into how to build a career path from junior to principal level. I also learned about the importance of soft skills alongside technical skills, and how to leverage community and mentorship for career growth. Contribution to Team/Project: I shared the program information and career insights with my team. I documented the different career pathways and skill requirements for various cloud roles. I created a personal development plan based on the roadmap shared by speakers. I also identified networking opportunities and connections that could benefit our team. The knowledge gained helps me understand the broader cloud computing ecosystem and plan my career development accordingly. "},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.5-deploy-frontend/","title":"Deploy Frontend","tags":[],"description":"","content":"Overview In this step, you will build the React frontend and deploy it to S3, then distribute it via CloudFront CDN to ensure high performance and low latency for global users.\nStep 1: Configure API Endpoint Get API Gateway URL from CloudFormation outputs:\nAPI_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`APIGatewayURL`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;API URL: $API_URL\u0026#34; Update file FE/.env:\ncd FE cat \u0026gt; .env \u0026lt;\u0026lt;EOF VITE_API_URL=${API_URL}/dna_service VITE_APP_NAME=DNA Analysis Workshop VITE_APP_VERSION=1.0.0 EOF Or update config file directly FE/src/config/api.ts:\nexport const API_BASE_URL = process.env.VITE_API_URL || \u0026#39;http://localhost:8080/dna_service\u0026#39;; export const API_TIMEOUT = 30000; export const API_ENDPOINTS = { AUTH: { LOGIN: \u0026#39;/auth/login\u0026#39;, REGISTER: \u0026#39;/auth/register\u0026#39;, LOGOUT: \u0026#39;/auth/logout\u0026#39;, REFRESH: \u0026#39;/auth/refresh\u0026#39;, }, DNA: { ANALYZE: \u0026#39;/dna/analyze\u0026#39;, HISTORY: \u0026#39;/dna/history\u0026#39;, RESULT: \u0026#39;/dna/result\u0026#39;, }, USER: { PROFILE: \u0026#39;/user/profile\u0026#39;, UPDATE: \u0026#39;/user/update\u0026#39;, }, }; Step 2: Install Dependencies cd FE # Install npm packages npm install # Verify installation npm list --depth=0 Step 3: Build Frontend # Build production bundle npm run build # Check build output ls -lh dist/ # View file structure tree dist/ -L 2 Expected result:\ndist/\r├── index.html\r├── assets/\r│ ├── index-[hash].js\r│ ├── index-[hash].css\r│ └── [other assets]\r└── vite.svg Step 4: Upload to S3 Get S3 bucket name from outputs:\nFRONTEND_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`FrontendBucketName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Frontend Bucket: $FRONTEND_BUCKET\u0026#34; Upload files to S3:\n# Sync all files aws s3 sync dist/ s3://${FRONTEND_BUCKET}/ \\ --delete \\ --region ap-southeast-1 # Set cache control for static assets aws s3 cp dist/assets/ s3://${FRONTEND_BUCKET}/assets/ \\ --recursive \\ --cache-control \u0026#34;max-age=31536000\u0026#34; \\ --region ap-southeast-1 # Set no-cache for index.html aws s3 cp dist/index.html s3://${FRONTEND_BUCKET}/ \\ --cache-control \u0026#34;no-cache,no-store,must-revalidate\u0026#34; \\ --region ap-southeast-1 # Verify upload aws s3 ls s3://${FRONTEND_BUCKET}/ --recursive Step 5: Invalidate CloudFront Cache After upload, need to invalidate CloudFront cache so users receive the new version:\n# Get CloudFront Distribution ID DIST_ID=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`CloudFrontDistributionId`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Distribution ID: $DIST_ID\u0026#34; # Create invalidation aws cloudfront create-invalidation \\ --distribution-id $DIST_ID \\ --paths \u0026#34;/*\u0026#34; \\ --region ap-southeast-1 # Monitor invalidation status aws cloudfront get-invalidation \\ --distribution-id $DIST_ID \\ --id \u0026lt;invalidation-id\u0026gt; Note: Invalidation takes 5-10 minutes to complete.\nStep 6: Access Frontend Get CloudFront domain name:\nCLOUDFRONT_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`CloudFrontDomainName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Frontend URL: https://${CLOUDFRONT_URL}\u0026#34; Open browser and access the URL above.\nStep 7: Verify Frontend Test Basic Functionality Home Page: Check page loads correctly Navigation: Test menus and routing API Connection: Open Developer Tools → Network tab Console Errors: Check for no JavaScript errors Test Authentication # Test login endpoint curl -X POST https://${CLOUDFRONT_URL}/api/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;username\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;test123\u0026#34;}\u0026#39; Test CORS Open Developer Tools → Console and run:\nfetch(\u0026#39;${API_URL}/dna_service/actuator/health\u0026#39;) .then(res =\u0026gt; res.json()) .then(data =\u0026gt; console.log(\u0026#39;API Response:\u0026#39;, data)) .catch(err =\u0026gt; console.error(\u0026#39;CORS Error:\u0026#39;, err)); Step 8: Configure Custom Domain (Optional) If you have a domain name:\n1. Create SSL Certificate in ACM # Certificate must be created in us-east-1 for CloudFront aws acm request-certificate \\ --domain-name yourdomain.com \\ --subject-alternative-names www.yourdomain.com \\ --validation-method DNS \\ --region us-east-1 2. Validate Certificate Add CNAME records to DNS as instructed by ACM.\n3. Update CloudFront Distribution aws cloudfront update-distribution \\ --id $DIST_ID \\ --distribution-config file://cloudfront-config.json 4. Update Route 53 # Create A record alias to CloudFront aws route53 change-resource-record-sets \\ --hosted-zone-id \u0026lt;zone-id\u0026gt; \\ --change-batch file://route53-changes.json Deployment Script Create script to automate deployment:\ncat \u0026gt; deploy-frontend.sh \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; #!/bin/bash set -e echo \u0026#34;Building frontend...\u0026#34; cd FE npm run build echo \u0026#34;Getting S3 bucket name...\u0026#34; FRONTEND_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`FrontendBucketName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Uploading to S3...\u0026#34; aws s3 sync dist/ s3://${FRONTEND_BUCKET}/ --delete --region ap-southeast-1 echo \u0026#34;Invalidating CloudFront cache...\u0026#34; DIST_ID=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`CloudFrontDistributionId`].OutputValue\u0026#39; \\ --output text) aws cloudfront create-invalidation \\ --distribution-id $DIST_ID \\ --paths \u0026#34;/*\u0026#34; \\ --region ap-southeast-1 echo \u0026#34;Deployment complete!\u0026#34; echo \u0026#34;Frontend URL: https://$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`CloudFrontDomainName`].OutputValue\u0026#39; \\ --output text)\u0026#34; EOF chmod +x deploy-frontend.sh Use script:\n./deploy-frontend.sh Troubleshooting Build Failed Error: \u0026ldquo;Module not found\u0026rdquo;\n# Delete node_modules and reinstall rm -rf node_modules package-lock.json npm install Error: \u0026ldquo;Out of memory\u0026rdquo;\n# Increase memory for Node.js export NODE_OPTIONS=\u0026#34;--max-old-space-size=4096\u0026#34; npm run build Upload Failed Error: \u0026ldquo;Access Denied\u0026rdquo;\nCheck AWS credentials Verify IAM permissions for S3 Error: \u0026ldquo;Bucket does not exist\u0026rdquo;\nCheck bucket name Verify CloudFormation stack created bucket CloudFront Issues Page won\u0026rsquo;t load:\nWait for CloudFront distribution to deploy (5-10 minutes) Check distribution status: Deployed Receiving old version:\nCreate new invalidation Clear browser cache (Ctrl+Shift+R) CORS errors:\nCheck API Gateway CORS configuration Verify backend CORS settings API Connection Failed # Test API from browser console fetch(\u0026#39;${API_URL}/dna_service/actuator/health\u0026#39;) .then(res =\u0026gt; res.text()) .then(data =\u0026gt; console.log(data)) If error:\nCheck API Gateway URL is correct Verify backend is running Check Security Groups Confirm Successful Deployment Checklist:\nFrontend built successfully Files uploaded to S3 CloudFront invalidation completed Website loads correctly via CloudFront URL No errors in browser console API calls working (check Network tab) Authentication flow working Routing between pages working Next Steps After frontend is ready:\n➡️ Testing and Validation\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Deploy DNA Analysis Application on AWS Overview In this workshop, you will learn how to deploy a production-ready full-stack DNA Analysis application on AWS using Infrastructure as Code (IaC) with CloudFormation. The application consists of a React frontend, Spring Boot backend, and MySQL database, all deployed with AWS best practices for security, scalability, and cost optimization.\nAWS Services Used:\nVPC \u0026amp; Networking: VPC, Subnets, Internet Gateway, NAT Gateway, VPC Endpoints Compute: EC2 Auto Scaling Group, Application Load Balancer Storage \u0026amp; CDN: S3 for frontend hosting, CloudFront for global content delivery Database: RDS MySQL for data persistence with automated backups Security: Security Groups, IAM Roles, AWS Cognito for user authentication Monitoring: CloudWatch Logs, Alarms, and SNS notifications API Management: API Gateway for secure backend API exposure What You Will Learn Infrastructure as Code: Deploy complete AWS infrastructure using CloudFormation templates VPC Design: Create a secure VPC with public and private subnets across multiple availability zones Cost Optimization: Use VPC Endpoints to reduce NAT Gateway costs (~$20-25/month savings) Auto Scaling: Configure EC2 Auto Scaling based on CPU metrics for high availability Database Management: Deploy and configure RDS MySQL with security best practices Frontend Deployment: Host static React website on S3 with CloudFront CDN Backend Deployment: Deploy Spring Boot application on EC2 with systemd service Security Best Practices: Implement security groups, IAM roles, and Cognito authentication Monitoring \u0026amp; Logging: Set up CloudWatch for application monitoring and alerting Architecture Diagram Internet\r│\r├─── CloudFront (CDN) ──\u0026gt; S3 (Frontend)\r│\r└─── API Gateway ──\u0026gt; ALB ──\u0026gt; EC2 (Backend) ──\u0026gt; RDS MySQL\r│\r└─── VPC Endpoints (S3, CloudWatch, SSM) Prerequisites AWS Account with appropriate permissions (Administrator or equivalent) AWS CLI installed and configured (aws configure) EC2 Key Pair created in your AWS region (ap-southeast-1) Basic understanding of AWS services and command line interface Familiarity with CloudFormation concepts Estimated Cost Running this workshop infrastructure will cost approximately $8.90/month (if running 24/7):\nService Instance Type Cost/month (USD) EC2 t3.nano $3.50 RDS MySQL db.t3.micro $2.80 API Gateway - $0.50 S3 + CloudFront - $0.80 Route 53 - $0.50 Cognito - $0.10 CloudWatch - $0.30 CI/CD (CodePipeline) - $0.40 Total $8.90 For workshop (2-3 hours): ~$0.50-1.00\n💡 Cost Saving Tips:\nDelete the stack immediately after workshop completion Use AWS Free Tier for eligible services Disable NAT Gateway when not in use (saves ~$32/month) Use VPC Endpoints instead of NAT Gateway for production Workshop Duration Total Time: 2-3 hours Infrastructure Deployment: 15-20 minutes Application Configuration: 30-45 minutes Testing \u0026amp; Validation: 15-30 minutes Cleanup: 5-10 minutes Content Workshop Overview Prerequisites \u0026amp; Preparation Deploy Infrastructure with CloudFormation Configure and Deploy Backend Application Deploy Frontend to S3 and CloudFront Testing and Validation Monitoring and Troubleshooting CI/CD Pipeline Setup Clean Up Resources "},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.6-testing/","title":"Testing and Validation","tags":[],"description":"","content":"Overview In this step, you will perform test cases to verify the application works correctly end-to-end, from frontend through API Gateway, Load Balancer, to backend and database.\nStep 1: Verify Infrastructure Verify All Services Running # Check CloudFormation stack status aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].StackStatus\u0026#39; # Expected: CREATE_COMPLETE # Check EC2 instances aws ec2 describe-instances \\ --filters \u0026#34;Name=tag:aws:cloudformation:stack-name,Values=workshop-aws-dev\u0026#34; \\ \u0026#34;Name=instance-state-name,Values=running\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;Reservations[*].Instances[*].[InstanceId,State.Name,PrivateIpAddress]\u0026#39; \\ --output table # Check RDS status aws rds describe-db-instances \\ --db-instance-identifier workshop-aws-dev-db \\ --region ap-southeast-1 \\ --query \u0026#39;DBInstances[0].[DBInstanceIdentifier,DBInstanceStatus]\u0026#39; \\ --output table # Expected: available # Check Load Balancer aws elbv2 describe-load-balancers \\ --names workshop-aws-dev-alb \\ --region ap-southeast-1 \\ --query \u0026#39;LoadBalancers[0].[LoadBalancerName,State.Code]\u0026#39; \\ --output table # Expected: active Step 2: Test Backend API Get API URLs ALB_DNS=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ALBDNSName`].OutputValue\u0026#39; \\ --output text) API_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`APIGatewayURL`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;ALB DNS: $ALB_DNS\u0026#34; echo \u0026#34;API Gateway URL: $API_URL\u0026#34; Test Health Endpoint # Test via ALB curl -v http://${ALB_DNS}/dna_service/actuator/health # Test via API Gateway curl -v ${API_URL}/dna_service/actuator/health # Expected response: # {\u0026#34;status\u0026#34;:\u0026#34;UP\u0026#34;} Test User Registration # Register new user curl -X POST ${API_URL}/dna_service/auth/register \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;username\u0026#34;: \u0026#34;testuser\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Test123!@#\u0026#34;, \u0026#34;fullName\u0026#34;: \u0026#34;Test User\u0026#34; }\u0026#39; # Expected: 200 OK with user data Test User Login # Login curl -X POST ${API_URL}/dna_service/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;username\u0026#34;: \u0026#34;testuser\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Test123!@#\u0026#34; }\u0026#39; # Expected: 200 OK with JWT token # Save token for next requests TOKEN=\u0026#34;\u0026lt;jwt-token-from-response\u0026gt;\u0026#34; Test Protected Endpoints # Get user profile curl -X GET ${API_URL}/dna_service/user/profile \\ -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; # Expected: 200 OK with user profile data Step 3: Test Frontend Access Frontend CLOUDFRONT_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`CloudFrontDomainName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Frontend URL: https://${CLOUDFRONT_URL}\u0026#34; Open browser and access the URL above.\nManual Testing Checklist 1. Home Page\nPage loads successfully Logo and branding display correctly Navigation menu works No errors in Console 2. User Registration\nForm validation works Can register new user Success message displays Redirects to login page 3. User Login\nCan login with created credentials JWT token saved in localStorage Redirects to dashboard after login User menu displays username 4. DNA Analysis\nCan upload DNA sequence file File validation works Analysis progress displays Analysis results display correctly Can view history 5. User Profile\nDisplays user information Can update profile Avatar upload works (if available) Logout works correctly 6. Responsive Design\nMobile view works well Tablet view works well Desktop view works well Step 4: Test Database Connection Connect to RDS # Get RDS endpoint RDS_ENDPOINT=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`RDSEndpoint`].OutputValue\u0026#39; \\ --output text) # Connect via MySQL client (from EC2 or local if public access) mysql -h ${RDS_ENDPOINT} -u admin -p workshop_aws Verify Tables Created -- Show all tables SHOW TABLES; -- Expected tables: -- users, dna_sequences, analysis_results, etc. -- Check user data SELECT id, username, email, created_at FROM users; -- Check DNA analysis data SELECT id, user_id, sequence_name, status, created_at FROM dna_sequences; -- Exit EXIT; Step 5: Load Testing (Optional) Install Apache Bench # Ubuntu/Debian sudo apt-get install apache2-utils # MacOS brew install httpd # Windows # Download from Apache website Run Load Test # Test health endpoint ab -n 1000 -c 10 http://${ALB_DNS}/dna_service/actuator/health # Test login endpoint ab -n 100 -c 5 -p login-data.json -T application/json \\ ${API_URL}/dna_service/auth/login Monitor During Load Test # Watch CloudWatch metrics aws cloudwatch get-metric-statistics \\ --namespace AWS/EC2 \\ --metric-name CPUUtilization \\ --dimensions Name=AutoScalingGroupName,Value=workshop-aws-dev-asg \\ --start-time $(date -u -d \u0026#39;5 minutes ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 60 \\ --statistics Average \\ --region ap-southeast-1 Step 6: Security Testing Test HTTPS Enforcement # CloudFront should redirect HTTP to HTTPS curl -I http://${CLOUDFRONT_URL} # Expected: 301 redirect to https:// Test CORS # Test preflight request curl -X OPTIONS ${API_URL}/dna_service/auth/login \\ -H \u0026#34;Origin: https://${CLOUDFRONT_URL}\u0026#34; \\ -H \u0026#34;Access-Control-Request-Method: POST\u0026#34; \\ -H \u0026#34;Access-Control-Request-Headers: Content-Type\u0026#34; \\ -v # Expected: CORS headers in response Test SQL Injection Protection # Try SQL injection in login curl -X POST ${API_URL}/dna_service/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;username\u0026#34;: \u0026#34;admin\u0026#39;\\\u0026#39;\u0026#39; OR \u0026#39;\\\u0026#39;\u0026#39;1\u0026#39;\\\u0026#39;\u0026#39;=\u0026#39;\\\u0026#39;\u0026#39;1\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;anything\u0026#34; }\u0026#39; # Expected: 401 Unauthorized (not SQL error) Test XSS Protection In browser console:\n// Try XSS in input fields document.querySelector(\u0026#39;input[name=\u0026#34;username\u0026#34;]\u0026#39;).value = \u0026#39;\u0026lt;script\u0026gt;alert(\u0026#34;XSS\u0026#34;)\u0026lt;/script\u0026gt;\u0026#39;; Expected: Script not executed, escaped or sanitized.\nStep 7: Performance Testing Measure Page Load Time Open Chrome DevTools → Network tab:\nFirst Contentful Paint (FCP): \u0026lt; 1.5s Largest Contentful Paint (LCP): \u0026lt; 2.5s Time to Interactive (TTI): \u0026lt; 3.5s Total Page Size: \u0026lt; 2MB Test API Response Time # Measure API response time time curl -s ${API_URL}/dna_service/actuator/health \u0026gt; /dev/null # Expected: \u0026lt; 200ms Test CloudFront Caching # First request (MISS) curl -I https://${CLOUDFRONT_URL}/assets/index.js # Second request (HIT) curl -I https://${CLOUDFRONT_URL}/assets/index.js # Check X-Cache header: Hit from cloudfront Troubleshooting Common Issues API Returns 502 Bad Gateway # Check backend health curl http://${ALB_DNS}/dna_service/actuator/health # Check target group health aws elbv2 describe-target-health \\ --target-group-arn \u0026lt;arn\u0026gt; \\ --region ap-southeast-1 # Check EC2 logs aws ssm start-session --target \u0026lt;instance-id\u0026gt; tail -f /opt/workshop/application.log Frontend Shows CORS Error Verify API Gateway CORS configuration Check backend CORS settings in application.properties Ensure CloudFront origin is whitelisted Database Connection Timeout Check RDS Security Group allows EC2 traffic Verify RDS endpoint in application.properties Test connection from EC2: telnet ${RDS_ENDPOINT} 3306 Slow Page Load Check CloudFront cache hit ratio Optimize images and assets Enable gzip compression Review CloudWatch metrics Test Results Documentation Create test-results.md file:\n# Workshop Test Results ## Date: 2025-12-08 ## Tester: [Your Name] ### Infrastructure Tests - [x] CloudFormation stack: CREATE_COMPLETE - [x] EC2 instances: Running - [x] RDS database: Available - [x] Load Balancer: Active ### Backend API Tests - [x] Health endpoint: OK - [x] User registration: OK - [x] User login: OK - [x] Protected endpoints: OK ### Frontend Tests - [x] Page load: OK - [x] User registration: OK - [x] User login: OK - [x] DNA analysis: OK - [x] Responsive design: OK ### Performance Tests - [x] Page load time: 1.2s - [x] API response time: 150ms - [x] CloudFront cache: Working ### Security Tests - [x] HTTPS enforcement: OK - [x] CORS: OK - [x] SQL injection protection: OK - [x] XSS protection: OK ## Issues Found None ## Recommendations - Enable CloudFront compression - Add more CloudWatch alarms - Implement rate limiting Confirm Testing Complete Checklist:\nAll infrastructure services running Backend API endpoints working Frontend loads and works correctly Database connection working User authentication flow working DNA analysis features working Performance meets requirements Security tests pass Test results documented Next Steps After testing is complete:\n➡️ Monitoring and Troubleshooting\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at First Cloud AI Journey Office (FCJ) from September 2025 to DECEMBER 2025, I had the opportunity to learn, practice, and apply cloud computing knowledge to a real-world AWS environment.\nI participated in the AWS Cloud Journey internship program, where I completed a comprehensive 12-week learning journey and deployed a production-ready web application architecture on AWS. Through this project, I improved my skills in cloud architecture design, AWS services, Infrastructure as Code (CloudFormation), CI/CD pipelines, monitoring and observability, security best practices, and problem-solving in cloud environments.\nThe main project involved designing and implementing a complete AWS web application architecture including:\nEdge Layer: Route 53, CloudFront CDN, AWS WAF, ACM Certificate, S3 static hosting Networking: VPC, subnets, Internet Gateway, NAT Gateway, Security Groups, VPC Flow Logs Compute \u0026amp; Database: EC2 with Auto Scaling, RDS, API Gateway, Amazon Cognito CI/CD: GitLab, CodePipeline, CodeBuild with automated deployments Monitoring \u0026amp; Security: CloudWatch, CloudTrail, SNS alerts, IAM, Secrets Manager In terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with mentors and colleagues to improve work efficiency. I maintained detailed worklogs documenting my progress, challenges encountered, and solutions implemented throughout the 12-week period.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ☐ ✅ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ☐ ✅ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ☐ ✅ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Strengths Strong Learning Ability: I demonstrated quick adaptation to new AWS services and concepts. When encountering unfamiliar services like API Gateway VPC Link or CloudFront OAC, I proactively researched documentation and implemented solutions effectively.\nProactive Approach: I took initiative in exploring advanced features beyond basic requirements, such as implementing SSH-less deployment using AWS Systems Manager and automating CloudFront cache invalidation in CI/CD pipelines.\nTechnical Documentation: I maintained comprehensive worklogs with detailed documentation of architecture decisions, challenges faced, and solutions implemented, which will be valuable for future reference and knowledge sharing.\nResponsibility and Quality: I consistently completed weekly tasks on schedule and ensured quality by thorough testing at each stage of the project, from edge layer setup to final end-to-end testing.\nNeeds Improvement Time Management and Discipline: While I completed all tasks, I sometimes struggled with time management when dealing with complex issues like API Gateway VPC Link configuration or CloudWatch alarm troubleshooting. I need to improve my ability to estimate task duration more accurately and allocate time more effectively.\nProblem-Solving Efficiency: When facing technical challenges (such as RDS connection issues or Cognito JWT token validation), I sometimes spent too much time troubleshooting before seeking help or consulting documentation. I should develop a more systematic approach to problem-solving: first checking documentation, then testing systematically, and finally seeking guidance when needed.\nCommunication Skills: I need to improve my ability to communicate technical issues and solutions more clearly, especially when presenting architecture decisions or explaining complex configurations to team members. This includes better documentation of troubleshooting steps and more effective verbal communication during team discussions.\nCost Optimization Awareness: Initially, I focused more on functionality than cost optimization. For example, VPC Flow Logs generated high CloudWatch costs before I implemented retention policies and S3 storage. I should consider cost implications earlier in the design phase.\nReflection This internship provided invaluable hands-on experience with AWS cloud services and best practices. The challenges I encountered, such as configuring API Gateway VPC Links, implementing SSH-less deployments, and troubleshooting CloudWatch alarms, have significantly strengthened my problem-solving skills and technical knowledge.\nThe project taught me the importance of:\nInfrastructure as Code: Using CloudFormation for reproducible deployments Security First: Implementing least-privilege IAM policies and network segmentation Monitoring and Observability: Establishing comprehensive logging and alerting Automation: Reducing manual errors through CI/CD pipelines Documentation: Maintaining detailed records for troubleshooting and knowledge transfer I am grateful for the opportunity to work on this comprehensive AWS project and look forward to applying these skills in future cloud architecture and DevOps roles.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.7-monitoring/","title":"Monitoring and Troubleshooting","tags":[],"description":"","content":"Overview In this step, you will learn how to monitor the application, view logs, and troubleshoot common issues using CloudWatch, CloudWatch Logs, and other AWS tools.\nCloudWatch Metrics EC2 Metrics # CPU Utilization aws cloudwatch get-metric-statistics \\ --namespace AWS/EC2 \\ --metric-name CPUUtilization \\ --dimensions Name=AutoScalingGroupName,Value=workshop-aws-dev-asg \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Average,Maximum \\ --region ap-southeast-1 # Memory Utilization (if CloudWatch Agent installed) aws cloudwatch get-metric-statistics \\ --namespace CWAgent \\ --metric-name mem_used_percent \\ --dimensions Name=AutoScalingGroupName,Value=workshop-aws-dev-asg \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Average \\ --region ap-southeast-1 RDS Metrics # Database Connections aws cloudwatch get-metric-statistics \\ --namespace AWS/RDS \\ --metric-name DatabaseConnections \\ --dimensions Name=DBInstanceIdentifier,Value=workshop-aws-dev-db \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Average,Maximum \\ --region ap-southeast-1 # CPU Utilization aws cloudwatch get-metric-statistics \\ --namespace AWS/RDS \\ --metric-name CPUUtilization \\ --dimensions Name=DBInstanceIdentifier,Value=workshop-aws-dev-db \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Average \\ --region ap-southeast-1 Application Load Balancer Metrics # Request Count aws cloudwatch get-metric-statistics \\ --namespace AWS/ApplicationELB \\ --metric-name RequestCount \\ --dimensions Name=LoadBalancer,Value=app/workshop-aws-dev-alb/xxxxx \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Sum \\ --region ap-southeast-1 # Target Response Time aws cloudwatch get-metric-statistics \\ --namespace AWS/ApplicationELB \\ --metric-name TargetResponseTime \\ --dimensions Name=LoadBalancer,Value=app/workshop-aws-dev-alb/xxxxx \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Average \\ --region ap-southeast-1 CloudWatch Logs View Application Logs # List log streams aws logs describe-log-streams \\ --log-group-name /aws/workshop-aws/dev/application \\ --order-by LastEventTime \\ --descending \\ --max-items 5 \\ --region ap-southeast-1 # Tail logs (real-time) aws logs tail /aws/workshop-aws/dev/application \\ --follow \\ --region ap-southeast-1 # Filter logs by pattern aws logs filter-log-events \\ --log-group-name /aws/workshop-aws/dev/application \\ --filter-pattern \u0026#34;ERROR\u0026#34; \\ --start-time $(date -d \u0026#39;1 hour ago\u0026#39; +%s)000 \\ --region ap-southeast-1 # Search for specific errors aws logs filter-log-events \\ --log-group-name /aws/workshop-aws/dev/application \\ --filter-pattern \u0026#34;NullPointerException\u0026#34; \\ --start-time $(date -d \u0026#39;1 hour ago\u0026#39; +%s)000 \\ --region ap-southeast-1 View EC2 System Logs # Get instance ID INSTANCE_ID=$(aws ec2 describe-instances \\ --filters \u0026#34;Name=tag:aws:cloudformation:stack-name,Values=workshop-aws-dev\u0026#34; \\ \u0026#34;Name=instance-state-name,Values=running\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;Reservations[0].Instances[0].InstanceId\u0026#39; \\ --output text) # Get console output aws ec2 get-console-output \\ --instance-id $INSTANCE_ID \\ --region ap-southeast-1 \\ --output text CloudWatch Alarms View Existing Alarms # List all alarms aws cloudwatch describe-alarms \\ --alarm-name-prefix workshop-aws-dev \\ --region ap-southeast-1 # Get alarm history aws cloudwatch describe-alarm-history \\ --alarm-name workshop-aws-dev-cpu-high \\ --max-records 10 \\ --region ap-southeast-1 Create Custom Alarms # Alarm for High Error Rate aws cloudwatch put-metric-alarm \\ --alarm-name workshop-aws-dev-high-error-rate \\ --alarm-description \u0026#34;Alert when error rate exceeds 5%\u0026#34; \\ --metric-name 5XXError \\ --namespace AWS/ApplicationELB \\ --statistic Sum \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 10 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=LoadBalancer,Value=app/workshop-aws-dev-alb/xxxxx \\ --alarm-actions arn:aws:sns:ap-southeast-1:123456789012:workshop-aws-dev-alarms \\ --region ap-southeast-1 # Alarm for Database Connections aws cloudwatch put-metric-alarm \\ --alarm-name workshop-aws-dev-high-db-connections \\ --alarm-description \u0026#34;Alert when DB connections exceed 80\u0026#34; \\ --metric-name DatabaseConnections \\ --namespace AWS/RDS \\ --statistic Average \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 80 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=DBInstanceIdentifier,Value=workshop-aws-dev-db \\ --alarm-actions arn:aws:sns:ap-southeast-1:123456789012:workshop-aws-dev-alarms \\ --region ap-southeast-1 CloudWatch Dashboards Create Dashboard # Create dashboard JSON cat \u0026gt; dashboard.json \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; { \u0026#34;widgets\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/EC2\u0026#34;, \u0026#34;CPUUtilization\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;EC2 CPU Utilization\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/RDS\u0026#34;, \u0026#34;DatabaseConnections\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;RDS Connections\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/ApplicationELB\u0026#34;, \u0026#34;RequestCount\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;ALB Request Count\u0026#34; } } ] } EOF # Create dashboard aws cloudwatch put-dashboard \\ --dashboard-name workshop-aws-dev-dashboard \\ --dashboard-body file://dashboard.json \\ --region ap-southeast-1 View Dashboard Open CloudWatch Console → Dashboards → workshop-aws-dev-dashboard\nTroubleshooting Common Issues Issue 1: High CPU Usage Symptoms:\nEC2 CPU \u0026gt; 80% Slow response times CloudWatch alarm triggered Diagnosis:\n# Check CPU metrics aws cloudwatch get-metric-statistics \\ --namespace AWS/EC2 \\ --metric-name CPUUtilization \\ --dimensions Name=AutoScalingGroupName,Value=workshop-aws-dev-asg \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 60 \\ --statistics Average,Maximum \\ --region ap-southeast-1 # SSH to EC2 and check processes aws ssm start-session --target $INSTANCE_ID top -bn1 | head -20 Solutions:\nScale up: Increase instance type Scale out: Increase number of instances Optimize code: Profile and optimize application Add caching: Implement Redis/ElastiCache Issue 2: Database Connection Pool Exhausted Symptoms:\n\u0026ldquo;Too many connections\u0026rdquo; errors Slow database queries Application timeouts Diagnosis:\n# Check DB connections aws cloudwatch get-metric-statistics \\ --namespace AWS/RDS \\ --metric-name DatabaseConnections \\ --dimensions Name=DBInstanceIdentifier,Value=workshop-aws-dev-db \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 60 \\ --statistics Average,Maximum \\ --region ap-southeast-1 # Connect to DB and check mysql -h $RDS_ENDPOINT -u admin -p SHOW PROCESSLIST; SHOW STATUS LIKE \u0026#39;Threads_connected\u0026#39;; Solutions:\nIncrease connection pool size in application.properties Fix connection leaks in code Scale up RDS instance Implement connection pooling best practices Issue 3: 502 Bad Gateway Symptoms:\nUsers receive 502 errors ALB cannot reach backend Health checks failing Diagnosis:\n# Check target health aws elbv2 describe-target-health \\ --target-group-arn \u0026lt;arn\u0026gt; \\ --region ap-southeast-1 # Check backend logs aws logs tail /aws/workshop-aws/dev/application --follow # Check Security Groups aws ec2 describe-security-groups \\ --group-ids \u0026lt;ec2-sg-id\u0026gt; \\ --region ap-southeast-1 Solutions:\nVerify backend is running: systemctl status workshop Check Security Group allows ALB traffic Verify health check path is correct Check application logs for errors Best Practices 1. Set Up Alerts CPU \u0026gt; 80% for 5 minutes Memory \u0026gt; 85% for 5 minutes Disk \u0026gt; 90% 5XX errors \u0026gt; 10 in 5 minutes Database connections \u0026gt; 80% of max 2. Regular Health Checks # Daily health check script #!/bin/bash echo \u0026#34;=== Daily Health Check ===\u0026#34; echo \u0026#34;Date: $(date)\u0026#34; # Check stack status echo \u0026#34;CloudFormation Stack:\u0026#34; aws cloudformation describe-stacks --stack-name workshop-aws-dev --query \u0026#39;Stacks[0].StackStatus\u0026#39; # Check EC2 echo \u0026#34;EC2 Instances:\u0026#34; aws ec2 describe-instances --filters \u0026#34;Name=tag:aws:cloudformation:stack-name,Values=workshop-aws-dev\u0026#34; --query \u0026#39;Reservations[*].Instances[*].[InstanceId,State.Name]\u0026#39; # Check RDS echo \u0026#34;RDS Database:\u0026#34; aws rds describe-db-instances --db-instance-identifier workshop-aws-dev-db --query \u0026#39;DBInstances[0].DBInstanceStatus\u0026#39; # Check API echo \u0026#34;API Health:\u0026#34; curl -s $API_URL/dna_service/actuator/health | jq . Monitoring Checklist CloudWatch metrics being collected CloudWatch Logs configured Alarms set up SNS notifications working Dashboard created Log retention configured Custom metrics published Health checks working Performance baseline established Next Steps After setting up monitoring:\n➡️ Clean Up Resources\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship?\nThe most satisfying aspect was successfully deploying a complete, production-ready AWS cloud architecture for the Blood Donation Support System. From designing the initial architecture with VPC, subnets, and security groups, to implementing CI/CD pipelines with CodePipeline and CodeBuild, and finally seeing the application running smoothly on EC2 with RDS database connectivity—every milestone felt like a significant achievement. Particularly rewarding was solving complex challenges like configuring API Gateway VPC Links, implementing SSH-less deployments using Systems Manager, and automating CloudFront cache invalidation. The moment when I successfully connected to the EC2 instance via Session Manager without SSH keys and deployed the Spring Boot application was incredibly satisfying. Additionally, receiving positive feedback from mentors during the mid-stage presentation and seeing the system handle real-world scenarios validated all the hard work.\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nAbsolutely, I would highly recommend this internship program to friends interested in cloud computing and AWS. Here\u0026rsquo;s why:\nPros:\nHands-on Experience: Unlike many internships that focus on theoretical learning, this program provides real, production-level projects that you can showcase in your portfolio. Mentor Support: The mentors are knowledgeable, patient, and genuinely invested in your growth. They guide you without spoon-feeding, which builds problem-solving skills. Comprehensive Learning: The 12-week program covers everything from basic EC2 setup to advanced topics like CI/CD, monitoring, and security best practices. Industry-Relevant Skills: The skills learned here (CloudFormation, CodePipeline, Systems Manager, etc.) are directly applicable to real-world DevOps and cloud engineering roles. Portfolio Project: You end up with a complete, deployable project that demonstrates your AWS expertise to future employers. Considerations:\nRequires self-motivation and proactive learning—not ideal for those who prefer highly structured, step-by-step guidance. The learning curve can be steep, especially for beginners in cloud computing. Overall, this internship is perfect for students who want to transition from academic knowledge to industry-ready skills in cloud computing.\nSuggestions \u0026amp; Expectations Would you like to continue this program in the future?\nYes, I would be very interested in continuing this program in the future, either as:\nA returning intern to work on more advanced projects or mentor new interns A part-time contributor to help maintain and improve the workshop materials A collaborator on open-source improvements to the infrastructure templates The experience has been invaluable, and I would love to contribute back to the program that helped me grow so much.\nAny other comments (free sharing):\nThis internship has been a transformative experience. Coming in with basic AWS knowledge from university courses, I\u0026rsquo;m leaving with confidence in designing, deploying, and managing production-grade cloud infrastructure. The Blood Donation Support System project taught me not just technical skills, but also the importance of documentation, security-first thinking, cost optimization, and systematic problem-solving.\nThe challenges I faced—from debugging API Gateway VPC Links to implementing SSH-less deployments—forced me to think critically and develop resilience. The mentor\u0026rsquo;s approach of guiding rather than giving answers was initially frustrating but ultimately made me a better engineer.\nI\u0026rsquo;m particularly grateful for:\nThe freedom to explore and implement solutions beyond the basic requirements The constructive feedback during presentations that helped refine both technical and communication skills The exposure to real-world scenarios like handling deployment failures, optimizing costs, and troubleshooting production issues This internship has solidified my interest in pursuing a career in cloud engineering and DevOps. The hands-on experience with AWS services, combined with the mentorship and collaborative environment, has prepared me well for future roles in the industry.\nThank you to the entire FCJ team for this incredible opportunity!\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.8-cicd-pipeline/","title":"CI/CD Pipeline Setup","tags":[],"description":"","content":"Overview In this section, you will set up a complete CI/CD pipeline using AWS CodePipeline, CodeBuild, and GitLab CI/CD to automate the build and deployment process for both backend and frontend applications.\nArchitecture GitLab → S3 (Artifacts) → CodePipeline → CodeBuild (Backend) → EC2\r↓\rCodeBuild (Frontend) → S3/CloudFront CI/CD Components 1. GitLab CI/CD Automatically builds and packages source code Uploads artifacts to S3 Triggers AWS CodePipeline 2. AWS CodePipeline Orchestrates the entire deployment workflow Monitors S3 for new artifacts Triggers CodeBuild projects 3. AWS CodeBuild Backend Build: Compiles Spring Boot application to JAR Frontend Build: Builds React application with Vite Deploys to respective AWS services Step 1: Configure GitLab CI/CD 1.1. Set GitLab CI/CD Variables In your GitLab project, go to Settings → CI/CD → Variables and add:\nVariable Value Protected Masked AWS_ACCESS_KEY_ID Your AWS Access Key ✅ ✅ AWS_SECRET_ACCESS_KEY Your AWS Secret Key ✅ ✅ 1.2. Review GitLab CI Configuration The .gitlab-ci.yml file in your project root:\nstages: - deploy deploy-to-aws: stage: deploy image: amazon/aws-cli:latest before_script: - | aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY aws configure set region ap-southeast-1 script: - echo \u0026#34;📦 Creating source.zip...\u0026#34; - | apk add zip zip -r source.zip . \\ -x \u0026#34;*.git*\u0026#34; \\ -x \u0026#34;node_modules/*\u0026#34; \\ -x \u0026#34;.idea/*\u0026#34; \\ -x \u0026#34;target/*\u0026#34; \\ -x \u0026#34;*.zip\u0026#34; - echo \u0026#34;📤 Uploading to S3...\u0026#34; - | aws s3 cp source.zip \\ s3://workshop-aws-dev-artifacts-502310717700-ap-southeast-1/source.zip - echo \u0026#34;✅ Upload completed! CodePipeline will trigger automatically.\u0026#34; only: - main when: on_success What it does:\nPackages entire project into source.zip Uploads to S3 artifacts bucket Triggers CodePipeline automatically Step 2: Deploy CodePipeline with CloudFormation 2.1. Review Pipeline Template The cicd-pipeline.yaml CloudFormation template creates:\nCodePipeline with S3 source CodeBuild project for backend CodeBuild project for frontend IAM roles and permissions 2.2. Deploy Pipeline Stack aws cloudformation create-stack \\ --stack-name workshop-aws-dev-cicd \\ --template-body file://aws/cicd-pipeline.yaml \\ --parameters \\ ParameterKey=ProjectName,ParameterValue=workshop-aws \\ ParameterKey=Environment,ParameterValue=dev \\ ParameterKey=SourceProvider,ParameterValue=S3 \\ ParameterKey=ArtifactBucketName,ParameterValue=workshop-aws-dev-artifacts-502310717700-ap-southeast-1 \\ --capabilities CAPABILITY_NAMED_IAM \\ --region ap-southeast-1 2.3. Wait for Stack Creation aws cloudformation wait stack-create-complete \\ --stack-name workshop-aws-dev-cicd \\ --region ap-southeast-1 Expected time: 3-5 minutes\nStep 3: Configure CodeBuild Projects 3.1. Backend BuildSpec CodeBuild uses buildspec-backend.yml:\nversion: 0.2 phases: pre_build: commands: - echo \u0026#34;Installing Maven...\u0026#34; - yum install -y maven build: commands: - echo \u0026#34;Building Backend JAR...\u0026#34; - cd BE/workshop_BE - mvn clean package -DskipTests post_build: commands: - echo \u0026#34;Uploading JAR to S3...\u0026#34; - aws s3 cp target/workshop-0.0.1-SNAPSHOT.jar \\ s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/jars/ - echo \u0026#34;Deploying to EC2...\u0026#34; - aws ssm send-command \\ --instance-ids i-09fdbf7739ee37b32 \\ --document-name \u0026#34;AWS-RunShellScript\u0026#34; \\ --parameters commands=[ \u0026#34;cd /opt/workshop\u0026#34;, \u0026#34;aws s3 cp s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/jars/workshop-0.0.1-SNAPSHOT.jar .\u0026#34;, \u0026#34;sudo systemctl restart workshop.service\u0026#34; ] artifacts: files: - \u0026#39;**/*\u0026#39; 3.2. Frontend BuildSpec CodeBuild uses buildspec-frontend.yml:\nversion: 0.2 phases: pre_build: commands: - echo \u0026#34;Installing Node.js...\u0026#34; - curl -sL https://rpm.nodesource.com/setup_18.x | bash - - yum install -y nodejs build: commands: - echo \u0026#34;Building Frontend...\u0026#34; - cd FE - npm install - npm run build post_build: commands: - echo \u0026#34;Deploying to S3...\u0026#34; - aws s3 sync dist/ \\ s3://workshop-aws-dev-frontend-502310717700-ap-southeast-1/ \\ --delete - echo \u0026#34;Invalidating CloudFront...\u0026#34; - aws cloudfront create-invalidation \\ --distribution-id E3K48K7CPOOLHZ \\ --paths \u0026#34;/*\u0026#34; artifacts: files: - \u0026#39;FE/dist/**/*\u0026#39; Step 4: Test CI/CD Pipeline 4.1. Trigger Pipeline from GitLab Make a code change and push to main branch:\ngit add . git commit -m \u0026#34;Test CI/CD pipeline\u0026#34; git push origin main 4.2. Monitor GitLab Pipeline Go to GitLab → CI/CD → Pipelines Watch the deploy-to-aws job Verify successful upload to S3 4.3. Monitor AWS CodePipeline Go to AWS Console → CodePipeline Select workshop-aws-dev-pipeline Watch the pipeline stages: Source: Detects new source.zip in S3 Build-Backend: Builds JAR and deploys to EC2 Build-Frontend: Builds React app and deploys to S3/CloudFront 4.4. Check Build Logs For detailed logs:\nClick on Details in any stage View Build logs in CodeBuild Check for errors or warnings Step 5: Verify Deployment 5.1. Test Backend API curl https://98385v3jef.execute-api.ap-southeast-1.amazonaws.com/dev/dna_service/actuator/health Expected response:\n{\u0026#34;status\u0026#34;:\u0026#34;UP\u0026#34;} 5.2. Test Frontend Open browser and navigate to:\nhttps://d3gmmg22uirq0t.cloudfront.net Verify the application loads with latest changes.\nPipeline Flow Step-by-step flow:\nGitLab Commit → Developer pushes code to main branch GitLab CI → Builds and creates source.zip S3 Bucket → Stores source.zip artifact CodePipeline → Detects new artifact and triggers builds CodeBuild Backend → Builds JAR and deploys to EC2 CodeBuild Frontend → Builds React app and deploys to S3/CloudFront CloudWatch Logs → Monitors all build and deployment activities Troubleshooting Pipeline Not Triggering Issue: CodePipeline doesn\u0026rsquo;t start after GitLab push\nSolutions:\nCheck S3 bucket for source.zip Verify CodePipeline source configuration Check IAM permissions for CodePipeline Backend Build Fails Issue: CodeBuild backend fails with Maven errors\nSolutions:\nCheck buildspec-backend.yml syntax Verify Maven dependencies in pom.xml Check CodeBuild logs for specific errors Ensure EC2 instance has SSM Agent running Frontend Build Fails Issue: CodeBuild frontend fails with npm errors\nSolutions:\nCheck buildspec-frontend.yml syntax Verify package.json dependencies Check Node.js version compatibility Ensure S3 bucket permissions are correct Deployment Fails Issue: Build succeeds but deployment fails\nSolutions:\nCheck EC2 Security Groups allow SSM Verify S3 bucket names are correct Check CloudFront distribution ID Review IAM role permissions Best Practices 1. Environment Variables Store sensitive data in GitLab CI/CD variables Use AWS Systems Manager Parameter Store for application configs Never commit credentials to Git 2. Build Optimization Cache dependencies (Maven .m2, npm node_modules) Use smaller Docker images for faster builds Parallelize independent build stages 3. Deployment Strategy Use blue-green deployment for zero downtime Implement health checks before routing traffic Keep rollback capability ready 4. Monitoring Enable CloudWatch Logs for all CodeBuild projects Set up SNS notifications for pipeline failures Monitor build times and optimize bottlenecks Cost Optimization CodeBuild Pricing Build minutes: $0.005 per minute (general1.small) Typical build: 5-10 minutes Cost per build: ~$0.025-0.05 CodePipeline Pricing Active pipeline: $1.00 per month Free tier: 1 active pipeline per month Estimated Monthly Cost 10 deployments/day: ~$15-20/month Includes: CodePipeline + CodeBuild + S3 storage Summary You have successfully set up a complete CI/CD pipeline that:\n✅ Automatically builds and packages code from GitLab\n✅ Uploads artifacts to S3\n✅ Triggers AWS CodePipeline on changes\n✅ Builds backend JAR with Maven\n✅ Builds frontend with Vite\n✅ Deploys backend to EC2\n✅ Deploys frontend to S3/CloudFront\n✅ Provides monitoring and logging\nNext: Clean Up Resources\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.9-cleanup/","title":"Clean Up Resources","tags":[],"description":"","content":"Overview After completing the workshop, you need to delete all resources to avoid unexpected charges. CloudFormation will automatically delete most resources, but some require manual deletion.\nStep 1: Delete S3 Bucket Contents CloudFormation cannot delete S3 buckets containing objects. Delete contents first:\n# Get bucket names from outputs FRONTEND_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`FrontendBucketName`].OutputValue\u0026#39; \\ --output text) # Delete all objects in frontend bucket aws s3 rm s3://$FRONTEND_BUCKET --recursive --region ap-southeast-1 # If backend bucket exists BACKEND_BUCKET=\u0026#34;workshop-aws-dev-backend-$(aws sts get-caller-identity --query Account --output text)-ap-southeast-1\u0026#34; aws s3 rm s3://$BACKEND_BUCKET --recursive --region ap-southeast-1 2\u0026gt;/dev/null || true Step 2: Delete CloudFormation Stack Method 1: Using Deploy Script Windows:\ncd aws deploy.bat delete Linux/Mac:\ncd aws ./deploy.sh delete Method 2: Using AWS CLI aws cloudformation delete-stack \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 Step 3: Monitor Deletion Progress # Check status aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].StackStatus\u0026#39; # Wait for stack deletion (may take 10-15 minutes) aws cloudformation wait stack-delete-complete \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 Via AWS Console:\nOpen CloudFormation Console Select stack workshop-aws-dev Events tab: View resources being deleted Stack will disappear from list when deletion completes Step 4: Verify Resources Deleted Check VPC # Should not see workshop VPC aws ec2 describe-vpcs \\ --filters \u0026#34;Name=tag:aws:cloudformation:stack-name,Values=workshop-aws-dev\u0026#34; \\ --region ap-southeast-1 Check EC2 Instances # Should not see workshop instances aws ec2 describe-instances \\ --filters \u0026#34;Name=tag:aws:cloudformation:stack-name,Values=workshop-aws-dev\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;Reservations[*].Instances[*].[InstanceId,State.Name]\u0026#39; Check RDS # Should not see RDS instance (may have snapshot) aws rds describe-db-instances \\ --region ap-southeast-1 \\ --query \u0026#39;DBInstances[?DBInstanceIdentifier==`workshop-aws-dev-db`]\u0026#39; Check S3 Buckets # Buckets should be deleted aws s3 ls | grep workshop-aws-dev Step 5: Delete RDS Snapshots (Optional) CloudFormation creates snapshot before deleting RDS. Delete if not needed:\n# List snapshots aws rds describe-db-snapshots \\ --region ap-southeast-1 \\ --query \u0026#39;DBSnapshots[?contains(DBSnapshotIdentifier,`workshop-aws-dev`)].DBSnapshotIdentifier\u0026#39; # Delete snapshot aws rds delete-db-snapshot \\ --db-snapshot-identifier \u0026lt;snapshot-id\u0026gt; \\ --region ap-southeast-1 Step 6: Delete CloudWatch Logs (Optional) Log groups are not automatically deleted:\n# List log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/workshop-aws\u0026#34; \\ --region ap-southeast-1 # Delete log groups aws logs delete-log-group \\ --log-group-name \u0026#34;/aws/workshop-aws/dev/application\u0026#34; \\ --region ap-southeast-1 Step 7: Delete EC2 Key Pair (Optional) If key pair is no longer needed:\naws ec2 delete-key-pair \\ --key-name workshop-aws-key \\ --region ap-southeast-1 # Delete local .pem file rm workshop-aws-key.pem Troubleshooting Stack Deletion Failed If stack deletion fails:\nView error: aws cloudformation describe-stack-events \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;StackEvents[?ResourceStatus==`DELETE_FAILED`].[LogicalResourceId,ResourceStatusReason]\u0026#39; \\ --output table Common errors: Error: \u0026ldquo;S3 bucket is not empty\u0026rdquo;\nDelete all objects in bucket Retry stack deletion Error: \u0026ldquo;Network interface is in use\u0026rdquo;\nWait a few minutes for ENIs to be released Retry stack deletion Error: \u0026ldquo;Resource being used by another resource\u0026rdquo;\nIdentify resource dependencies Delete dependent resources first Force delete: # Retain problematic resources and delete stack aws cloudformation delete-stack \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --retain-resources \u0026lt;ResourceLogicalId\u0026gt; # Then delete resources manually Cleanup Checklist Ensure all resources are deleted:\nCloudFormation stack deleted S3 buckets deleted EC2 instances terminated RDS database deleted Load Balancer deleted VPC and subnets deleted CloudFront distribution disabled and deleted NAT Gateway deleted Elastic IPs released RDS snapshots deleted (optional) CloudWatch log groups deleted (optional) EC2 Key Pair deleted (optional) Confirm No Ongoing Charges After 24-48 hours, check AWS Cost Explorer to ensure no charges from workshop.\nConclusion You have successfully completed the workshop and cleaned up all resources!\nWhat you learned: ✅ Deploy full-stack application on AWS ✅ Infrastructure as Code with CloudFormation ✅ AWS networking and security best practices ✅ Cost optimization strategies ✅ Monitoring and troubleshooting\nNext resources:\nAWS Well-Architected Framework AWS Solutions Library AWS Workshops Thank you for participating in this workshop! 🎉\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://Fatnotfatt.github.io/learning-aws/tags/","title":"Tags","tags":[],"description":"","content":""}]