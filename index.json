[{"uri":"https://Fatnotfatt.github.io/learning-aws/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Summary Report: \u0026ldquo;Building Agentic AI: Context Optimization with Amazon Bedrock\u0026rdquo; Event Objectives Introduce Building Agentic AI and Context Optimization with Amazon Bedrock Build autonomous AI agents with Amazon Bedrock through hands-on techniques Share real-world use cases for agentic workflows Introduce CloudThinker and Agentic Orchestration solution Provide hands-on workshop with real AWS environments Connect with AWS experts and AI practitioners Event Details Date: Friday, December 5, 2025 Time: 9:00 AM – 12:00 PM (Check-in opens at 8:15 AM) Location: 26th Floor, Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh Duration: 3 hours (including tea break, networking, and lunch buffet) Agenda 9:00 – 9:10 AM | Opening (10 minutes) Nguyen Gia Hung, Head of Solutions Architect, AWS\nWelcome and event introduction Overview of Building Agentic AI Context Optimization with Amazon Bedrock 9:10 – 9:40 AM | AWS Bedrock Agent Core (30 minutes) Kien Nguyen, Solutions Architect, AWS\nIntroduction to AWS Bedrock Agent Architecture and core components How Bedrock Agent works Context management and optimization Integration with other AWS services Demo: Create and configure basic Bedrock Agent 9:40 – 10:00 AM | [Use Case] Building Agentic Workflow on AWS (20 minutes) Viet Pham, Founder \u0026amp; CEO, Diaflow\nReal-world use case for building agentic workflow Architecture and best practices Challenges and solutions Context optimization in production Demo: Agentic workflow in practice 10:00 – 10:10 AM | CloudThinker Introduction (10 minutes) Thang Ton, Co-founder \u0026amp; COO, CloudThinker\nIntroduction to CloudThinker Agentic Orchestration solution Vision and roadmap Integration with Amazon Bedrock 10:10 – 10:40 AM | CloudThinker Agentic Orchestration, Context Optimization on Amazon Bedrock (L300) (30 minutes) Henry Bui, Head of Engineering, CloudThinker\nAgentic orchestration patterns Advanced context optimization techniques Integration with Amazon Bedrock Advanced use cases and best practices Performance optimization Cost optimization strategies Demo: CloudThinker platform 10:40 – 11:00 AM | Tea Break \u0026amp; Networking (20 minutes) Break time Networking with experts and participants Informal Q\u0026amp;A Complimentary refreshments 11:00 AM – 12:00 PM | CloudThinker Hack: Hands-on Workshop (60 minutes) Kha Van, Community Leader, AWS\nHands-on workshop with real AWS environments Build Bedrock Agent from scratch Practice context optimization Agentic orchestration patterns Troubleshooting and best practices Q\u0026amp;A and direct support 12:00 PM Onwards | Networking \u0026amp; Lunch Buffet Extended networking Lunch buffet Meet the experts Share experiences and learnings Meet Our Experts Nguyen Gia Hung Head of Solutions Architect, AWS\nLeading expert in AWS architecture and solutions Extensive experience in consulting and implementing cloud solutions Leader of AWS programs in Vietnam Kien Nguyen Solutions Architect, AWS\nExpert in AWS Bedrock and AI services Experience in building AI/ML solutions Expert in agentic AI and context optimization Viet Pham Founder \u0026amp; CEO, Diaflow\nEntrepreneur with experience in AI and cloud computing Expert in agentic workflows and automation Founder of Diaflow - AI workflow solution Kha Van Community Leader, AWS\nAWS Community Leader Expert in hands-on training and workshops Mentor for AWS community Thang Ton Co-Founder \u0026amp; COO, CloudThinker\nCo-founder of CloudThinker Expert in cloud orchestration and automation Expert in agentic orchestration platforms Henry Bui Head of Engineering, CloudThinker\nExpert in agentic orchestration Experience in context and performance optimization Expert in L300 technical deep-dive sessions Key Highlights Building Agentic AI Introduction to Agentic AI:\nWhat Agentic AI is and why it matters Autonomous AI agents vs traditional AI Use cases and applications Future of AI with agentic systems Context Optimization:\nWhy context optimization is important Techniques to optimize context Cost reduction strategies Performance improvement Best practices AWS Bedrock Agent Core Bedrock Agent Architecture:\nAgent: Main entity that performs tasks Knowledge Base: Information source for the agent Action Groups: Actions the agent can perform Orchestration: Flow and context management Context Management: Context optimization Key Features:\nNatural language understanding Context management and optimization Multi-step reasoning Integration with AWS services Custom actions and workflows Agentic Workflow Use Case Building Agentic Workflow:\nDesigning complex workflows with multiple agents Orchestration and coordination between agents Context optimization in workflows Error handling and retry logic Monitoring and observability Best Practices:\nDesign patterns for agentic workflows Context management strategies Performance optimization Cost optimization Security and compliance CloudThinker Agentic Orchestration Agentic Orchestration Patterns:\nSequential workflows Parallel execution Conditional branching Error recovery and fallback Context sharing between agents Context Optimization Techniques:\nContext compression and summarization Relevant information extraction Memory management Cost optimization strategies Performance tuning Integration with Amazon Bedrock:\nSeamless integration with Bedrock models Custom model selection Prompt engineering and optimization Response formatting and validation Context optimization APIs Key Takeaways Understanding Building Agentic AI What Agentic AI is: Autonomous AI agents capable of performing tasks independently Context Optimization: Techniques to optimize context and reduce costs Use cases: Real-world use cases for agentic AI Architecture: Architecture and design patterns AWS Bedrock Agent Bedrock Agent Core: Understanding how agents work and interact Context Management: Managing and optimizing context Integration: How to integrate with other AWS services Best Practices: Best practices from AWS experts Agentic Workflow Design Workflow patterns: Common patterns for agentic workflows Orchestration: How to manage and coordinate multiple agents Context Optimization: Strategies for context optimization Error handling: Strategies for error handling and recovery CloudThinker Platform Agentic orchestration: How CloudThinker addresses orchestration challenges Context optimization: Advanced techniques to optimize context Platform capabilities: Features and capabilities of CloudThinker Integration patterns: How to integrate CloudThinker into existing systems Hands-on Experience Practical skills: Practical skills in building Bedrock Agents Context optimization: Practicing context optimization techniques Troubleshooting: How to debug and troubleshoot common issues Real-world scenarios: Working with real-world scenarios Applying to Work Build Agentic AI: Use AWS Bedrock Agent to build autonomous AI agents Context Optimization: Apply context optimization techniques to reduce costs and improve performance Design Workflows: Apply agentic workflow patterns to projects Integrate CloudThinker: Evaluate and integrate CloudThinker into existing solutions Best Practices: Apply best practices from the workshop to production systems Event Experience Attending the \u0026ldquo;Building Agentic AI: Context Optimization with Amazon Bedrock\u0026rdquo; workshop was an intensive learning experience about agentic AI and context optimization. The event provided both theoretical knowledge and hands-on practice, giving me a clear understanding of how to build and optimize autonomous AI agents.\nOpening and Introduction Opening session by Nguyen Gia Hung created a professional and inspiring atmosphere. I understood the importance of Building Agentic AI and Context Optimization. Overview of event agenda helped me visualize the learning journey. AWS Bedrock Agent Core Session by Kien Nguyen provided a solid foundation about Bedrock Agent. I learned about architecture, components, and how Bedrock Agent works. Context management was a key highlight in this session. Demo of creating Bedrock Agent showed the actual process from start to finish. Real-World Use Case Use case presentation by Viet Pham illustrated how agentic workflows are used in production. Learning about real-world challenges and how to solve them. Context optimization in production was very practical and insightful. Agentic workflow demo showed performance and capabilities in practice. CloudThinker Platform CloudThinker introduction by Thang Ton introduced the platform and solution. L300 session by Henry Bui went deep into technical details and advanced patterns. Learning about advanced context optimization techniques to improve performance and reduce costs. CloudThinker platform demo showed capabilities and ease of use. Hands-on Workshop Hands-on workshop by Kha Van provided direct practice opportunities. Building Bedrock Agent from scratch with guidance from an expert. Practicing context optimization and agentic orchestration. Troubleshooting session helped me understand how to solve common issues. Direct Q\u0026amp;A provided answers to specific questions. Networking and Connections Networking sessions allowed connections with AWS experts and AI practitioners. Sharing experiences and learnings with other participants. Lunch buffet created opportunities for informal discussions and connections. Meeting experts and receiving advice about career development. Lessons Learned Agentic AI is the future: Autonomous AI agents will change how we build AI applications. Context Optimization is key: Optimizing context can significantly reduce costs and improve performance. Hands-on practice is essential: Practical experience is crucial to truly understand and apply concepts. Platform solutions matter: CloudThinker and similar platforms simplify building agentic systems. Community is valuable: Networking with experts and practitioners provides valuable insights and opportunities. Some event photos Overall, this workshop provided me with comprehensive knowledge about Building Agentic AI and Context Optimization with Amazon Bedrock. The combination of theory, real-world use cases, and hands-on practice gave me confidence to start building autonomous AI agents on AWS. Particularly, the context optimization and hands-on workshop sections provided practical skills that can be applied immediately to work. This workshop is essential for anyone wanting to understand agentic AI in depth and how to optimize context to reduce costs and improve performance.\nWhat\u0026rsquo;s Included: ✓ Technical deep-dive sessions (L300)\n✓ Live demos and use case presentations\n✓ Hands-on workshop with real AWS environments\n✓ Networking with AWS experts and AI practitioners\n✓ Complimentary refreshments and lunch buffet\nIMPORTANT: Please bring your laptop to participate in the hands-on exercises\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Well-Architected Security Pillar\u0026rdquo; Event Objectives Introduce AWS Well-Architected Framework Security Pillar Demonstrate 5 main Security pillars: IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response Share best practices and core principles about cloud security Provide practical knowledge about threats and prevention in Vietnam Guide building security architecture according to AWS Well-Architected standards Connect with security experts and cloud practitioners Event Details Date: Saturday, November 29, 2025 Time: 8:30 AM – 12:00 PM Location: AWS Vietnam Office, Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh Duration: 3.5 hours (including coffee break) Agenda 8:30 – 8:50 AM | Opening \u0026amp; Security Foundation (20 minutes) Role of Security Pillar in Well-Architected Framework Core Principles: Least Privilege: Grant minimum necessary permissions Zero Trust: Never trust, always verify Defense in Depth: Multiple layers of protection Shared Responsibility Model: AWS and customer responsibilities Top threats in cloud environment in Vietnam Q\u0026amp;A ⭐ Pillar 1 — Identity \u0026amp; Access Management 8:50 – 9:30 AM | Modern IAM Architecture (40 minutes) IAM Fundamentals:\nUsers, Roles, Policies – avoid long-term credentials Best practices for IAM setup Temporary credentials and session management IAM Identity Center:\nSingle Sign-On (SSO) configuration Permission sets and assignment Multi-account management Advanced IAM:\nService Control Policies (SCP) for multi-account Permission boundaries to limit permissions MFA (Multi-Factor Authentication) requirements Credential rotation strategies Access Analyzer to detect external access Mini Demo: Validate IAM Policy + simulate access\nCheck policy syntax and permissions Simulate access scenarios Troubleshoot common IAM issues ⭐ Pillar 2 — Detection 9:30 – 9:55 AM | Detection \u0026amp; Continuous Monitoring (25 minutes) AWS Security Services:\nCloudTrail: Organization-level logging and audit GuardDuty: Threat detection and intelligent security Security Hub: Centralized security findings Comprehensive Logging:\nVPC Flow Logs: Network traffic monitoring ALB Access Logs: Application layer monitoring S3 Access Logs: Object access tracking Logging at every layer of infrastructure Alerting \u0026amp; Automation:\nEventBridge rules for security events Automated response workflows Integration with notification systems Detection-as-Code:\nInfrastructure as Code for security rules Version control for detection rules Automated deployment and testing 9:55 – 10:10 AM | Coffee Break (15 minutes) Break time Networking with participants Informal Q\u0026amp;A ⭐ Pillar 3 — Infrastructure Protection 10:10 – 10:40 AM | Network \u0026amp; Workload Security (30 minutes) Network Security:\nVPC Segmentation: Separate network segments Private vs Public placement strategies Network isolation and security zones Security Groups vs NACLs:\nWhen to use Security Groups When to use NACLs Practical application models Best practices and common mistakes Advanced Network Protection:\nAWS WAF: Web Application Firewall AWS Shield: DDoS protection Network Firewall: Managed network firewall service Workload Protection:\nEC2 Security: Instance hardening, patch management ECS Security: Container security best practices EKS Security: Kubernetes security fundamentals Security baselines and compliance ⭐ Pillar 4 — Data Protection 10:40 – 11:10 AM | Encryption, Keys \u0026amp; Secrets (30 minutes) AWS KMS (Key Management Service):\nKey policies and access control Grants and delegation Key rotation strategies Multi-region key management Encryption at Rest:\nS3: Server-side encryption (SSE-S3, SSE-KMS, SSE-C) EBS: Volume encryption and snapshots RDS: Database encryption DynamoDB: Table encryption Encryption in Transit:\nTLS/SSL best practices Certificate management End-to-end encryption Secrets Management:\nSecrets Manager: Automated rotation patterns Parameter Store: Secure parameter storage Rotation patterns and best practices Integration with applications Data Classification \u0026amp; Access Guardrails:\nData classification frameworks Access controls based on classification Compliance and regulatory requirements ⭐ Pillar 5 — Incident Response 11:10 – 11:40 AM | IR Playbook \u0026amp; Automation (30 minutes) IR Lifecycle according to AWS:\nPrepare: Preparation and planning Detect: Incident detection Respond: Response and containment Recover: Recovery and lessons learned IR Playbooks for Common Scenarios:\n1. Compromised IAM Key:\nDetect compromised credentials Immediate response steps Key rotation and access revocation Investigation and forensics 2. S3 Public Exposure:\nDetect public buckets Immediate remediation Access review and audit Prevention strategies 3. EC2 Malware Detection:\nDetect malware and suspicious activity Isolation procedures Evidence collection Cleanup and recovery Automated Response:\nLambda functions for automated response Step Functions for complex workflows Integration with security services Playbook automation patterns Evidence Collection:\nSnapshot creation for forensics Log preservation Chain of custody Compliance with legal requirements 11:40 AM – 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A (20 minutes) Summary of 5 Security pillars Common pitfalls and frequent mistakes Vietnamese enterprise reality: Security challenges in Vietnam Compliance requirements Best practices for local context Security learning roadmap: AWS Certified Security – Specialty AWS Certified Solutions Architect – Professional Security training paths Q\u0026amp;A session Commemorative photos Key Highlights Security Foundation Principles Least Privilege:\nGrant only minimum necessary permissions to perform work Regular review and audit permissions Use temporary credentials instead of long-term keys Principle of least privilege at every layer Zero Trust:\nNever trust by default, always verify Verify identity and authorization for every request Network segmentation and micro-segmentation Continuous verification and monitoring Defense in Depth:\nMultiple layers of protection: Network, Application, Data, Identity Don\u0026rsquo;t rely on a single layer of protection Layered security controls Fail-safe defaults Shared Responsibility Model:\nAWS: Security OF the cloud (infrastructure) Customer: Security IN the cloud (data, applications, configurations) Understand responsibilities of each party Best practices for customer responsibilities Pillar 1: Identity \u0026amp; Access Management Modern IAM Architecture:\nUse IAM Roles instead of Users when possible Temporary credentials with STS IAM Identity Center for SSO Permission boundaries and SCPs Best Practices:\nEnable MFA for all users Regular credential rotation Use Access Analyzer to detect external access Least privilege policies Regular access reviews Pillar 2: Detection Comprehensive Monitoring:\nCloudTrail for audit trail GuardDuty for threat detection Security Hub for centralized view VPC Flow Logs for network monitoring Detection-as-Code:\nVersion control for detection rules Automated testing CI/CD for security rules Infrastructure as Code approach Pillar 3: Infrastructure Protection Network Security:\nVPC segmentation Security Groups and NACLs WAF, Shield, Network Firewall Private subnets and NAT gateways Workload Security:\nEC2 hardening Container security Kubernetes security Patch management Pillar 4: Data Protection Encryption:\nEncryption at rest with KMS Encryption in transit with TLS Key management best practices Secrets management Data Classification:\nClassify data by sensitivity Apply appropriate controls Access guardrails Compliance requirements Pillar 5: Incident Response IR Lifecycle:\nPrepare: Planning and tools Detect: Monitoring and alerting Respond: Containment and investigation Recover: Restoration and lessons learned Automation:\nAutomated response with Lambda Step Functions for workflows Integration with security services Playbook automation Key Takeaways Security Foundation Well-Architected Framework: Understanding Security Pillar and its role Core Principles: Least Privilege, Zero Trust, Defense in Depth Shared Responsibility: AWS and customer responsibilities Threat Landscape: Top threats in Vietnam and prevention methods IAM Best Practices Modern IAM: Use roles, temporary credentials IAM Identity Center: SSO and permission management Advanced Features: SCPs, permission boundaries, Access Analyzer Security: MFA, credential rotation, least privilege Detection \u0026amp; Monitoring Security Services: CloudTrail, GuardDuty, Security Hub Comprehensive Logging: VPC Flow Logs, ALB logs, S3 logs Alerting: EventBridge and automation Detection-as-Code: Infrastructure as Code for security Infrastructure Protection Network Security: VPC segmentation, Security Groups, NACLs Advanced Protection: WAF, Shield, Network Firewall Workload Security: EC2, ECS, EKS security Best Practices: Hardening and patch management Data Protection Encryption: At rest and in transit KMS: Key management and rotation Secrets Management: Secrets Manager and Parameter Store Data Classification: Access guardrails and compliance Incident Response IR Lifecycle: Prepare, Detect, Respond, Recover Playbooks: Common scenarios and response procedures Automation: Lambda and Step Functions Forensics: Evidence collection and preservation Applying to Work Design Security Architecture: Apply 5 pillars to architecture design Implement IAM Best Practices: Use modern IAM patterns Setup Detection: Deploy comprehensive monitoring Protect Infrastructure: Apply network and workload security Protect Data: Implement encryption and secrets management Prepare IR: Build incident response playbooks and automation Security Reviews: Regular security assessments and improvements Event Experience Attending the \u0026ldquo;AWS Well-Architected Security Pillar\u0026rdquo; workshop was an intensive learning experience about cloud security. The event provided comprehensive knowledge about the 5 Security pillars and practical best practices, helping me understand how to build secure cloud architecture.\nSecurity Foundation Opening session introduced Security Pillar in Well-Architected Framework. I learned about core principles: Least Privilege, Zero Trust, Defense in Depth. Shared Responsibility Model helped me understand AWS and customer responsibilities. Top threats in Vietnam provided real-world context for security challenges. Modern IAM Architecture IAM session went deep into modern IAM patterns and best practices. Learning about IAM Identity Center for SSO and multi-account management. Advanced features like SCPs and permission boundaries were very useful. Demo validate IAM policy showed practical approach to test policies. Detection \u0026amp; Continuous Monitoring Detection session covered comprehensive monitoring strategy. CloudTrail, GuardDuty, and Security Hub form a powerful security monitoring stack. Logging at every layer helped me understand defense in depth. Detection-as-Code approach was very innovative and practical. Network \u0026amp; Workload Security Infrastructure Protection session went deep into network security. Clear understanding of when to use Security Groups vs NACLs. Advanced protection with WAF, Shield, Network Firewall. Workload security for EC2, ECS, EKS provided practical guidance. Data Protection Data Protection session covered encryption and secrets management. KMS key management and rotation strategies are very important. Encryption at rest and in transit for all services. Secrets Manager patterns for automated rotation. Incident Response IR session provided practical playbooks for common scenarios. Learning about IR lifecycle and best practices. Automated response with Lambda and Step Functions is very powerful. Evidence collection procedures for forensics and compliance. Wrap-up and Q\u0026amp;A Wrap-up session summarized 5 pillars comprehensively. Common pitfalls helped me avoid frequent mistakes. Vietnamese enterprise reality provided local context. Learning roadmap for security certifications was very useful. Lessons Learned Security is foundation: Must design security from the start, not add-on later. Defense in Depth: Don\u0026rsquo;t rely on a single layer of protection. Automation is key: Automated detection and response reduce response time. Continuous improvement: Security is an ongoing process, not a one-time setup. Compliance matters: Understand regulatory requirements and best practices. Practice makes perfect: Need regular practice and review. Some event photos Overall, this workshop provided me with comprehensive knowledge about AWS Well-Architected Security Pillar. The combination of theory, best practices, and practical demos gave me a solid foundation to build secure cloud architecture. Particularly, the incident response and automation sections provided practical skills that can be applied immediately to work. This workshop is essential for anyone wanting to understand cloud security on AWS in depth.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.11-week11/","title":"Worklog Week 11","tags":[],"description":"","content":"Week 11 Objectives: Set up CI/CD Pipeline: Connect GitLab repository to AWS CodePipeline for automated deployments. Configure AWS CodeBuild for frontend and backend builds with automatic S3 upload and CloudFront invalidation. Implement SSH-less deployment for backend using AWS Systems Manager or CodeDeploy. Set up comprehensive monitoring with CloudWatch logs, metrics, and enhanced monitoring for EC2 and RDS. Configure AWS CloudTrail for audit logging and security compliance. Set up SNS Alerts with CloudWatch alarms for critical metrics (EC2 CPU, RDS connections, API 5xx errors). Perform end-to-end testing and create final project documentation with complete architecture diagram. Tasks for the Week: Day Task Start Date Completion Date Reference 19 - GitLab to CodePipeline Integration: + Create GitLab repository for the project (if not already created). + Set up AWS CodePipeline with source stage connected to GitLab repository. + Configure webhook or polling for automatic pipeline triggers on code commits. + Create S3 bucket for pipeline artifacts storage. + Test pipeline trigger by making a test commit to GitLab repository. + Verify CodePipeline can successfully connect to GitLab and retrieve source code. 16/11/2025 16/11/2025 CodePipeline documentation 20 - CodeBuild for Frontend: + Create CodeBuild project for frontend build process. + Configure buildspec.yml file for frontend build steps (install dependencies, build assets, optimize). + Set up CodeBuild environment with appropriate Docker image (Node.js, npm, etc.). + Configure build output to upload built files to S3 bucket (FE Bucket). + Set up automatic CloudFront invalidation after S3 upload (invalidate cache for updated files). + Test frontend build process and verify files are uploaded to S3 and CloudFront cache is invalidated. 17/11/2025 17/11/2025 CodeBuild documentation 21 - CodeBuild for Backend \u0026amp; SSH-less Deployment: + Create CodeBuild project for backend build process. + Configure buildspec.yml for backend build steps (compile, test, package artifacts). + Set up CodeBuild environment for backend (Java/Python/Node.js based on application). + Configure artifact upload to S3 or CodeDeploy. + Implement SSH-less deployment using AWS Systems Manager (SSM) or CodeDeploy: - Option 1: Use SSM Run Command to deploy to EC2 instances without SSH. - Option 2: Use CodeDeploy to deploy application to Auto Scaling Group. + Test backend build and deployment process end-to-end. 18/11/2025 18/11/2025 CodeDeploy / SSM documentation 22 - CloudWatch Logs \u0026amp; Metrics Setup: + Create CloudWatch log groups for EC2 application logs. + Configure CloudWatch agent on EC2 instances to send logs and custom metrics. + Set up CloudWatch metrics for EC2: CPU utilization, memory, disk I/O, network. + Enable RDS Enhanced Monitoring for detailed database metrics. + Configure API Gateway access logs to CloudWatch Logs. + Create CloudWatch dashboards for monitoring application health and performance. + Configure log retention policies for cost optimization. 19/11/2025 19/11/2025 CloudWatch documentation 23 - CloudTrail \u0026amp; Audit Dashboard: + Enable AWS CloudTrail for API call logging across all AWS services. + Create CloudTrail trail with S3 bucket for log storage. + Configure CloudTrail log file validation and encryption. + Set up CloudWatch Logs integration for CloudTrail events (optional). + Create CloudWatch dashboard for audit and security monitoring. + Review CloudTrail logs to verify API call logging is working correctly. + Document CloudTrail configuration and log retention policies. 20/11/2025 20/11/2025 CloudTrail documentation 24 - SNS Alerts \u0026amp; CloudWatch Alarms: + Create SNS topic for alarm notifications. + Subscribe email/SMS endpoints to SNS topic. + Create CloudWatch alarm for EC2 CPU utilization (threshold: \u0026gt;80% for 5 minutes). + Create CloudWatch alarm for RDS database connections (threshold: \u0026gt;80% of max connections). + Create CloudWatch alarm for API Gateway 5xx errors (threshold: \u0026gt;10 errors in 5 minutes). + Configure alarm actions to send notifications via SNS. + Test alarms by triggering conditions and verify email/SMS notifications are received. 21/11/2025 21/11/2025 CloudWatch Alarms \u0026amp; SNS 25 - End-to-End Testing \u0026amp; Final Documentation: + Perform comprehensive end-to-end testing: Users → Route 53 → CloudFront → WAF → API Gateway → EC2 → RDS. + Test CI/CD pipeline with code changes: verify automated frontend and backend deployments. + Test monitoring and alerting: trigger alarms and verify SNS notifications are received. + Test security: verify IAM permissions, Cognito authentication, Secrets Manager access, WAF protection. + Test scalability: verify Auto Scaling Group responds to load changes. + Create final architecture diagram with all components, data flows, and resource relationships. + Write comprehensive project documentation: deployment procedures, troubleshooting guides, runbooks, and architecture overview. + Prepare Worklog summary for all 4 weeks (Week 8-11). - Week 11 Summary: Complete AWS web application architecture deployed with CI/CD, monitoring, security, and automation. Project ready for production use. 22/11/2025 22/11/2025 Project documentation Achievements in Week 11: Successfully set up CI/CD Pipeline:\nConnected GitLab repository to AWS CodePipeline for automated deployments. Configured automatic pipeline triggers on code commits (webhook or polling). Created S3 bucket for pipeline artifacts storage. Verified end-to-end pipeline connectivity and source code retrieval. Configured CodeBuild for Frontend:\nCreated CodeBuild project with buildspec.yml for frontend build automation. Configured build environment with appropriate Docker image and dependencies. Set up automatic S3 upload of built frontend files. Implemented automatic CloudFront cache invalidation after deployments. Verified frontend build and deployment process works correctly. Implemented CodeBuild for Backend with SSH-less Deployment:\nCreated CodeBuild project for backend build automation. Configured buildspec.yml for backend compilation, testing, and packaging. Implemented SSH-less deployment using AWS Systems Manager (SSM) or CodeDeploy. Eliminated need for SSH keys and improved security posture. Verified backend build and deployment process works end-to-end. Set up comprehensive CloudWatch Monitoring:\nCreated CloudWatch log groups for EC2 application logs. Configured CloudWatch agent on EC2 instances for logs and custom metrics. Set up CloudWatch metrics for EC2 (CPU, memory, disk, network). Enabled RDS Enhanced Monitoring for detailed database insights. Configured API Gateway access logs to CloudWatch Logs. Created CloudWatch dashboards for real-time monitoring. Configured log retention policies for cost optimization. Configured CloudTrail for Audit and Compliance:\nEnabled CloudTrail for comprehensive API call logging. Created CloudTrail trail with S3 bucket for secure log storage. Configured log file validation and encryption. Set up CloudWatch dashboard for audit monitoring. Established audit trail for security and compliance requirements. Implemented SNS Alerts and CloudWatch Alarms:\nCreated SNS topic for alarm notifications with email/SMS subscriptions. Created CloudWatch alarm for EC2 CPU utilization monitoring. Created CloudWatch alarm for RDS database connection monitoring. Created CloudWatch alarm for API Gateway 5xx error detection. Configured alarm actions to send notifications via SNS. Tested alarms and verified notification delivery. Performed comprehensive end-to-end testing:\nVerified complete application flow: Users → Route 53 → CloudFront → WAF → API Gateway → EC2 → RDS. Tested CI/CD pipeline with code changes and verified automated deployments. Tested monitoring and alerting: triggered alarms and verified SNS notifications. Tested security: verified IAM permissions, Cognito authentication, Secrets Manager, WAF protection. Tested scalability: verified Auto Scaling Group responds to load changes. Created final project documentation:\nCreated comprehensive architecture diagram with all components, data flows, and resource relationships. Documented deployment procedures, troubleshooting guides, and runbooks. Prepared architecture overview and system design documentation. Completed Worklog summary for all 4 weeks (Week 8-11). After Week 11, the complete AWS web application architecture is fully deployed, monitored, secured, and automated:\nEdge Layer: Route 53, CloudFront, AWS WAF, ACM Certificate, S3 (Frontend). Networking Layer: VPC, public/private subnets, Internet Gateway, NAT Gateway, Security Groups, VPC Flow Logs. Compute \u0026amp; Database Layer: EC2 (with Auto Scaling), RDS, API Gateway, Cognito. CI/CD Pipeline: GitLab, CodePipeline, CodeBuild (Frontend \u0026amp; Backend), SSH-less deployment. Monitoring \u0026amp; Security: CloudWatch (Logs, Metrics, Dashboards, Alarms), CloudTrail, SNS Alerts, IAM, Secrets Manager. The project demonstrates a production-ready, scalable, secure, and well-monitored AWS architecture following best practices with complete automation and observability.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.10-week10/","title":"Worklog Week 10","tags":[],"description":"","content":"Week 10 Objectives: Deploy Backend Layer: EC2 instances in private subnet with application runtime and Auto Scaling configuration. Set up Amazon RDS database in private subnet with proper configuration and parameter groups. Deploy backend application and establish connectivity between EC2 and RDS using Secrets Manager. Configure API Gateway REST API with integration to EC2 backend. Integrate Amazon Cognito User Pool with API Gateway for authentication and authorization. Configure Auto Scaling Group for EC2 instances with Launch Template for scalability. Tasks for the Week: Day Task Start Date Completion Date Reference 13 - RDS Database Setup: + Create RDS subnet group spanning private subnet (10.0.2.0/24). + Launch RDS instance (MySQL/PostgreSQL) in private subnet with appropriate instance class. + Configure RDS parameter group with database-specific settings (character set, timezone, etc.). + Set up automated backups, encryption at rest, and Multi-AZ deployment (optional for cost optimization). + Configure RDS security group to allow connections only from EC2 Security Group. + Store initial database credentials in AWS Secrets Manager. 09/11/2025 09/11/2025 RDS documentation 14 - EC2 Backend Instance Setup: + Launch EC2 instance in private subnet (10.0.2.0/24) with appropriate instance type. + Install application runtime environment: Java/Python/Node.js based on application requirements. + Install and configure application dependencies and libraries. + Configure EC2 instance with IAM role (created in Week 9) for AWS service access. + Create base AMI from configured EC2 instance for Auto Scaling Group (to be used on Day 18). + Document EC2 configuration and application setup steps. 10/11/2025 10/11/2025 EC2 documentation 15 - Backend Application Deployment: + Deploy backend application code to EC2 instance (manual deployment for initial setup). + Configure application to connect to RDS database using credentials from Secrets Manager. + Test database connectivity from EC2 instance (verify connection string, credentials retrieval). + Configure application environment variables and configuration files. + Test basic application functionality and database operations (CRUD operations). + Document deployment process and application configuration. 11/11/2025 11/11/2025 Application deployment guide 16 - API Gateway REST API Configuration: + Create REST API in API Gateway with appropriate name and description. + Define API resources and methods (GET, POST, PUT, DELETE) based on application requirements. + Configure API Gateway integration with EC2 backend (HTTP/HTTPS integration or VPC Link for private resources). + Set up API Gateway VPC Link to connect to private subnet resources (EC2). + Enable CORS for frontend access (configure CORS headers: Access-Control-Allow-Origin, etc.). + Test API endpoints and verify integration with EC2 backend. 12/11/2025 12/11/2025 API Gateway documentation 17 - Amazon Cognito Integration: + Create Cognito User Pool for user authentication with appropriate name. + Configure user pool settings: password policies (minimum length, complexity), MFA (optional), email verification. + Create Cognito User Pool App Client for application integration. + Configure Cognito Authorizer in API Gateway for authenticated API access. + Test user registration flow: create test user in Cognito User Pool. + Test login flow: authenticate user and obtain JWT tokens. + Test authenticated API access: use JWT token to access protected API endpoints. 13/11/2025 13/11/2025 Cognito documentation 18 - Auto Scaling Group Configuration: + Create Launch Template based on base AMI created on Day 14. + Configure Launch Template with instance type, security groups, IAM role, and user data scripts. + Create Auto Scaling Group with Launch Template in private subnet. + Configure Auto Scaling policies: target tracking (CPU utilization, network in/out), step scaling, or scheduled scaling. + Set minimum, desired, and maximum capacity for Auto Scaling Group. + Test scale-out: trigger scaling by increasing load (or manually adjust desired capacity). + Test scale-in: reduce load and verify instances are terminated automatically. - Week 10 Summary: Backend and database layer complete, ready for CI/CD and monitoring setup in Week 11. 14/11/2025 14/11/2025 Auto Scaling documentation Achievements in Week 10: Successfully deployed Amazon RDS database:\nCreated RDS subnet group in private subnet for database isolation. Launched RDS instance (MySQL/PostgreSQL) with appropriate instance class and configuration. Configured RDS parameter group with database-specific settings. Set up automated backups, encryption at rest, and monitoring. Configured RDS security group to allow connections only from EC2 Security Group. Stored database credentials securely in AWS Secrets Manager. Set up EC2 backend infrastructure:\nLaunched EC2 instance in private subnet with appropriate instance type. Installed and configured application runtime environment (Java/Python/Node.js). Configured EC2 instance with IAM role for AWS service access. Created base AMI from configured EC2 instance for Auto Scaling Group. Documented EC2 configuration and application setup procedures. Deployed backend application:\nDeployed backend application code to EC2 instance. Configured application to connect to RDS using credentials from Secrets Manager. Tested database connectivity and verified connection functionality. Tested basic application functionality and database operations (CRUD). Documented deployment process and application configuration. Configured API Gateway REST API:\nCreated REST API with resources, methods, and integration points. Set up API Gateway VPC Link to connect to private subnet resources (EC2). Configured API Gateway integration with EC2 backend using HTTP/HTTPS. Enabled CORS for frontend access with proper headers. Tested API endpoints and verified integration with EC2 backend. Integrated Amazon Cognito for authentication:\nCreated Cognito User Pool with password policies, MFA, and email verification. Created Cognito User Pool App Client for application integration. Configured Cognito Authorizer in API Gateway for authenticated API access. Tested user registration, login, and authenticated API access flows. Established secure user authentication and authorization. Configured Auto Scaling Group for scalability:\nCreated Launch Template based on base AMI for consistent instance configuration. Created Auto Scaling Group with Launch Template in private subnet. Configured Auto Scaling policies (target tracking, step scaling) for automatic scaling. Set appropriate capacity limits (minimum, desired, maximum). Tested scale-out and scale-in functionality to verify automatic scaling. After Week 10, the backend and database layer is operational with scalable infrastructure. The application is ready for CI/CD automation and comprehensive monitoring in Week 11.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.9-week9/","title":"Worklog Week 9","tags":[],"description":"","content":"Week 9 Objectives: Build VPC and Networking Core: Create VPC with public and private subnets, Internet Gateway, and NAT Gateway. Establish secure network boundaries with proper routing and subnet segmentation. Configure Security Groups for EC2, RDS, and ALB following least-privilege principles. Set up IAM roles and policies for EC2 instances with custom permissions. Enable VPC Flow Logs for network traffic monitoring and auditing. Tasks for the Week: Day Task Start Date Completion Date Reference 7 - VPC Creation \u0026amp; Subnet Configuration: + Create VPC with CIDR block 10.0.0.0/16 in selected AWS region. + Create public subnet (10.0.1.0/24) in one Availability Zone with appropriate tags. + Create private subnet (10.0.2.0/24) in the same Availability Zone with appropriate tags. + Apply consistent tagging strategy (Name, Environment, Project, etc.) to all resources. + Document subnet allocation and IP addressing scheme. 02/11/2025 02/11/2025 AWS VPC documentation 8 - Internet Gateway Setup: + Create and attach Internet Gateway to VPC. + Configure public subnet route table to route internet-bound traffic (0.0.0.0/0) to Internet Gateway. + Verify public subnet route table configuration. + Test internet connectivity from public subnet (launch test EC2 instance if needed). + Document routing configuration and gateway associations. 03/11/2025 03/11/2025 Internet Gateway guide 9 - NAT Gateway Configuration: + Allocate Elastic IP address for NAT Gateway. + Create NAT Gateway in public subnet (10.0.1.0/24). + Configure private subnet route table to route internet-bound traffic (0.0.0.0/0) through NAT Gateway. + Verify private subnet route table configuration. + Test outbound internet connectivity from private subnet (launch test EC2 instance in private subnet). + Verify private subnet instances can reach internet while remaining isolated from inbound connections. 04/11/2025 04/11/2025 NAT Gateway documentation 10 - Security Groups Design \u0026amp; Implementation: + Create Security Group for EC2 instances: allow inbound from API Gateway/ALB, outbound to RDS and internet via NAT. + Create Security Group for RDS: allow inbound only from EC2 Security Group on database port (3306/5432). + Create Security Group for ALB (if used): allow inbound HTTP/HTTPS from internet, outbound to EC2 Security Group. + Apply least-privilege principle: grant minimum necessary permissions. + Document security group rules and relationships. 05/11/2025 05/11/2025 Security Groups guide 11 - IAM Roles \u0026amp; Policies for EC2: + Create IAM role for EC2 instances with descriptive name. + Create custom IAM policy for EC2 to access required AWS services (S3, Secrets Manager, CloudWatch, etc.). + Attach IAM role to EC2 instance profile. + Test IAM role permissions from EC2 instance (use AWS CLI or SDK). + Verify EC2 can access Secrets Manager to retrieve database credentials. + Document IAM roles and their permissions. 06/11/2025 06/11/2025 IAM best practices 12 - Network ACLs \u0026amp; VPC Flow Logs: + Review and configure Network ACLs (optional, default ACLs are usually sufficient). + Test Network ACL rules if custom rules are implemented. + Enable VPC Flow Logs to capture IP traffic flow information. + Configure Flow Logs destination (CloudWatch Logs or S3 bucket). + Review Flow Logs to audit network traffic patterns. + Audit and document all network configurations for security review. - Week 9 Summary: VPC and networking core complete, ready for backend and database deployment in Week 10. 07/11/2025 07/11/2025 VPC Flow Logs documentation Achievements in Week 9: Successfully created VPC and subnet infrastructure:\nCreated VPC with CIDR block 10.0.0.0/16 in selected AWS region. Configured public subnet (10.0.1.0/24) for internet-facing resources with proper tagging. Configured private subnet (10.0.2.0/24) for application servers with proper tagging. Applied consistent tagging strategy across all network resources for better management. Set up Internet Gateway for public subnet connectivity:\nCreated and attached Internet Gateway to VPC. Configured public subnet route table to route internet traffic (0.0.0.0/0) to Internet Gateway. Verified public subnet instances can access internet directly. Documented routing configuration and gateway associations. Configured NAT Gateway for private subnet outbound access:\nAllocated Elastic IP address and created NAT Gateway in public subnet. Configured private subnet route table to route internet traffic through NAT Gateway. Verified private subnet instances can reach internet for outbound connections (updates, downloads, API calls). Confirmed private subnet remains isolated from inbound internet connections (security best practice). Implemented Security Groups following least-privilege principles:\nCreated Security Group for EC2: allows inbound from API Gateway/ALB, outbound to RDS and internet. Created Security Group for RDS: allows inbound only from EC2 Security Group on database port. Created Security Group for ALB (if used): allows inbound HTTP/HTTPS, outbound to EC2. Applied least-privilege principle: granted minimum necessary permissions for each component. Documented security group rules and relationships for maintainability. Configured IAM roles and policies for EC2:\nCreated IAM role for EC2 instances with descriptive naming. Created custom IAM policy for EC2 to access required AWS services (S3, Secrets Manager, CloudWatch). Attached IAM role to EC2 instance profile. Tested IAM permissions from EC2 instance and verified access to Secrets Manager. Documented IAM roles and permissions for security audit. Enabled VPC Flow Logs for network monitoring:\nEnabled VPC Flow Logs to capture IP traffic flow information. Configured Flow Logs destination (CloudWatch Logs or S3 bucket). Reviewed Flow Logs to audit network traffic patterns and identify anomalies. Audited all network configurations for security compliance. After Week 9, the VPC and networking core infrastructure is complete with secure network boundaries, proper routing, and monitoring capabilities. The system is ready for backend and database deployment in Week 10.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.8-week8/","title":"Worklog Week 8","tags":[],"description":"","content":"Week 8 Objectives: Complete the Edge Layer and Frontend Storage: Route 53, S3, CloudFront, AWS WAF, and ACM Certificate. Set up DNS management with Route 53 hosted zone and domain configuration. Configure S3 bucket for static frontend hosting with proper access policies. Deploy CloudFront distribution for global content delivery with Origin Access Control. Implement AWS WAF protection with security rules (SQL injection, XSS, bot control). Set up ACM Certificate and enable HTTPS for secure content delivery. Tasks for the Week: Day Task Start Date Completion Date Reference 1 - System Requirements Analysis \u0026amp; Architecture Design: + Analyze system requirements and review the complete architecture diagram. + Identify all components: Route 53, S3, CloudFront, WAF, ACM, VPC, EC2, RDS, API Gateway. + Create High-Level Design (HLD) document with architecture overview. + Document data flow: Users → Route 53 → CloudFront → WAF → S3 (Frontend). + Plan IP addressing scheme and resource naming conventions. 26/10/2025 26/10/2025 Architecture diagram 2 - Route 53 Setup: + Create Route 53 hosted zone for domain management. + Create A record pointing to CloudFront distribution (planned for Day 4). + Create CNAME records for subdomains if needed. + Configure DNS settings and verify domain ownership. + Document DNS configuration and record types. 27/10/2025 27/10/2025 Route 53 documentation 3 - S3 Frontend Bucket Configuration: + Create S3 bucket for frontend static assets (FE Bucket) with appropriate naming. + Enable static website hosting on S3 bucket. + Configure public access policy for CloudFront access (block public access, allow CloudFront via OAC). + Upload test frontend files (HTML, CSS, JS, images) to S3 bucket. + Test static website hosting endpoint and verify file accessibility. 28/10/2025 28/10/2025 S3 documentation 4 - CloudFront Distribution Setup: + Create CloudFront distribution with S3 bucket as origin. + Configure Origin Access Control (OAC) for secure S3 access (replacing OAI). + Set up cache policies (CachingOptimized, CachingDisabled, etc.). + Configure default root object (index.html). + Map Route 53 domain to CloudFront distribution (update A record from Day 2). + Test CloudFront distribution and verify content delivery. 29/10/2025 29/10/2025 CloudFront documentation 5 - AWS WAF Integration: + Create AWS WAF WebACL for CloudFront protection. + Add managed rules: AWS Managed Rules for SQL injection protection. + Add managed rules: AWS Managed Rules for XSS (Cross-Site Scripting) protection. + Configure bot control rules to block common bots and scrapers. + Associate WAF WebACL with CloudFront distribution. + Test WAF rules by attempting common attack patterns and verify blocking. 30/10/2025 30/10/2025 AWS WAF documentation 6 - ACM Certificate \u0026amp; HTTPS Configuration: + Request ACM Certificate in us-east-1 region (required for CloudFront). + Validate certificate using DNS validation method (add CNAME records to Route 53). + Wait for certificate validation and issuance. + Bind ACM certificate to CloudFront distribution. + Configure CloudFront to use HTTPS only (redirect HTTP to HTTPS). + Test HTTPS connection and verify SSL/TLS certificate is working correctly. - Week 8 Summary: Edge layer and frontend storage complete, ready for VPC and networking setup in Week 9. 31/10/2025 31/10/2025 ACM documentation Achievements in Week 8: Successfully completed system analysis and architecture design:\nAnalyzed system requirements and reviewed complete architecture diagram. Created High-Level Design (HLD) document with architecture overview and component relationships. Documented data flow from users through edge services to frontend storage. Established resource naming conventions and planning documentation. Set up Route 53 DNS management:\nCreated Route 53 hosted zone for domain management. Configured A and CNAME records for domain routing. Established DNS foundation for connecting domain to CloudFront distribution. Configured S3 for static frontend hosting:\nCreated S3 bucket for frontend static assets with proper naming conventions. Enabled static website hosting on S3 bucket. Configured public access policies: blocked public access, allowed CloudFront access via Origin Access Control. Uploaded test frontend files and verified static website hosting functionality. Deployed CloudFront distribution:\nCreated CloudFront distribution with S3 bucket as origin. Configured Origin Access Control (OAC) for secure S3 access (modern replacement for OAI). Set up cache policies for optimized content delivery. Mapped Route 53 domain to CloudFront distribution. Verified content delivery through CloudFront CDN globally. Implemented AWS WAF protection:\nCreated AWS WAF WebACL with comprehensive security rules. Added AWS Managed Rules for SQL injection protection. Added AWS Managed Rules for XSS (Cross-Site Scripting) protection. Configured bot control rules to block malicious bots and scrapers. Associated WAF WebACL with CloudFront distribution. Tested WAF rules and verified protection against common attack patterns. Enabled HTTPS with ACM Certificate:\nRequested and validated ACM Certificate in us-east-1 region (required for CloudFront). Used DNS validation method with CNAME records in Route 53. Bound ACM certificate to CloudFront distribution. Configured CloudFront to enforce HTTPS (redirect HTTP to HTTPS). Verified SSL/TLS certificate is working correctly and secure connections are established. After Week 8, the edge layer and frontend storage are complete with secure, global content delivery. The system is ready for VPC and networking core setup in Week 9.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.7-week7/","title":"Worklog Week 7","tags":[],"description":"","content":"Week 7 Objectives: Understand Amazon DynamoDB as a fully managed NoSQL database service: key-value and document data models, partition keys, sort keys, global secondary indexes (GSI), and on-demand vs provisioned capacity modes. Learn how to build and manage Data Lakes on AWS using services like Amazon S3, AWS Glue, Amazon Athena, and Amazon QuickSight for analytics workloads. Explore AWS Analytics services: Amazon Athena for serverless SQL queries on S3, AWS Glue for ETL operations, and Amazon QuickSight for business intelligence and visualization. Practice data ingestion, transformation, and analysis workflows in the AWS cloud environment. Understand cost optimization and performance tuning strategies for analytics workloads on AWS. Tasks for the Week: Day Task Start Date Completion Date Reference 2 - Lab Practice: Data Lake on AWS. + Understand data lake architecture on AWS using S3 as the data lake storage layer. + Learn about data ingestion, cataloging, and querying patterns. + Explore integration between S3, Glue, and Athena for analytics. 20/10/2025 20/10/2025 https://000035.awsstudygroup.com/ 3 - Lab Practice: Amazon DynamoDB Immersion Day. + Deep dive into DynamoDB core concepts: tables, items, attributes, primary keys, and indexes. + Practice creating tables, inserting data, and querying with partition keys and sort keys. + Understand DynamoDB capacity modes (on-demand vs provisioned) and pricing models. 21/10/2025 21/10/2025 https://000039.awsstudygroup.com/ 4 - Lab Practice: Cost and performance analysis with AWS Glue and Amazon Athena. + Use AWS Glue to catalog data stored in S3 and create data catalogs. + Run SQL queries on S3 data using Amazon Athena. + Analyze cost implications and optimize query performance. + Understand partitioning strategies for cost-effective analytics. 22/10/2025 22/10/2025 https://000040.awsstudygroup.com/ 5 - Lab Practice: Work with Amazon DynamoDB. + Create DynamoDB tables with appropriate key schemas. + Perform CRUD operations (Create, Read, Update, Delete) on DynamoDB items. + Work with Global Secondary Indexes (GSI) and Local Secondary Indexes (LSI). + Practice querying and scanning operations, understanding the differences. 23/10/2025 23/10/2025 https://000060.awsstudygroup.com/ 6 - Lab Practice: Building a Datalake with Your Data. + Build a complete data lake solution using AWS services. + Implement data ingestion pipelines. + Set up data transformation workflows with AWS Glue. + Create analytics-ready datasets for downstream consumption. 24/10/2025 24/10/2025 https://000070.awsstudygroup.com/ 7 - Lab Practice: Analytics on AWS workshop. + Comprehensive workshop covering the full analytics stack on AWS. + Integrate multiple services: S3, Glue, Athena, and visualization tools. + Build end-to-end analytics solutions from raw data to insights. 25/10/2025 25/10/2025 https://000072.awsstudygroup.com/ 8 - Lab Practice: Get started with QuickSight. + Create visualizations and dashboards using Amazon QuickSight. + Connect QuickSight to various data sources (S3, Athena, RDS, etc.). + Build interactive reports and share insights with stakeholders. - Summarize and review all AWS Analytics and Data Lake Services covered in Week 7. 26/10/2025 26/10/2025 https://000073.awsstudygroup.com/ Achievements in Week 7: Gained comprehensive understanding of Amazon DynamoDB:\nDynamoDB as a fully managed, serverless NoSQL database service with single-digit millisecond latency. Key concepts: tables, items, attributes, primary keys (partition key + optional sort key), and data modeling best practices. Global Secondary Indexes (GSI) and Local Secondary Indexes (LSI) for flexible query patterns. Capacity modes: on-demand (pay-per-request) vs provisioned (reserved capacity) and when to use each. DynamoDB Streams for real-time data processing and change data capture. Built hands-on experience with Data Lake architecture on AWS:\nAmazon S3 as the foundation for data lake storage with lifecycle policies, versioning, and encryption. Data lake architecture patterns: raw zone, processed zone, and curated zone. Data ingestion strategies: batch uploads, streaming data, and integration with various data sources. Best practices for organizing data in S3: partitioning, naming conventions, and folder structures. Mastered AWS Analytics services:\nAWS Glue: Serverless ETL service for discovering, cataloging, and transforming data. Glue Data Catalog as a centralized metadata repository. Glue ETL jobs for data transformation using Apache Spark. Glue Crawlers for automatic schema discovery. Amazon Athena: Serverless interactive SQL query service for analyzing data in S3. Pay-per-query pricing model and cost optimization strategies. Integration with Glue Data Catalog for schema-on-read queries. Query performance optimization through partitioning and columnar formats (Parquet, ORC). Amazon QuickSight: Cloud-native business intelligence and visualization service. Creating dashboards, visualizations, and reports. Connecting to various data sources (S3, Athena, RDS, Redshift, etc.). Sharing insights with teams and embedding analytics in applications. Completed comprehensive lab practice sessions:\nData Lake on AWS (Lab 35): Built foundational understanding of data lake architecture and S3-based storage patterns. Amazon DynamoDB Immersion Day (Lab 39): Deep dive into DynamoDB operations, data modeling, and best practices. Cost and performance analysis with AWS Glue and Amazon Athena (Lab 40): Learned to optimize analytics workloads for cost and performance. Work with Amazon DynamoDB (Lab 60): Practiced CRUD operations, indexing strategies, and query patterns. Building a Datalake with Your Data (Lab 70): Implemented end-to-end data lake solution with ingestion and transformation pipelines. Analytics on AWS workshop (Lab 72): Comprehensive workshop integrating multiple analytics services. Get started with QuickSight (Lab 73): Created visualizations and dashboards for business intelligence. After Week 7, established a complete understanding of the AWS Analytics and Data Lake ecosystem:\nFrom NoSQL databases (DynamoDB) → Data Lake storage (S3) → ETL and cataloging (Glue) → Query and analysis (Athena) → Visualization (QuickSight), ready to design and implement modern analytics solutions on AWS. "},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.6-week6/","title":"Worklog Week 6","tags":[],"description":"","content":"Week 6 Objectives: Review fundamental Database Concepts: relational model, primary/foreign keys, ACID, normalization, OLTP vs OLAP. Understand Amazon RDS as a managed relational database service on AWS: engines, Multi-AZ, read replicas, backup, and scaling. Learn the benefits of Amazon Aurora compared to standard RDS engines: performance, high availability, automatic storage scaling, MySQL/PostgreSQL compatibility. Get familiar with Amazon Redshift as a petabyte-scale data warehouse for analytics, and distinguish it from RDS (OLTP workloads). Learn how Amazon ElastiCache (Redis / Memcached) provides an in-memory cache layer to reduce latency and offload backend databases. Practice Database Schema Conversion \u0026amp; Migration using AWS DMS and AWS Schema Conversion Tool (SCT) for moving databases to AWS. Tasks for the Week: Day Task Start Date Completion Date Reference 2 - Review Database Concepts (Module 06-01): relational model, ACID, transactions, indexing, normalization, OLTP vs OLAP. - Map traditional on-premises database concepts to AWS cloud services. 13/10/2025 13/10/2025 Class material – Module 06-01 3 - Study Amazon RDS \u0026amp; Amazon Aurora theory (Module 06-02). - Learn about supported engines, Multi-AZ, automated backups, snapshots, read replicas, and scaling. - Compare RDS vs Aurora in terms of performance, availability, and cost. 14/10/2025 14/10/2025 https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html, https://aws.amazon.com/rds/aurora/ 4 - Study Amazon Redshift \u0026amp; Amazon ElastiCache (Module 06-03). - Distinguish OLTP (RDS/Aurora) vs OLAP (Redshift) and in-memory cache layer (ElastiCache). - Explore common use cases: data warehouse \u0026amp; BI, caching sessions, leaderboard, rate limiting, etc. 15/10/2025 15/10/2025 https://aws.amazon.com/redshift/, https://aws.amazon.com/elasticache/ 5 - Lab Practice: Module 06-Lab 5 – Amazon Relational Database Service (Amazon RDS). + Create an RDS instance, configure security group, parameter group, backups. + Connect from a client, run queries, and test behavior (e.g., failover/Multi-AZ if available in the lab). 16/10/2025 16/10/2025 https://000005.awsstudygroup.com/ 6 - Lab Practice: Module 06-Lab 43 – Database Schema Conversion \u0026amp; Migration. + Use AWS Schema Conversion Tool (SCT) to analyze and convert schema from source DB to RDS/Aurora/Redshift target. + Use AWS Database Migration Service (DMS) to migrate data (full load and change data capture if supported in the lab). - Summarize and review all AWS Database Services covered in Week 6. 17/10/2025 17/10/2025 https://000043.awsstudygroup.com/ Achievements in Week 6: Consolidated understanding of core database concepts:\nRelational tables, primary/foreign keys, relational integrity, and basic indexing. ACID properties of transactions and why they matter in OLTP workloads. Clear distinction between OLTP vs OLAP and how this maps to AWS services. Gained hands-on familiarity with Amazon RDS:\nCreated and managed RDS instances via AWS Management Console. Reviewed supported engines (MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, Aurora) and their typical use cases. Practiced configuring Multi-AZ, automated backups, snapshots, monitoring, and basic scaling options. Understood the strengths of Amazon Aurora:\nAurora as a cloud-native, MySQL/PostgreSQL-compatible database with significantly improved performance over standard engines. Aurora DB cluster architecture, with a distributed storage layer across multiple AZs. Reader and writer endpoints, automatic storage scaling, and high availability design. Built a big-picture view of Amazon Redshift \u0026amp; ElastiCache:\nRedshift as a columnar, petabyte-scale data warehouse optimized for analytics and BI workloads. How Redshift differs from RDS/Aurora: optimized for complex queries over large datasets rather than transactional workloads. ElastiCache (Redis/Memcached) as a fully managed, low-latency in-memory cache layer to increase throughput and reduce load on backend databases. Completed the main labs of the week:\nModule 06-Lab 5 – Amazon RDS: Deployed an RDS instance, connected from a client, executed basic SQL queries. Observed the impact of configuration changes (instance class, storage, backup settings) on behavior and management. Module 06-Lab 43 – Database Schema Conversion \u0026amp; Migration: Used AWS Schema Conversion Tool (SCT) to assess and convert schemas, identifying what can be auto-converted vs. what needs manual adjustment. Used AWS Database Migration Service (DMS) to migrate data from a source database into RDS/Aurora/Redshift targets according to the lab scenario. After Week 6, established a clear mental model of the AWS database ecosystem:\nFrom traditional database concepts → RDS/Aurora for OLTP → Redshift for OLAP → ElastiCache for caching → DMS/SCT for migration, ready to apply in real-world architectures. "},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.5-week5/","title":"Worklog Week 5","tags":[],"description":"","content":"Week 5 Objectives This week, my main goal was to master the concepts and security services in AWS, including the shared responsibility model, access management, encryption, and resource protection.\nI also familiarized myself with various AWS tools and services to apply them in hands-on practice.\nSpecific objectives include:\nUnderstand the Shared Responsibility Model of AWS. Master key AWS security services: IAM, Cognito, Security Hub, KMS, Identity Center. Improve resource management and security through IAM Permissions Boundaries, Resource Tags, and encryption techniques. Weekly Tasks Day Task Start Date Completion Date Reference Source 2 - Study the theory of the Shared Responsibility Model and AWS security principles. - Review documentation for AWS security services: + Amazon IAM + Amazon Cognito + AWS Identity Center + AWS KMS + AWS Security Hub 04/10/2025 04/10/2025 AWS Study Group 3 - Hands-on: + Configure and use AWS Security Hub to monitor and detect security issues. + Create and manage IAM Users, Roles, and Policies for AWS accounts. + Create IAM Groups and manage access permissions for user groups. 05/10/2025 05/10/2025 AWS Study Group 4 - Hands-on: + Optimize EC2 costs using Lambda for automated start/stop of EC2 instances. + Manage EC2 access via Resource Tags using IAM. + Configure IAM Permission Boundaries to limit user privileges. + Encrypt data using AWS KMS. 06/10/2025 06/10/2025 AWS Study Group 5 - Advanced Practice: + Learn and apply security methods in AWS Organizations for multi-account management. + Enhance proficiency in AWS Identity Center for managing and synchronizing users and groups across AWS services. 07/10/2025 07/10/2025 AWS Study Group Results Achieved in Week 5 During this week, I achieved significant progress in understanding and applying AWS security services, effectively bridging theory with practice. Specifically:\nUnderstanding and Applying the AWS Shared Responsibility Model\nI fully grasped that AWS is responsible for the security of the cloud infrastructure, while users are responsible for securing their own data and applications. This clarified my role in ensuring compliance and protection when deploying services on AWS. Theoretical Knowledge of AWS Core Security Services\nAmazon IAM: Learned how to create and manage Users, Roles, and Policies to control user and group access. Amazon Cognito: Studied user management and authentication for AWS applications. AWS Identity Center: Understood how to link and manage user access across multiple AWS services. AWS Security Hub: Configured and utilized it to monitor and detect security threats. AWS KMS: Practiced encrypting data at rest and securing sensitive data using encryption keys. Practical Implementation of AWS Security Services\nSuccessfully installed and configured AWS Security Hub for continuous monitoring and vulnerability detection. Configured IAM Permissions Boundaries to restrict user privileges and prevent unauthorized access. EC2 Cost Optimization with Lambda: Automated the shutdown of unused EC2 instances to minimize operational costs. EC2 Access Control via IAM \u0026amp; Resource Tags: Applied IAM Policies that use Tags to precisely define access scope. Enhanced AWS Resource Management Skills\nCreated and managed IAM Groups and Policies, improving group-based access control. Learned how to manage multiple AWS accounts using AWS Organizations, ensuring consistent security policies across the organization. LAB PRACTICE Table of Contents Lab 18 Lab 22 VPC EC2 Slack Lambda + EventBridge Test Results Lab 27 Lab 28 Regions \u0026amp; EC2 Tags Lab 30 Lab 33 Lab 18 Illustration:\nLab 22 VPC VPC configuration illustrations:\nEC2 EC2 configuration illustrations:\nSlack Note (New UI): Re-select the channel during setup to get the correct Webhook URL.\nLambda + EventBridge Lambda and EventBridge configuration illustrations:\nTest Results Lab 27 Illustrations:\nLab 28 Illustrations:\nRegions \u0026amp; EC2 EC2 in ap-northeast-1 (Tokyo)\nEC2 in us-east-1 (North Virginia)\nTags Sample key/value pairs used:\nKey Value Name Example Team Beta Team Alpha Team TEST Tag interface examples:\nName = Example, Team = Beta\nName = Example, Team = Alpha\nTeam = TEST\nLab 30 Policies: IAM: Check Permission: Lab 33 Policies Role User KMS S3 CloudTrail Athena Test after applying ACLs Conclusion During Week 5, I significantly improved my ability to use AWS security and access management tools.\nThese knowledge and skills form a solid foundation for implementing advanced security solutions and cost optimization in future AWS projects.\nThe lab exercises reinforced theoretical understanding and enhanced my practical skills for working effectively in real AWS environments.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.4-week4/","title":"Worklog Week 4","tags":[],"description":"","content":"Week 4 Objectives Gain an in-depth understanding of AWS’s core storage service Amazon S3. Master the key concepts: bucket, object, storage class, access point, static website hosting, and CORS. Study hybrid storage and data migration solutions such as AWS Storage Gateway and AWS Snow Family. Get familiar with Amazon FSx for Windows File Server and the automated backup service AWS Backup. Practice deploying, managing, and integrating AWS storage services in a real-world environment. Tasks to Be Completed in Week 4 Day Task Start Date Completion Date References 2 - Study the theory of AWS Storage Service (S3) – Module 04-01.\n- Get familiar with the concepts of Bucket, Object, and the storage mechanism. 29/09/2025 29/09/2025 https://docs.aws.amazon.com/s3/ 3 - Learn about Access Point and Storage Class in S3 – Module 04-02.\n- Distinguish between storage classes: Standard, IA, Glacier, Deep Archive. 30/09/2025 30/09/2025 https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html 4 - Explore S3 Static Website \u0026amp; CORS, Access Control, Object Key, Performance, and Glacier – Module 04-03. 01/10/2025 01/10/2025 https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html 5 - Hands-on: Module 04-Lab13 – Deploy AWS Backup to the System.\n- Hands-on: Module 04-Lab14 – VM Import/Export. 02/10/2025 02/10/2025 Lab13, Lab14 6 - Hands-on: Module 04-Lab24 – Using File Storage Gateway.\n- Hands-on: Module 04-Lab25 – Amazon FSx for Windows File Server.\n- Review and consolidate all AWS storage services. 03/10/2025 03/10/2025 Lab24, Lab25 Results Achieved in Week 4 Gained a clear understanding of the architecture and operating principles of Amazon S3, including:\nHow to create and manage Buckets, Objects, and Access Policies. Different Storage Classes and strategies for optimizing storage costs. How to configure S3 Static Website Hosting and handle CORS for web applications. Became familiar with S3 Glacier – a cold storage service that helps save costs for infrequently accessed data.\nGained a solid understanding of Hybrid Storage \u0026amp; Data Migration through:\nAWS Snow Family (Snowcone, Snowball, Snowmobile). AWS Storage Gateway – a solution to connect on-premises systems with AWS Cloud. Successfully completed the following labs:\nLab 13 – AWS Backup Goal: Configure and deploy resource backups with AWS Backup. Step 1:\nStep 2:\nStep 3:\nStep 4:\nSuccess:\nLab 14 – VM Import/Export Goal: Perform VM Import/Export – migrate virtual machines between the local environment and AWS. Step 1:\nSuccess:\nStep 2:\nStep 3:\nStep 4:\nStep 5:\nStep 6 (successfully uploaded the VM to EC2 (AMIs)):\nStep 7:\nStep 8 (Internet connectivity test):\nStep 9:\nStep 10 (Done):\nLab 24 – File Storage Gateway Goal: Configure File Storage Gateway – create and link file storage between on-premises systems and AWS. Note: The account must be upgraded.\nStep 1 – After creating the S3 bucket, create the File Storage Gateway (FSG):\nStep 2 – EC2 settings:\nStep 3:\nLab 25 – Amazon FSx for Windows File Server Goal: Deploy a file storage system for Windows using Amazon FSx for Windows File Server. Step 1 (Lambda error – Node.js version):\nStep 2:\nCompleted the entire Module 04 – AWS Storage Services, building a solid foundation to move on to compute, database, and security services in the following weeks. "},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.3-week3/","title":"Worklog Week 3","tags":[],"description":"","content":"Week 3 Objectives: Gain a solid understanding of Amazon EC2 and its ecosystem (AMI, Backup, Key Pair, EBS, Instance Store, User Data, Metadata). Learn about EC2 Auto Scaling and its role in elasticity and cost optimization. Explore related compute services including EFS, FSx, Lightsail, and AWS MGN. Strengthen AWS storage knowledge with hands-on labs covering S3, AWS Backup, and Storage Gateway. Develop practical skills in configuring, managing, and scaling EC2 workloads. Tasks for this week: Day Task Start Date Completion Date Reference 1 - Theory: + EC2 overview: AMI, Backup, Key Pair 22/09/2025 22/09/2025 AWS EC2 Documentation 2 - Theory: + EBS (Elastic Block Store) + Instance Store 23/09/2025 23/09/2025 AWS EBS Documentation 3 - Theory: + EC2 User Data + EC2 Metadata 24/09/2025 24/09/2025 AWS EC2 User Guide 4 - Theory: + EC2 Auto Scaling + Related services: EFS, FSx, Lightsail, MGN 25/09/2025 25/09/2025 AWS Auto Scaling 5 - Hands-on: + Lab 57: Start with Amazon S3 26/09/2025 26/09/2025 AWS Study Group - Lab57 6 - Hands-on: + Lab 13: Deploy AWS Backup to the System 27/09/2025 27/09/2025 AWS Study Group - Lab13 7 - Hands-on: + Lab 24: Using AWS Storage Gateway 28/09/2025 28/09/2025 AWS Study Group - Lab24 Week 3 Achievements: Built strong theoretical knowledge of Amazon EC2, including:\nAMI and backup strategies for resilience. Key Pair usage for secure SSH authentication. Differences between EBS (persistent storage) and Instance Store (ephemeral storage). How User Data and Metadata scripts automate instance initialization and provide dynamic configuration. The role of EC2 Auto Scaling in maintaining performance and cost efficiency. Expanded understanding of related services:\nAmazon EFS and FSx for shared and high-performance file storage. Amazon Lightsail as a simplified alternative for small-scale workloads. AWS MGN for migrating workloads into AWS. Completed practical labs:\nLaunched and managed an S3 bucket (Lab57). Implemented AWS Backup to protect workloads (Lab13). Integrated on-premises systems with AWS using Storage Gateway (Lab24). Key skills acquired:\nConfidently distinguish storage types (EBS vs Instance Store vs EFS vs FSx). Automate EC2 lifecycle with User Data and Auto Scaling. Combine backup and hybrid storage solutions to create more resilient architectures. Direction:\nThis week provided a deeper dive into compute and storage fundamentals. By combining theoretical concepts with hands-on practice, I strengthened my ability to not only launch and manage EC2 instances but also design scalable, reliable, and cost-efficient architectures that integrate with AWS’s broader storage and migration services.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Get familiar with the First Cloud Journey (FCJ) team. Understand AWS core services and learn how to interact with them via Console \u0026amp; CLI. Tasks completed this week: Day Task Start Date Completion Date Reference 2 - Meet and get to know FCJ members.\n- Read and understand internship rules and guidelines. 2025-09-02 2025-09-02 3 - Study AWS and its main service groups:\n+ Compute + Storage + Networking + Database + \u0026hellip; 2025-09-03 2025-09-03 https://000001.awsstudygroup.com/en/ 4 - Register AWS Free Tier account.\n- Explore AWS Console \u0026amp; CLI.\n- Practice:\n+ Create AWS account + Install \u0026amp; configure AWS CLI + Basic CLI tasks 2025-09-04 2025-09-04 https://000001.awsstudygroup.com/en/ 5 - Configure basic security:\n+ Setup Virtual MFA Device + Create admin group \u0026amp; admin user + Account authentication support + Create Budget 2025-09-05 2025-09-06 https://000007.awsstudygroup.com/en/ 6 - Practice cost management:\n+ Create Cost Budget + Create Usage Budget + Reservation Instance (RI) + Savings Plans Budget 2025-09-06 2025-09-06 https://000007.awsstudygroup.com/en/ 7 - Submit AWS support request and manage responses.\n- Write worklog \u0026amp; self-assess AWS fundamentals.\n- Prepare for Week 2 goals. 2025-09-07 2025-09-07 https://000009.awsstudygroup.com/en/ Week 1 Outcomes: Onboarding completed:\nConnected with FCJ members. Understood internship guidelines and basic rules. AWS foundation knowledge:\nLearned what AWS is and its main service categories: Compute Storage Networking Database \u0026hellip; Account setup \u0026amp; configuration:\nSuccessfully registered AWS Free Tier account. Configured basic security: enabled MFA, created admin group \u0026amp; admin user. Created budgets to monitor costs: Cost Budget, Usage Budget, RI, Savings Plans. Management tools:\nPracticed AWS Management Console: navigating and using services via GUI. Installed and configured AWS CLI with: Access Key, Secret Key, default Region. Hands-on with AWS CLI:\nChecked account and configuration info. Listed regions. Viewed EC2 information. Created and managed key pairs. Monitored running services. Console \u0026amp; CLI integration:\nManaged AWS resources in parallel using Console and CLI. Compared approaches and gained insights on when to use each tool. Personal reflection:\nCompleted Week 1 worklog (submitted late in Week 2). Assessed current understanding and set targets for the upcoming week. "},{"uri":"https://Fatnotfatt.github.io/learning-aws/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Accelerating Enterprise ML Experimentation with Amazon SageMaker AI and Comet By: Vikesh Pandey, Naufal Mir and Sarah Ostermeier \u0026ndash; Date: September 22, 2025\nTopics: Amazon SageMaker AI, SageMaker Unified Studio, Partner solutions, Sarah Ostermeier\nAs organizations scale their machine learning (ML) operations from proof-of-concept to production, managing experiments, tracking model lineage, and ensuring reproducibility become significantly more complex. The main reason is that data scientists and ML engineers often experiment with numerous combinations of hyperparameters, model architectures, and dataset versions, generating large amounts of metadata that need to be tracked to ensure reproducibility and regulatory compliance. As ML models scale across multiple teams and regulatory requirements increase, experiment tracking becomes mandatory rather than just a \u0026ldquo;best practice\u0026rdquo;.\nAmazon SageMaker AI provides managed infrastructure for enterprises to scale ML workloads, handling compute provisioning, distributed training, and deployment without worrying about infrastructure. However, teams still need a robust experiment tracking system, model comparison capabilities, and collaboration beyond basic logging.\nComet is a comprehensive ML experiment management platform that automatically tracks, compares, and optimizes ML experiments throughout the model lifecycle. It provides data scientists and ML engineers with powerful tools for tracking, model monitoring, hyperparameter optimization, and collaborative model development. It also includes Opik — Comet\u0026rsquo;s open-source platform for LLM (large language model) observability and development.\nComet is available in SageMaker AI as a Partner AI App, as a fully managed capability for experimentation, with enterprise-grade security, seamless workflow integration, and a simple procurement process through AWS Marketplace.\nThis combination addresses the needs of end-to-end enterprise ML workflows: SageMaker AI handles infrastructure and compute, while Comet provides the experiment management, model registry, and production monitoring capabilities that teams need for regulatory compliance and operational efficiency. In this article, we demonstrate a complete fraud detection workflow using SageMaker AI + Comet, showcasing the reproducibility and audit-ready logging that modern enterprises require.\nMaking Comet \u0026ldquo;Enterprise-ready\u0026rdquo; on SageMaker AI Before diving into the deployment guide, organizations need to define their operating model and determine how to deploy Comet. AWS recommends setting up Comet following a federated operating model: Comet is centrally managed in a shared services account, and each ML data team has its own autonomous environment. Each operating model has its own pros and cons. (Refer to SageMaker Studio Administration Best Practices for details).\nIn this architecture, there are typically two roles:\nAdministrator \u0026ndash; responsible for setting up shared infrastructure and environments for use-case teams\nUser \u0026ndash; ML practitioners from use-case teams who use the established environment to solve business problems\nComet works well with both SageMaker AI and Amazon SageMaker (SageMaker AI uses the integrated environment in SageMaker Studio IDE; SageMaker uses Unified Studio IDE). Here, we use SageMaker Studio in the example.\nAdministrator Journey When a team wants to deploy a fraud detection use-case, the admin performs:\nComplete the prerequisite steps to set up Partner AI Apps \u0026mdash; grant permissions so Comet can assume the user\u0026rsquo;s SageMaker AI role and manage Comet subscription through AWS Marketplace.\\\nIn the SageMaker AI console, go to Applications and IDEs → Partner AI Apps → Comet to view details.\nDisplays subscription details, pricing model, and estimated Comet infrastructure costs. Select Go to Marketplace to subscribe to Comet from AWS Marketplace.\nSelect \u0026ldquo;View purchase options\u0026rdquo; and fill in the subscription information.\nAfter subscription is complete, the admin begins configuring Comet.\nWhen deploying Comet, add the fraud detection team\u0026rsquo;s project lead as an admin to manage the Comet dashboard. The Comet deployment process takes a few minutes. (Refer to the Partner AI App provisioning guide for details).\nSet up the SageMaker AI domain following the Use custom setup for Amazon SageMaker AI guide. As a best practice, provide a pre-signed domain URL so the use-case team can access the Comet UI without logging into the SageMaker console.\nAdd team members to the domain and enable Comet access when configuring the domain.\nAfter these steps, the SageMaker AI domain is ready for users to log in and begin working.\nUser Journey (ML Practitioner) When the environment is ready, the user performs:\nLog into the SageMaker AI domain via the pre-signed URL.\nAutomatically redirects to SageMaker Studio IDE, with username and IAM execution role pre-configured by the admin. Create a JupyterLab Space following the JupyterLab user guide.\nBegin the fraud detection use-case by launching a notebook.\nThe admin has already granted data access through the necessary S3 buckets. To use Comet\u0026rsquo;s API, install the comet_ml package and configure environment variables following the Set up Partner AI Apps SDKs guide.\nIn SageMaker Studio, select Partner AI Apps → Open Comet to access the Comet UI.\nBegin the experiment workflow. Solution Overview This use-case highlights common challenges in enterprises:\nImbalanced datasets (e.g., only ~0.17% of transactions are fraudulent)\nMultiple iteration rounds\nRequirements for complete reproducibility and audit compliance\nData \u0026amp; model lineage must be recorded in detail\nUsing the Credit Card Fraud Detection dataset, with binary labels \u0026mdash; 1 for fraud, 0 for legitimate. The following steps illustrate the key parts of the implementation (complete code is available in Comet\u0026rsquo;s GitHub repo).\nPrerequisites Configure the imports and Comet + SageMaker environment variables:\n# Comet ML for experiment tracking\nimport comet_ml\nfrom comet_ml import Experiment, API, Artifact\nfrom comet_ml.integration.sagemaker import log_sagemaker_training_job_v1\nAWS_PARTNER_APP_AUTH = True\nAWS_PARTNER_APP_ARN = \u0026lt;Your_AWS_PARTNER_APP_ARN\u0026gt;\nCOMET_API_KEY = \u0026lt;Your_Comet_API_Key\u0026gt;\nCOMET_WORKSPACE = \u0026lsquo;\u0026lt;your-comet-workspace-name\u0026gt;\u0026rsquo;\nCOMET_PROJECT_NAME = \u0026lsquo;\u0026lt;your-comet-project-name\u0026gt;\u0026rsquo;\nThe AWS_PARTNER_APP_ARN and COMET_API_KEY variables are obtained from the Comet details page in SageMaker.\nCOMET_WORKSPACE and COMET_PROJECT_NAME are the workspace and project names you will use to group experiments.\nPreparing the Dataset A key feature of Comet is automatic dataset versioning \u0026amp; lineage tracking. This enables full audit trails of which data was used to train each model \u0026mdash; crucial in regulated environments.\nExample:\n# Create dataset Artifact to track the original dataset\ndataset_artifact = Artifact(\nname=\u0026ldquo;fraud-dataset\u0026rdquo;,\nartifact_type=\u0026ldquo;dataset\u0026rdquo;,\naliases=[\u0026ldquo;raw\u0026rdquo;]\n)\ndataset_artifact.add_remote(s3_data_path, metadata={\n\u0026ldquo;dataset_stage\u0026rdquo;: \u0026ldquo;raw\u0026rdquo;,\n\u0026ldquo;dataset_split\u0026rdquo;: \u0026ldquo;not_split\u0026rdquo;,\n\u0026ldquo;preprocessing\u0026rdquo;: \u0026ldquo;none\u0026rdquo;\n})\nArtifact allows tagging of dataset files and associated metadata\nRaw data is added to the artifact for Comet to track the dataset source\nStarting a Comet Experiment After the artifact has been logged, you start an experiment and Comet will automatically record background metadata, environment, libraries, code, etc.\nexperiment_1 = comet_ml.Experiment(\nproject_name=COMET_PROJECT_NAME,\nworkspace=COMET_WORKSPACE,\n)\nexperiment_1.log_artifact(dataset_artifact)\nExperiment automatically begins recording information\nlog_artifact logs the dataset artifact to the experiment for traceability\nData Preprocessing Preprocessing steps include:\nRemoving duplicate records\nDropping unnecessary columns\nSplitting data into train/validation/test sets\nFeature standardization using scikit-learn\u0026rsquo;s StandardScaler\nThe preprocessing code is written in the preprocess.py file and run as a SageMaker Processing Job:\nprocessor = SKLearnProcessor(\nframework_version=\u0026lsquo;1.0-1\u0026rsquo;,\nrole=sagemaker.get_execution_role(),\ninstance_count=1,\ninstance_type=\u0026lsquo;ml.t3.medium\u0026rsquo;\n)\nprocessor.run(\ncode=\u0026lsquo;preprocess.py\u0026rsquo;,\ninputs=[ProcessingInput(source=s3_data_path, destination=\u0026rsquo;/opt/ml/processing/input\u0026rsquo;)],\noutputs=[ProcessingOutput(source=\u0026rsquo;/opt/ml/processing/output\u0026rsquo;, destination=f\u0026rsquo;s3://{bucket_name}/{processed_data_prefix}\u0026rsquo;)]\n)\nWhen the job starts, SageMaker AI creates an instance, processes the data, and then releases the resource.\nPreprocessing results are saved to S3.\nAfter completion, you create a new version of the dataset artifact to track the processed data:\npreprocessed_dataset_artifact = Artifact(\nname=\u0026ldquo;fraud-dataset\u0026rdquo;,\nartifact_type=\u0026ldquo;dataset\u0026rdquo;,\naliases=[\u0026ldquo;preprocessed\u0026rdquo;],\nmetadata={\n\u0026ldquo;description\u0026rdquo;: \u0026ldquo;Credit card fraud detection dataset\u0026rdquo;,\n\u0026ldquo;fraud_percentage\u0026rdquo;: f\u0026quot;{fraud_percentage:.3f}%\u0026quot;,\n\u0026ldquo;dataset_stage\u0026rdquo;: \u0026ldquo;preprocessed\u0026rdquo;,\n\u0026ldquo;preprocessing\u0026rdquo;: \u0026ldquo;StandardScaler + train/val/test split\u0026rdquo;,\n}\n)\npreprocessed_dataset_artifact.add_remote(\nuri=f\u0026rsquo;s3://{bucket_name}/{processed_data_prefix}',\nlogical_path=\u0026lsquo;split_data\u0026rsquo;\n)\nexperiment_1.log_artifact(preprocessed_dataset_artifact)\nArtifacts with the same name but different aliases allow Comet to manage versioning\nAdditional metadata helps document what was done (split, preprocessing\u0026hellip;)\nComet + SageMaker AI Experiment Workflow To accelerate rapid experimentation, you should organize the workflow into utility functions that can be called multiple times with different hyperparameters while ensuring consistent logging and evaluation.\nKey functions:\ntrain() \u0026mdash; creates an XGBoost training job on SageMaker:\nestimator = Estimator(\nimage_uri=xgboost_image,\nrole=execution_role,\ninstance_count=1,\ninstance_type=\u0026lsquo;ml.m5.large\u0026rsquo;,\noutput_path=model_output_path,\nsagemaker_session=sagemaker_session_obj,\nhyperparameters=hyperparameters_dict,\nmax_run=1800 # maximum time (seconds)\n)\nestimator.fit({\n\u0026rsquo;train\u0026rsquo;: train_channel,\n\u0026lsquo;validation\u0026rsquo;: val_channel\n})\nlog_training_job() \u0026mdash; logs training metadata to Comet and links the model: log_sagemaker_training_job_v1(\nestimator=training_estimator,\nexperiment=api_experiment\n)\nlog_model_to_comet() \u0026mdash; logs model artifact to Comet: experiment.log_remote_model(\nmodel_name=model_name,\nuri=model_artifact_path,\nmetadata=metadata\n)\ndeploy_and_evaluate_model() \u0026mdash; deploys endpoint and evaluates, logs metrics: predictor = estimator.deploy(initial_instance_count=1, instance_type=\u0026ldquo;ml.m5.xlarge\u0026rdquo;)\nexperiment.log_metrics(metrics)\nexperiment.log_confusion_matrix(matrix=cm, labels=[\u0026lsquo;Normal\u0026rsquo;, \u0026lsquo;Fraud\u0026rsquo;])\nfpr, tpr, _ = roc_curve(y_test, y_pred_prob_as_np_array)\nexperiment.log_curve(\u0026ldquo;roc_curve\u0026rdquo;, x=fpr, y=tpr)\nComplete prediction and evaluation code is available in the GitHub repo. Running the Experiments You can try multiple experiments by calling utility functions with different hyperparameter configurations and comparing results to select the optimal configuration for your use-case.\nFor example, the first experiment (baseline):\nhyperparameters_v1 = {\n\u0026lsquo;objective\u0026rsquo;: \u0026lsquo;binary:logistic\u0026rsquo;,\n\u0026rsquo;num_round\u0026rsquo;: 100,\n\u0026rsquo;eval_metric\u0026rsquo;: \u0026lsquo;auc\u0026rsquo;,\n\u0026rsquo;learning_rate\u0026rsquo;: 0.15,\n\u0026lsquo;booster\u0026rsquo;: \u0026lsquo;gbtree\u0026rsquo;\n}\nestimator_1 = train(\nmodel_output_path=f\u0026quot;s3://{bucket_name}/{model_output_prefix}/1\u0026quot;,\nexecution_role=role,\nsagemaker_session_obj=sagemaker_session,\nhyperparameters_dict=hyperparameters_v1,\ntrain_channel_loc=train_channel_location,\nval_channel_loc=validation_channel_location\n)\nlog_training_job(experiment_key = experiment_1.get_key(), training_estimator=estimator_1)\nlog_model_to_comet(\nexperiment = experiment_1,\nmodel_name=\u0026ldquo;fraud-detection-xgb-v1\u0026rdquo;,\nmodel_artifact_path=estimator_1.model_data,\nmetadata=metadata\n)\ndeploy_and_evaluate_model(\nexperiment=experiment_1,\nestimator=estimator_1,\nX_test_scaled=X_test_scaled,\ny_test=y_test\n)\nWhen running a Comet experiment from a Jupyter notebook, you need to call experiment_1.end() to ensure all information is recorded and saved to the Comet server.\nAfter the baseline experiment completes, you can launch the next experiment with different hyperparameters and compare the two experiments in the Comet UI.\nViewing Experiments in Comet UI To access the UI, you can get the URL from SageMaker Studio IDE or print it from the notebook using experiment_2.url.\nScreenshots of the Comet interface show experiments being compared \u0026mdash; these details are for illustration purposes and do not represent actual experiments.\n(Note: insert Comet UI screenshot here)\nClean Up Due to the ephemeral nature of SageMaker infrastructure (processing, training) \u0026mdash; it automatically shuts down after the job completes. However, you still need to:\nShut down JupyterLab Space when not in use (following the Idle shutdown guide).\nUnsubscribe from Comet if not continuing to use it (to avoid charges) \u0026mdash; the subscription will auto-renew if not cancelled.\nBenefits of SageMaker + Comet Integration Streamlined Model Development The SageMaker-Comet combination reduces manual burden when running experiments. While SageMaker handles infrastructure provisioning, Comet automatically logs hyperparameters, metrics, code, libraries, system information \u0026mdash; no additional configuration needed.\nComet supports visualization beyond simple metric charts: integrated charts enable quick experiment comparison; custom Python panels help you debug model behavior, optimize hyperparameters, or create custom visuals when default tools don\u0026rsquo;t meet needs.\nEnterprise Collaboration \u0026amp; Governance In enterprise environments, this combination creates a strong foundation for scaling ML projects in heavily regulated environments. SageMaker ensures consistent, secure ML environments; Comet supports collaboration with complete artifact flows and lineage. This helps avoid errors when teams cannot reproduce previous results.\nComplete ML Lifecycle Integration Unlike fragmented solutions that only support training or monitoring, SageMaker + Comet supports the entire ML lifecycle.\nModels can be registered in Comet\u0026rsquo;s model registry with versioning and management.\nSageMaker handles deployment.\nComet maintains lineage and approval workflow for promotion.\nComet monitors model performance and tracks data drift after deployment \u0026mdash; creating a feedback loop where information from production influences subsequent experiments.\nConclusion In this article, we presented how to integrate SageMaker and Comet to create a fully managed ML environment supporting reproducibility and experiment tracking. To complement your SageMaker workflow, you can deploy Comet directly within the SageMaker environment through AWS Marketplace.\nAbout the Authors "},{"uri":"https://Fatnotfatt.github.io/learning-aws/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Using Apache Airflow workflows to orchestrate data processing on Amazon SageMaker Unified Studio by Vinod Jayendra, Kamen Sharlandjiev, Sean Bjurstrom, and Suba Palanisamy\nSEPTEMBER 22, 2025\nOrchestrating machine learning pipelines is a complex task, especially when data processing, model training, and deployment are performed across multiple services and tools. In this article, we will walk through a practical \u0026ldquo;end-to-end\u0026rdquo; example — building, testing, and running an ML pipeline using SageMaker workflows through the SageMaker Unified Studio interface. These workflows are powered by Amazon Managed Workflows for Apache Airflow (Amazon MWAA).\nAlthough SageMaker Unified Studio has a visual builder (low-code) to create workflows, this article focuses on the code-based approach: writing and managing workflows as DAGs (Directed Acyclic Graphs) using Python in Apache Airflow.\nWe will examine a pipeline example that includes the following steps: ingesting weather data and taxi data, transforming \u0026amp; merging the data, then using ML to predict taxi fares — all orchestrated through SageMaker Unified Studio workflows.\nSolution overview This solution demonstrates how to use workflows in SageMaker Unified Studio to orchestrate a pipeline from data to ML model in a centralized environment. The pipeline consists of the following tasks:\nIngest \u0026amp; preprocess weather data\nUse a notebook in SageMaker Unified Studio to ingest simulated weather data, processing attributes such as time, temperature, rainfall, humidity, and wind speed.\nIngest, process, and merge taxi data\nUse a second notebook to ingest NYC taxi data (including pickup time, drop-off time, distance, passenger count, fare amount). Then process and join the taxi \u0026amp; weather data, saving the results to Amazon S3 for use in the next step.\nTrain and predict ML model\nA third notebook applies regression techniques to build a model that predicts taxi fares based on the merged data. The model is then used to make predictions on new data.\nThrough this approach, ETL (extract, transform, load) and ML steps are orchestrated within the same workflow, with full traceability of the data process and ensuring reproducibility through workflow management in SageMaker Unified Studio.\nPrerequisites Before building the workflow, you need to:\nCreate a SageMaker Unified Studio domain — follow AWS guidelines.\n(Create an Amazon SageMaker Unified Studio domain – quick setup)\nLog in to SageMaker Unified Studio domain — use the domain you created.\n(Access Amazon SageMaker Unified Studio)\nCreate a project in SageMaker Unified Studio — when creating the project, select the \u0026ldquo;All capabilities\u0026rdquo; profile to support full workflow functionality.\n(project creation guide)\nSet up workflow environment You can use workflows in SageMaker Unified Studio to set up and run a series of tasks such as notebooks, querybooks, and jobs. Workflows are written in Python code (Airflow DAGs), then you can access the Airflow UI from SageMaker for monitoring.\nSpecific steps:\nIn your project, go to Compute → Workflow environment.\nSelect Create environment to set up a new workflow environment.\nBy default, SageMaker Unified Studio will use the mw1.micro environment type — suitable for small testing.\nIf needed, you can override the default configuration (e.g., increase resources) when creating the project or adjust in blueprint deployment settings.\nDevelop workflows Workflows allow you to orchestrate notebooks, querybooks, etc. within the project. You can write Python DAGs, test, and share them with other members.\nExample:\nDownload 3 sample notebooks: Weather Data Ingestion, Taxi Ingest \u0026amp; Join, Prediction to your machine.\nIn SageMaker Unified Studio, go to Build → JupyterLab, upload the 3 notebooks.\nConfigure space: stop the current space → change instance type (e.g., ml.m5.8xlarge) → restart space.\nGo to Build → Orchestration → Workflows, select \u0026ldquo;Create new workflow\u0026rdquo; → select \u0026ldquo;Create in code editor\u0026rdquo;.\nIn the editor, create a new Python file multinotebook_dag.py in the src/workflows/dags folder. Paste the following example DAG code (modify \u0026lt;REPLACE-OWNER\u0026gt; and notebook paths accordingly):\nfrom airflow.decorators import dag\nfrom airflow.utils.dates import days_ago\nfrom workflows.airflow.providers.amazon.aws.operators.sagemaker_workflows import NotebookOperator\nWORKFLOW_SCHEDULE = \u0026lsquo;@daily\u0026rsquo;\nNOTEBOOK_PATHS = [\n\u0026lsquo;\u0026lt;FULL_PATH/Weather_Data_Ingestion.ipynb\u0026gt;\u0026rsquo;,\n\u0026lsquo;\u0026lt;FULL_PATH/Taxi_Weather_Data_Collection.ipynb\u0026gt;\u0026rsquo;,\n\u0026lsquo;\u0026lt;FULL_PATH/Prediction.ipynb\u0026gt;\u0026rsquo;\n]\ndefault_args = {\n\u0026lsquo;owner\u0026rsquo;: \u0026lsquo;\u0026lt;REPLACE-OWNER\u0026gt;\u0026rsquo;,\n}\n@dag(\ndag_id=\u0026lsquo;workflow-multinotebooks\u0026rsquo;,\ndefault_args=default_args,\nschedule_interval=WORKFLOW_SCHEDULE,\nstart_date=days_ago(2),\nis_paused_upon_creation=False,\ntags=[\u0026lsquo;MLPipeline\u0026rsquo;],\ncatchup=False\n)\ndef multi_notebook():\nprevious_task = None\nfor idx, notebook_path in enumerate(NOTEBOOK_PATHS, 1):\ncurrent_task = NotebookOperator(\ntask_id=f\u0026quot;Notebook{idx}task\u0026quot;,\ninput_config={\u0026lsquo;input_path\u0026rsquo;: notebook_path, \u0026lsquo;input_params\u0026rsquo;: {}},\noutput_config={\u0026lsquo;output_formats\u0026rsquo;: [\u0026lsquo;NOTEBOOK\u0026rsquo;]},\nwait_for_completion=True,\npoll_interval=5\n)\nif previous_task: previous_task \u0026gt;\u0026gt; current_task previous_task = current_task multi_notebook()\nNotebookOperator is used to run each notebook, with dependencies to ensure execution order.\nYou can customize WORKFLOW_SCHEDULE (e.g., @daily, @hourly, or cron expression).\nAfter the workflow environment is created and the DAG file is synced to the project, project members can view and run the shared workflow.\nTest and monitor workflow Go to Build → Orchestration → Workflows, you will see the workflow running on schedule or triggered.\nWhen the workflow completes, the status changes to \u0026ldquo;success\u0026rdquo;.\nYou can view each execution for details, logs of each task.\nAccess the Airflow UI from SageMaker to view DAGs, run history, detailed logs.\ny\nResults \u0026amp; outputs The model results are written to the results directory on Amazon S3. You need to check:\nPrediction accuracy\nConsistency in relationships between variables\nIf there are anomalous results, review the data processing steps, pipeline, and model assumptions.\nClean up To avoid unnecessary costs, you should delete the created resources:\nSageMaker Unified Studio domain\nS3 buckets related to the domain\nWorkflow environments, projects if no longer in use\nConclusion In this article, we demonstrated how you can use SageMaker Unified Studio to build an integrated ML workflow, including:\nCreating a SageMaker Unified Studio project\nUsing multi-compute notebooks to process data\nBuilding a DAG workflow in Python to orchestrate the entire pipeline\nRunning and monitoring workflows in SageMaker Unified Studio\nSageMaker provides a comprehensive toolset to execute steps from data preparation, model training to deployment. When used through SageMaker Unified Studio, these tools are consolidated in a single working environment, helping eliminate friction between disparate tools.\nAs organizations build complex data applications, teams can use SageMaker + Unified Studio to collaborate effectively and operate AI/ML with high reliability. You can discover data, build models, and orchestrate workflows in a managed and controlled environment.\nAbout the authors "},{"uri":"https://Fatnotfatt.github.io/learning-aws/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Migrating full-text search from SQL Server to Amazon Aurora PostgreSQL-Compatible Edition or Amazon RDS for PostgreSQL Authors: Sivaprasad Appana, Surya Nallu, and Saumitra Das\nPublished: August 19, 2024\nIn today\u0026rsquo;s data world, the ability to find and retrieve information from large datasets is crucial. While some database management systems (both commercial and open source) excel at handling structured data, PostgreSQL also provides powerful tools for searching unstructured or semi-structured data. PostgreSQL has built-in full-text search (FTS) capabilities, and also supports extensions like pg_trgm and pg_bigm for text search.\nTraditional queries using the LIKE, ILIKE operators or regular expressions are well-suited for finding exact strings or structured data, but have limitations when searching within large text fields such as documents, articles, or product descriptions.\nWhen migrating from a commercial database like SQL Server to PostgreSQL (such as Amazon Aurora PostgreSQL-Compatible or Amazon RDS for PostgreSQL), migrating full-text search requires modifying queries and schema structure, as the FTS implementation differs between the two systems. The AWS Schema Conversion Tool (AWS SCT) does not automatically convert full-text search related code.\nSQL Server FTS is designed to find words, phrases, or word forms (stemming) in unstructured text data. It supports fast searching, ranking, and text indexing, helping applications efficiently process large amounts of text information.\nIn this article, we will guide you on how to migrate full-text search from SQL Server to Amazon Aurora PostgreSQL by using the tsvector and tsquery data types. We will also show how to implement FTS using the pg_trgm and pg_bigm extensions.\nPrerequisites In this article, we use the AdventureWorks2019 sample database to illustrate how to migrate FTS from SQL Server 2019 Standard to PostgreSQL.\nThe main steps to set up FTS in SQL Server:\nEnable full-text search for the AdventureWorks2019 database: USE [AdventureWorks2019]\nGO\nEXEC sp_fulltext_database \u0026rsquo;enable\u0026rsquo;\nGO\nCreate a full-text catalog: CREATE FULLTEXT CATALOG DescFTSCatalog;\nGO\nA full-text catalog is a logical component for managing full-text indexes, defining word breakers and stemmers by language.\nDefine a full-text index for columns containing text data that you want to search: CREATE FULLTEXT INDEX\nON\n[AdventureWorks2019].[Production].ProductDescription\nKEY INDEX [PK_ProductDescription_ProductDescriptionID]\nON DescFTSCatalog\nGO\nUse AWS SCT and AWS Database Migration Service (AWS DMS) to migrate the AdventureWorks 2019 database from SQL Server to Amazon Aurora PostgreSQL. In this article, we migrate the Product Description table. PostgreSQL has several options for searching text: exact matching, pattern matching, regular expressions, and full-text search. In the following sections, we will guide you on how to use FTS in PostgreSQL on the migrated database to achieve similar results.\nFull-text search in PostgreSQL The LIKE, ILIKE operators and regular expressions are used in the WHERE clause for pattern matching. However, LIKE/ILIKE do not support ranking and typically ignore words such as \u0026ldquo;the\u0026rdquo;, \u0026ldquo;is\u0026rdquo;, etc. PostgreSQL provides FTS by using tsvector and tsquery, along with related functions, operators, and parameters.\ntsvector: a data type representing the processed version of text (word tokenization, removal of stop words, reduction to lexemes), optimized for fast text searching. The to_tsvector function converts text to tsvector.\ntsquery: contains one or more lexemes used for searching. Lexemes can be combined with operators to create complex search conditions. The to_tsquery or plainto_tsquery function converts search queries to tsquery.\nExample: \u0026ldquo;He is running in the park\u0026rdquo; → the words \u0026ldquo;he\u0026rdquo;, \u0026ldquo;run\u0026rdquo;, \u0026ldquo;park\u0026rdquo; after removing stop words and stemming.\nCONTAINS predicate with AND operator A simple FTS query in SQL Server uses the CONTAINS predicate. The CONTAINS predicate in Transact-SQL provides a flexible way to perform advanced FTS in SQL Server databases. It supports multiple search conditions, fuzzy searching, wildcards, and thesaurus features, allowing you to customize queries to meet specific requirements.\nIn the following sample query, the CONTAINS predicate checks for the words \u0026ldquo;entry\u0026rdquo; and \u0026ldquo;level\u0026rdquo; in the Description column:\nSELECT ProductDescriptionID,Description\nFROM [AdventureWorks2019].[Production].[ProductDescription]\nWHERE CONTAINS([Description], \u0026rsquo;entry \u0026amp; level\u0026rsquo;);\nCONTAINS predicate with OR operator This is similar to the previous use case using the CONTAINS predicate, except the check is performed using the OR operator. In the following sample query, the predicate checks for \u0026ldquo;entry\u0026rdquo;, \u0026ldquo;level\u0026rdquo;, or both:\nSELECT ProductDescriptionID,Description\nFROM [AdventureWorks2019].[Production].[ProductDescription]\nWHERE CONTAINS([Description], \u0026rsquo;entry | level\u0026rsquo;);\nYou can rewrite the query in PostgreSQL by using the to_tsvector and to_tsquery functions as follows and using the default built-in text search dictionary value of pg_catalog.simple.\nFREETEXT predicate The FREETEXT predicate in Transact-SQL (T-SQL) is used to perform full-text search in SQL Server databases. Unlike the CONTAINS function, which requires specific terms and conditions, the FREETEXT predicate allows more flexible and natural language-based searching.\nIn the following sample queries, FREETEXT checks for the words \u0026ldquo;entry\u0026rdquo; or \u0026ldquo;level\u0026rdquo; and their forms (using inflection) in the Description column:\nSELECT ProductDescriptionID, Description\nFROM [AdventureWorks2019].[Production].[Product description]\nWHERE FREETEXT([Description], \u0026rsquo;entry level\u0026rsquo;);\nYou can rewrite the query in PostgreSQL using the to_tsvector and to_tsquery functions as follows with the pg_catalog.english configuration value. This configuration uses english_stem and a simple dictionary to convert tokens to lexemes. Therefore, a lexeme represents a normalized form of a word or token that can be indexed and used for search operations.\nFREETEXTTABLE function with RANK FTS in SQL Server can generate an optional score (or rank value) that represents the relevance of data returned by the full-text query. This rank value is calculated per row and can be used as a sorting criterion to order the query result set by relevance. The rank value only shows the relative relevance order of rows in the result set. The actual value is not important and often differs each time you run the query. The rank value has no meaning between queries.\nIn the following sample queries, FREETEXTTABLE checks for the words \u0026ldquo;entry\u0026rdquo; or \u0026ldquo;level\u0026rdquo; and their forms (using inflection) in the Description column and also retrieves RANK information:\nSELECT FT_TBL.[ProductDescriptionID],FT_TBL.[Description], KEY_TBL.[RANK]\nFROM [AdventureWorks2019].[Production].[ProductDescription] FT_TBL\nINNER JOIN FREETEXTTABLE([AdventureWorks2019].[Production].[ProductDescription], [Description], \u0026rsquo;entry OR level\u0026rsquo;,1033) AS KEY_TBL\nON FT\\_TBL.[ProductDescriptionID] \\=KEY\\_TBL.[KEY] ORDER BY KEY_TBL.[RANK] DESC,FT_TBL.[ProductDescriptionID];\nIn PostgreSQL, the ts_rank function is used to calculate the relevance rank of search results based on how well they match a specific query. The rank is calculated using a numeric value indicating how well the document matches the search terms in the query.\nThe ts_headline function is used to create a text summary version of the document, highlighting the most relevant portions that match the specific search query. This function is useful for creating excerpts or search result headlines, providing context to users about why a particular document matches their search. The following screenshot shows the results of the PostgreSQL query headline column created using the ts_headline function.\n![][image5]\nCONTAINSTABLE and FORMSOF functions with RANK The FORMSOF function in SQL Server is used to perform inflectional search. Inflectional search involves searching for different forms of a word, such as plural forms, verb tenses, or related word forms. This can help you find relevant documents even when they contain variations of the search term, thus improving search accuracy.\nIn the following sample queries, CONTAINSTABLE checks for the word \u0026ldquo;gear\u0026rdquo; and its forms (using INFLECTIONAL) in the Description column and also retrieves RANK information:\nSELECT FT_TBL.[ProductDescriptionID],FT_TBL.[Description], KEY_TBL.[RANK]\nFROM [AdventureWorks2019].[Production].[ProductDescription] FT_TBL\n**INNER JOIN CONTAINSTABLE([AdventureWorks2019].[Production].[ProductDescription],** **[Description], 'FORMSOF(INFLECTIONAL,''gear'')',1033)** AS KEY_TBL\nON FT_TBL.[ProductDescriptionID] = KEY_TBL.[KEY]\nORDER BY KEY_TBL.[RANK] DESC,FT_TBL.[ProductDescriptionID];\nIn PostgreSQL queries, phrases are first broken down into words or tokens, and these words are normalized and classified into root words (lexemes) using the pg_catalog.english FTS configuration. These lexemes will be the same for different forms (inflections) of a word. Therefore, this feature automatically handles inflectional searches.\nSQL Server SELECT FT_TBL.[ProductDescriptionID],FT_TBL.[Description], KEY_TBL.[RANK] FROM [AdventureWorks2019].[Production].[ProductDescription] FT_TBL INNER JOIN CONTAINSTABLE([AdventureWorks2019].[Production].[ProductDescription], [Description], \u0026lsquo;FORMSOF(INFLECTIONAL,\u0026lsquo;\u0026lsquo;gear\u0026rsquo;\u0026rsquo;)\u0026rsquo;,1033) AS KEY_TBL ON FT_TBL.[ProductDescriptionID] = KEY_TBL.[KEY] ORDER BY KEY_TBL.[RANK] DESC,FT_TBL.[ProductDescriptionID]; ![][image6] PostgreSQL | SELECT p.productdescriptionid ,p.description, ts_rank(to_tsvector(\u0026lsquo;pg_catalog.english\u0026rsquo;,p.Description), query) AS rank, ts_headline(\u0026lsquo;pg_catalog.english\u0026rsquo;,p.Description,query) headline FROM production.productdescription p, to_tsquery(\u0026lsquo;pg_catalog.english\u0026rsquo;,\u0026lsquo;gear\u0026rsquo;) query WHERE query @@ to_tsvector(\u0026lsquo;pg_catalog.english\u0026rsquo;,p.Description) ORDER BY rank desc,p.productdescriptionid; Improving query performance in PostgreSQL For the sample PostgreSQL queries shown previously, the to_tsvector function retrieves tsvector values from the Description column in the productdescription table. In the following sections, we will introduce you to various options to improve query performance.\nSolution 1: Using GIN index A GIN (Generalized Inverted Index) in PostgreSQL is a popular indexing method used to efficiently speed up searching for complex data types such as JSON and full-text search. Standard database indexes, a B-tree, are designed to check for equality, while GIN is designed for search patterns that operate on nested or aggregate data structures, allowing more expressive search patterns. By indexing the components of complex data types separately, GIN indexes enable faster queries on arrays, JSON data, and text search operations. This makes GIN indexes a useful tool for improving the performance of queries related to complex data structures in PostgreSQL databases.\nIn this approach, you create a GIN index based on an expression on the column of interest in the product description table.\nRun the following command: CREATE INDEX productdescription_gin_idx ON production.productdescription\nUSING GIN (to_tsvector(\u0026lsquo;pg_catalog.english\u0026rsquo;, Description)); If the table has millions of rows, you can increase the maintenance_work_mem configuration parameter at the session level to speed up index creation time. maintenance_work_mem specifies the maximum amount of memory in MB that will be used for maintenance operations such as creating INDEX—by default (PostgreSQL), it is 64 MB. Run the following EXPLAIN ANALYZE query: EXPLAIN ANALYZE\nSELECT * FROM production.productdescription\nWHERE to_tsvector(\u0026lsquo;pg_catalog.english\u0026rsquo;, Description) @@ to_tsquery(\u0026lsquo;pg_catalog.english\u0026rsquo;,\u0026rsquo;entry | level\u0026rsquo;)\nORDER BY productdescriptionid DESC; The output shows that a bitmap index scan is being performed on productdescription_gin_index, improving query performance. The following screenshot shows the explain plan before creating the index.\nThe following screenshot shows the explain plan after creating the index.\nIn this case, we can see that query performance is improved when using the GIN index. Although generally, using GIN indexes for full-text search in PostgreSQL can help improve performance, you need to be aware of other performance trade-offs, including the time required to build the index and the additional storage space that the index requires.\nSolution 2: Using stored generated column In this approach, you create a computed column description_tsv containing tsvector values from the description column in the table followed by a GIN index on the computed column.\nRun the following commands: ALTER TABLE production.productdescription\nADD COLUMN description_tsv tsvector\nGENERATED ALWAYS AS (to_tsvector(\u0026lsquo;pg_catalog.english\u0026rsquo;,Description )) STORED;\nCREATE INDEX productdescription_gin_idx ON production.productdescription USING GIN(description_tsv); Run the following sample EXPLAIN ANALYZE query: EXPLAIN ANALYZE\nSELECT *\nFROM production.productdescription\nWHERE description_tsv @@ to_tsquery(\u0026lsquo;pg_catalog.english\u0026rsquo;,\u0026rsquo;entry | level\u0026rsquo;)\nORDER BY productdescriptionid DESC; The output shows that a bitmap index scan is being performed on productdescription_gin_index, in this case demonstrating improved query performance:\nFull-text search in PostgreSQL using the pg_trgm extension In PostgreSQL, the pg_trgm extension is implemented for text search functions using trigrams. A trigram is essentially a set of three consecutive characters extracted from a given string. By using trigrams, users can identify similarity or matches in text patterns within strings by comparing the number of matching trigrams between strings, along with a predefined similarity threshold parameter set before performing the search.\nThe pg_trgm extension provides operators that can be used to create trigram indexes on text columns in tables that need to be searched. This index enables efficient similarity operations on indexed columns. The extension provides three similarity operators: similarity (%), word_similarity (\u0026lt;%), and strict_word_similarity (\u0026lt;\u0026lt;%). The threshold parameters for the respective operators are pg_trgm.similarity_threshold, pg_trgm.word_similarity_threshold, and pg_trgm.strict_word_similarity_threshold, which can be set to values from 0 (no similarity) to 1 (perfect match). The similarity(), word_similarity(), and strict_word_similarity() functions are used to calculate similarity scores. You can use pg_trgm as in the following code snippet:\nRun the following command to create the pg_trgm extension:\nCREATE EXTENSION pg_trgm; Run the following command to create a GIN index on the productdescription column: CREATE INDEX productdescription_trgm_idx ON production.productdescription USING GIN (Description gin_trgm_ops); Run the following command to set the similarity_threshold configuration value to 0.2. The similarity feature will check for common trigrams between two strings and return a value from 0–1. SET pg_trgm.similarity_threshold = 0.2;\nSET enable_seqscan = off;\nSELECT productdescriptionid, Description, similarity(Description, \u0026rsquo;entry level\u0026rsquo;) AS sml\nFROM production.product description\nWHERE Description % \u0026rsquo;entry level\u0026rsquo;\nORDER BY sml DESC, Description; Run the following command to set the word_similarity_threshold configuration value to 0.6. word_similarity checks for common trigrams between strings at the word level. SET pg_trgm.word_similarity_threshold = 0.6;\nSET enable_seqscan = off;\nSELECT productdescriptionid, Description, word_similarity(\u0026rsquo;entry level\u0026rsquo;, Description) AS sml\nFROM production.productdescription\nWHERE \u0026rsquo;entry level\u0026rsquo; \u0026lt;% Description\nORDER BY sml DESC, Description; Run the following command to set the strict_word_similarity_threshold configuration value to 0.6. strict_word_similarity is similar to word_similarity but it only considers common trigrams when both words are identical. SET pg_trgm.strict_word_similarity_threshold = 0.6;\nSET enable_seqscan = off;\nSELECT productdescriptionid, Description, strict_word_similarity(\u0026lsquo;aluminum cups and hollow axle\u0026rsquo;, Description) AS sml\nFROM production.productdescription\nWHERE \u0026rsquo;entry level\u0026rsquo; \u0026lt;\u0026lt;% Description\nORDER BY sml DESC, Description; Run the following command to drop the index and enable sequential scan: DROP INDEX production.productdescription_trgm_idx; Full-text search in PostgreSQL using the pg_bigm extension The pg_bigm extension in PostgreSQL enhances full-text search capabilities, especially for languages with complex character sets such as Asian languages.\nA bigram is a group of two consecutive characters taken from a string. This extension uses a bigram indexing method, which involves dividing text into consecutive character pairs and building an index based on these bigrams. The pg_bigm extension provides the bigm_similarity() function, the bigm similarity operator = %, and the pg_bigm.similarity_limit threshold parameter. You can use pg_bigm as follows:\nRun the following command to create the pg_bigm extension. For instructions on creating extensions in Amazon RDS for PostgreSQL, refer to the article Using PostgreSQL extensions with Amazon RDS for PostgreSQL . CREATE EXTENSION pg_bigm; Run the following command to create a GIN index on the productdescription column: CREATE INDEX productdescription_bigm_idx ON production.productdescription USING gin (Description gin_bigm_ops); Run the following command to set the similarity_limit configuration value to 0.15. The similarity check finds common bigrams between two strings and returns a value from 0–1. SET pg_bigm.similarity_limit TO 0.15;\nSELECT *,bigm_similarity(Description, \u0026lsquo;%entry level%\u0026rsquo;) rank1\nFROM production.productdescription\nWHERE Description =% \u0026lsquo;%entry level%\u0026rsquo;\nORDER BY rank1 DESC; Run the following command to drop the index and enable sequential scan: DROP INDEX production.productdescription_bigm_idx;\nSET enable_seqscan = on; Conclusion In this article, we have guided you on how to migrate FTS from SQL Server to PostgreSQL and compared some common use cases. Migrating full-text search from SQL Server to PostgreSQL requires manual code rewriting. To learn more, please refer to the limitations of text search features in PostgreSQL . We have also guided you on how to use the pg_trgm and pg_bigm extensions in PostgreSQL to implement FTS.\nAbout the authors\n![][image15]\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AI-Driven Development Life Cycle: Reimagining Software Engineering\u0026rdquo; Event Objectives Explore the transformative impact of generative AI on software development Understand how to integrate AI into the software development lifecycle (SDLC) Demonstrate AI tools for automating development tasks: Amazon Q Developer and Kiro Learn how to leverage AI to increase productivity and focus on high-value work Speakers \u0026amp; Organizers Instructors:\nToan Huynh – AI-Driven Development Life Cycle overview and Amazon Q Developer demonstration My Nguyen – Kiro demonstration Coordinators:\nDiem My Dai Truong Dinh Nguyen Event Details Date: Friday, October 3, 2025 Time: 2:00 PM – 4:30 PM Location: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City Duration: 2.5 hours Agenda 2:00 PM - 2:15 PM: Welcoming 2:15 PM - 3:30 PM: AI-Driven Development Life Cycle overview and Amazon Q Developer demonstration (by Toan Huynh) 3:30 PM - 3:45 PM: Break 3:45 PM - 4:30 PM: Kiro demonstration (by My Nguyen) Key Highlights The Rise of Generative AI in Software Development Transformative shift: Generative AI reimagines how developers and organizations learn, plan, create, deploy, and manage applications securely SDLC integration: AI can be integrated across the entire software development lifecycle: architecture, development, testing, deployment, and maintenance Automation benefits: Automates undifferentiated heavy lifting tasks, allowing developers to focus on higher-value, creative work Productivity gains: Increases productivity while enabling developers to concentrate on innovative problem-solving AI-Driven Development Life Cycle End-to-end coverage: From initial architecture planning through development, testing, deployment, and ongoing maintenance Workflow transformation: How AI tools reshape traditional development workflows Best practices: Guidelines for effectively integrating AI into existing development processes Amazon Q Developer SDLC support: Comprehensive AI tool that supports the entire software development lifecycle Key capabilities: Code generation, debugging, testing, documentation, and refactoring assistance Integration: Seamless integration with popular IDEs and development environments Practical demonstration: Real-world examples of using Amazon Q Developer to accelerate development tasks Kiro AI-powered development: Introduction to Kiro as an AI development assistant Use cases: Specific scenarios where Kiro enhances developer productivity Features: Key features and capabilities demonstrated in the session Hands-on experience: Practical demonstration of Kiro in action Key Takeaways AI Integration Strategy Gradual adoption: Start with specific use cases and gradually expand AI tool integration Quality assurance: AI tools assist but human oversight and review remain critical Learning curve: Understanding AI tools requires time and practice to maximize benefits Team collaboration: AI enhances team productivity but requires clear workflows and guidelines Development Workflow Enhancement Automated tasks: Identify repetitive, low-value tasks that can be automated with AI Code quality: Use AI for code review, testing, and documentation to maintain high standards Acceleration: Leverage AI to speed up development cycles without sacrificing quality Continuous learning: AI tools evolve rapidly—stay updated with new features and best practices Productivity and Focus Value creation: Free developers from routine tasks to focus on complex problem-solving and innovation Time savings: Significant time savings in code writing, debugging, and documentation Knowledge augmentation: AI tools help bridge knowledge gaps and provide contextual assistance Scalability: AI enables teams to handle larger projects with the same resources Applying to Work Integrate Amazon Q Developer: Start using it in daily development tasks for code generation and assistance Explore Kiro: Evaluate Kiro for specific use cases in your development workflow Establish AI workflows: Define guidelines for when and how to use AI tools in team projects Measure productivity: Track improvements in development speed and code quality after AI tool adoption Share learnings: Document best practices and share experiences with team members Stay updated: Follow updates to AI development tools and incorporate new features as they become available Outcomes \u0026amp; Value Gained Attending this event provided significant value through new knowledge, skills, and practical insights that can be directly applied to current and future projects.\nNew Knowledge Acquired AI-Driven Development Concepts:\nDeep understanding of how generative AI transforms the software development lifecycle from planning to maintenance Comprehensive knowledge of integrating AI tools across different stages: architecture design, code generation, testing, deployment, and ongoing monitoring Insights into best practices for AI-assisted development, including when to leverage AI and when human judgment is critical Amazon Q Developer Expertise:\nPractical knowledge of using Amazon Q Developer for code generation, debugging, documentation, and refactoring Understanding of how to integrate Amazon Q Developer with existing IDEs and development environments Learning about specific capabilities: intelligent code suggestions, automated testing assistance, and code quality improvements Kiro Platform Understanding:\nExploration of Kiro as an AI development assistant and its unique features Knowledge of specific use cases where Kiro can enhance developer productivity Understanding of how Kiro complements other AI development tools in the workflow New Skills Developed AI Tool Integration:\nSkill: Ability to identify opportunities for AI automation in development workflows Skill: Proficiency in integrating AI tools like Amazon Q Developer into daily development tasks Skill: Capability to evaluate and select appropriate AI tools for specific project needs Development Workflow Enhancement:\nSkill: Improved ability to automate repetitive coding tasks while maintaining code quality Skill: Enhanced code review capabilities using AI-assisted analysis Skill: Better documentation practices through AI-powered documentation generation AI-Assisted Problem Solving:\nSkill: Leveraging AI for debugging and troubleshooting complex issues Skill: Using AI suggestions to improve code efficiency and best practices Skill: Balancing AI assistance with critical human review and judgment Lessons Learned Practical Insights:\nAI tools are powerful productivity multipliers but require proper understanding and workflow integration Gradual adoption strategy is more effective than attempting to integrate all AI tools at once Human oversight remains essential—AI assists but doesn\u0026rsquo;t replace developer expertise and judgment Measuring productivity gains and code quality improvements helps justify AI tool adoption Strategic Understanding:\nSuccessful AI integration requires team alignment and clear guidelines on usage AI tools evolve rapidly—staying updated with new features maximizes long-term value Identifying the right use cases is crucial—not all development tasks benefit equally from AI assistance Code quality can actually improve with AI tools when used thoughtfully and reviewed properly Contribution to Team/Projects Knowledge Sharing:\nDocumentation: Created notes and documentation of best practices from the event to share with team members Presentation: Prepared to share insights about Amazon Q Developer and Kiro with the development team Guidelines: Developed preliminary guidelines for integrating AI tools into team workflows Practical Application:\nPilot Projects: Identified specific projects where AI tools can be piloted for immediate productivity gains Workflow Improvement: Proposed integration of Amazon Q Developer for code review and documentation tasks Training: Planning to conduct internal sessions on AI-driven development practices for the team Long-term Value:\nCompetitive Advantage: Gained knowledge that positions the team to leverage cutting-edge AI development tools Efficiency Gains: Anticipated 20-30% improvement in development speed for repetitive tasks using AI tools Quality Enhancement: Potential for improved code quality through AI-assisted review and best practice suggestions Innovation: New capabilities enable the team to tackle more complex problems by offloading routine work to AI Personal Growth Technical Growth:\nExpanded understanding of modern software development practices with AI integration Developed a forward-thinking perspective on the evolution of software engineering Enhanced ability to evaluate and adopt new technologies effectively Professional Development:\nIncreased confidence in working with AI-powered development tools Improved ability to communicate technical concepts to both technical and non-technical stakeholders Strengthened network connections with other developers interested in AI-driven development Event Experience Attending the \u0026ldquo;AI-Driven Development Life Cycle: Reimagining Software Engineering\u0026rdquo; session was an eye-opening experience that provided deep insights into how generative AI is transforming software development. Key experiences included:\nUnderstanding the AI transformation Learned how generative AI marks a transformative shift in software development practices. Gained insights into integrating AI across the entire SDLC: architecture, development, testing, deployment, and maintenance. Understood how AI automation enables developers to focus on higher-value, creative tasks. Hands-on tool demonstrations Witnessed Amazon Q Developer in action, seeing how it can assist with code generation, debugging, and documentation. Explored Kiro as an AI development assistant and learned about its specific capabilities and use cases. Saw practical examples of how these tools accelerate development while maintaining code quality. Practical learning Learned about best practices for integrating AI tools into existing development workflows. Understood the importance of human oversight and quality assurance when using AI tools. Gained insights into identifying opportunities for AI automation in development processes. Networking and discussions Connected with other developers interested in AI-driven development. Exchanged ideas about practical applications of AI tools in real-world projects. Discussed challenges and opportunities in adopting AI development tools. Lessons learned AI tools are powerful assistants but human judgment and review remain essential for quality code. Successful AI integration requires gradual adoption and team training. The right AI tools can significantly boost productivity and free developers for more creative work. Staying updated with AI tool evolution is crucial for maximizing benefits. Some event photos Overall, this event opened my eyes to the potential of AI-driven development and provided practical guidance on how to leverage these powerful tools to enhance productivity, improve code quality, and focus on high-value work in software engineering.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; Event Objectives Provide comprehensive overview of AWS AI/ML services and capabilities Introduce Amazon SageMaker as an end-to-end ML platform Explore Generative AI with Amazon Bedrock Demonstrate practical applications through live demos Share best practices for AI/ML implementation in Vietnam Event Details Date: Saturday, November 15, 2025 Time: 8:30 AM – 12:00 PM Location: AWS Vietnam Office Duration: 3.5 hours (excluding lunch break) Agenda 8:30 – 9:00 AM | Welcome \u0026amp; Introduction Participant registration and networking Workshop overview and learning objectives Ice-breaker activity Overview of the AI/ML landscape in Vietnam 9:00 – 10:30 AM | AWS AI/ML Services Overview Amazon SageMaker – End-to-end ML platform Data preparation and labeling Model training, tuning, and deployment Integrated MLOps capabilities Live Demo: SageMaker Studio walkthrough 10:30 – 10:45 AM | Coffee Break 10:45 AM – 12:00 PM | Generative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan – comparison \u0026amp; selection guide Prompt Engineering: Techniques, Chain-of-Thought reasoning, Few-shot learning Retrieval-Augmented Generation (RAG): Architecture \u0026amp; Knowledge Base integration Bedrock Agents: Multi-step workflows and tool integrations Guardrails: Safety and content filtering Live Demo: Building a Generative AI chatbot using Bedrock Key Highlights Amazon SageMaker Platform Comprehensive ML Platform: Complete solution for building, training, and deploying machine learning models Data Preparation: Tools for data labeling, feature engineering, and data validation Model Training: Support for various ML frameworks and algorithms with distributed training capabilities Model Deployment: Flexible deployment options including real-time inference, batch processing, and serverless inference MLOps Integration: Built-in capabilities for model monitoring, versioning, and automated workflows Generative AI with Amazon Bedrock Foundation Model Selection: Understanding differences between Claude, Llama, and Titan models Claude: Strong reasoning and conversation capabilities Llama: Open-source models with good performance Titan: AWS-developed models optimized for specific use cases Prompt Engineering Techniques: Chain-of-Thought reasoning for complex problem solving Few-shot learning with examples Context management and prompt optimization RAG Architecture: Combining retrieval with generation for accurate, context-aware responses Knowledge base integration Vector embeddings and similarity search Document chunking strategies Bedrock Agents: Autonomous agents that can perform multi-step tasks Tool integrations and API calling Workflow orchestration Decision-making capabilities Guardrails for AI Safety: Content filtering and safety controls Harmful content detection Custom policy configurations Compliance and governance AI/ML Landscape in Vietnam Current adoption trends and opportunities Use cases specific to Vietnamese market Challenges and solutions for local businesses Success stories and case studies Key Takeaways Machine Learning Best Practices End-to-end Platform Approach: Use SageMaker for complete ML lifecycle management Data Quality First: Invest in data preparation and labeling for better model performance MLOps Integration: Implement monitoring and automated workflows from the start Model Selection Strategy: Choose the right model based on use case, not just performance metrics Generative AI Implementation Foundation Model Selection: Understand strengths of each model (Claude, Llama, Titan) for different scenarios Prompt Engineering Mastery: Chain-of-Thought and Few-shot learning significantly improve results RAG for Accuracy: Use RAG architecture when factual accuracy is critical Agent Design: Build agents that can handle multi-step workflows with proper tool integration Safety First: Always implement guardrails for content filtering and compliance Production Readiness Start Small, Scale Gradually: Begin with pilot projects before full deployment Cost Optimization: Monitor and optimize inference costs with serverless options Security \u0026amp; Compliance: Implement proper access controls and data privacy measures Continuous Improvement: Monitor model performance and iterate based on real-world feedback Applying to Work Explore SageMaker: Start with SageMaker Studio for ML experimentation and model development Implement RAG Solutions: Build knowledge bases for domain-specific applications using RAG architecture Develop Bedrock Agents: Create autonomous agents for customer service or workflow automation Prompt Engineering Practice: Apply Chain-of-Thought and Few-shot techniques to improve AI responses Deploy Guardrails: Implement content filtering and safety controls for production GenAI applications MLOps Setup: Establish model monitoring and automated deployment pipelines using SageMaker capabilities Event Experience Attending the \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; workshop was an exceptional learning experience that provided comprehensive insights into AWS\u0026rsquo;s AI and machine learning capabilities. The event combined theoretical knowledge with practical demonstrations, giving me a clear understanding of how to implement AI/ML solutions on AWS.\nLearning from comprehensive agenda The structured agenda covered everything from foundational ML concepts to advanced Generative AI implementations. Starting with SageMaker platform overview helped me understand the complete ML lifecycle before diving into GenAI specifics. The progression from traditional ML to Generative AI showed the evolution and complementary nature of these technologies. Hands-on technical exposure The SageMaker Studio walkthrough demonstrated the practical workflow of building ML models, from data preparation to deployment. I learned about data labeling tools and how they can significantly improve model accuracy with proper data quality. The MLOps capabilities showed me how to implement continuous integration and monitoring for ML models in production. Generative AI deep dive The Amazon Bedrock session was eye-opening, showing me how to leverage foundation models without training them from scratch. Prompt Engineering techniques like Chain-of-Thought reasoning and Few-shot learning were demonstrated with practical examples. Learning about RAG architecture helped me understand how to build accurate AI applications that combine retrieval with generation. The Bedrock Agents demo showed how to build autonomous AI systems that can perform complex multi-step tasks. Practical demonstrations The live demo of building a Generative AI chatbot using Bedrock gave me a complete picture of implementation from start to finish. Seeing Guardrails in action demonstrated the importance of safety and content filtering in production GenAI applications. The comparison between Claude, Llama, and Titan models helped me understand when to use each model. Networking and discussions The workshop provided excellent networking opportunities with other AI/ML enthusiasts and practitioners in Vietnam. Discussing AI/ML landscape in Vietnam gave me context-specific insights into local market opportunities and challenges. Sharing experiences with peers helped me understand real-world implementation challenges and solutions. Lessons learned SageMaker provides a complete platform that simplifies the entire ML lifecycle, from data preparation to deployment. Foundation models in Bedrock eliminate the need to train large models from scratch, significantly reducing time and cost. RAG architecture is crucial for building accurate GenAI applications that need to reference specific knowledge bases. Prompt engineering is a skill that requires practice and understanding of different techniques for optimal results. Guardrails are essential for production GenAI applications to ensure safety and compliance. Some event photos Overall, this workshop provided me with both foundational knowledge and practical skills needed to implement AI/ML and Generative AI solutions on AWS. The combination of comprehensive platform overview, detailed GenAI capabilities, and hands-on demonstrations gave me confidence to start building AI-powered applications.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: \u0026ldquo;DevOps on AWS\u0026rdquo; Event Objectives Introduce DevOps culture, principles, and key metrics Demonstrate AWS DevOps services for CI/CD pipeline automation Explore Infrastructure as Code (IaC) with CloudFormation and CDK Cover container services and microservices deployment strategies Provide monitoring and observability best practices Share real-world DevOps case studies and best practices Event Details Date: Monday, November 17, 2025 Time: 8:30 AM – 5:00 PM Location: Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh Duration: Full day (8.5 hours with breaks) Agenda Morning Session (8:30 AM – 12:00 PM) 8:30 – 9:00 AM | Welcome \u0026amp; DevOps Mindset\nRecap of AI/ML session DevOps culture and principles Benefits and key metrics (DORA, MTTR, deployment frequency) 9:00 – 10:30 AM | AWS DevOps Services – CI/CD Pipeline\nSource Control: AWS CodeCommit, Git strategies (GitFlow, Trunk-based) Build \u0026amp; Test: CodeBuild configuration, testing pipelines Deployment: CodeDeploy with Blue/Green, Canary, and Rolling updates Orchestration: CodePipeline automation Demo: Full CI/CD pipeline walkthrough 10:30 – 10:45 AM | Break\n10:45 AM – 12:00 PM | Infrastructure as Code (IaC)\nAWS CloudFormation: Templates, stacks, and drift detection AWS CDK (Cloud Development Kit): Constructs, reusable patterns, and language support Demo: Deploying with CloudFormation and CDK Discussion: Choosing between IaC tools Lunch Break (12:00 – 1:00 PM) Afternoon Session (1:00 – 5:00 PM) 1:00 – 2:30 PM | Container Services on AWS\nDocker Fundamentals: Microservices and containerization Amazon ECR: Image storage, scanning, lifecycle policies Amazon ECS \u0026amp; EKS: Deployment strategies, scaling, and orchestration AWS App Runner: Simplified container deployment Demo \u0026amp; Case Study: Microservices deployment comparison 2:30 – 2:45 PM | Break\n2:45 – 4:00 PM | Monitoring \u0026amp; Observability\nCloudWatch: Metrics, logs, alarms, and dashboards AWS X-Ray: Distributed tracing and performance insights Demo: Full-stack observability setup Best Practices: Alerting, dashboards, and on-call processes 4:00 – 4:45 PM | DevOps Best Practices \u0026amp; Case Studies\nDeployment strategies: Feature flags, A/B testing Automated testing and CI/CD integration Incident management and postmortems Case Studies: Startups and enterprise DevOps transformations 4:45 – 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up\nDevOps career pathways AWS certification roadmap Key Highlights DevOps Culture and Principles DevOps Mindset: Collaboration between development and operations teams Cultural Transformation: Breaking down silos and fostering shared responsibility Key Metrics (DORA): Measuring DevOps performance Deployment Frequency: How often deployments occur Lead Time: Time from code commit to production MTTR (Mean Time To Recovery): Time to recover from failures Change Failure Rate: Percentage of deployments causing failures Benefits: Faster delivery, improved reliability, better collaboration AWS CI/CD Pipeline Services AWS CodeCommit:\nFully managed source control service Git-based version control Integration with other AWS services Git strategies: GitFlow, Trunk-based development, feature branches AWS CodeBuild:\nFully managed build service Scalable build environments Supports multiple programming languages and build tools Build artifacts and test reports Integration with testing frameworks AWS CodeDeploy:\nAutomated deployment service Deployment strategies: Blue/Green: Zero-downtime deployments with instant rollback Canary: Gradual rollout with automatic rollback on errors Rolling: Rolling updates with configurable batch sizes Application deployment across EC2, Lambda, and on-premises AWS CodePipeline:\nFully managed continuous delivery service Visual workflow builder Integration with third-party tools Automated pipeline orchestration Approval gates and manual intervention points Infrastructure as Code (IaC) AWS CloudFormation:\nDeclarative IaC service JSON/YAML template syntax Stack management and resource provisioning Drift detection and stack updates Change sets for previewing changes Nested stacks for modular infrastructure AWS CDK (Cloud Development Kit):\nProgrammatic IaC using familiar programming languages TypeScript, Python, Java, C#, and Go support Constructs for reusable infrastructure patterns Higher-level abstractions and best practices Integration with CloudFormation CLI tools for deployment and management Choosing Between IaC Tools:\nCloudFormation: Declarative, template-based, AWS-native CDK: Programmatic, type-safe, developer-friendly Use cases and when to choose each approach Container Services on AWS Docker Fundamentals:\nContainerization benefits and use cases Microservices architecture with containers Docker image creation and optimization Multi-stage builds and best practices Amazon ECR (Elastic Container Registry):\nFully managed Docker container registry Image storage and versioning Image scanning for vulnerabilities Lifecycle policies for automated cleanup Integration with ECS and EKS Amazon ECS (Elastic Container Service):\nFully managed container orchestration Fargate (serverless) and EC2 launch types Task definitions and service configurations Auto-scaling and load balancing Service discovery and networking Amazon EKS (Elastic Kubernetes Service):\nManaged Kubernetes service Kubernetes-native orchestration Worker nodes management Add-ons and ecosystem integration Multi-tenant and namespace isolation AWS App Runner:\nSimplified container deployment Automatic scaling and load balancing Source code or container image deployment Built-in CI/CD integration Pay-per-use pricing model Monitoring \u0026amp; Observability Amazon CloudWatch:\nMetrics: Application and infrastructure metrics Logs: Centralized log management and analysis Alarms: Automated alerting and notifications Dashboards: Custom visualization of metrics and logs Insights: Automated anomaly detection Composite Alarms: Complex alerting logic AWS X-Ray:\nDistributed tracing for microservices Request flow visualization Performance bottleneck identification Service map generation Integration with Lambda, ECS, and API Gateway Trace analysis and filtering Best Practices:\nSetting up effective alerting strategies Creating meaningful dashboards On-call processes and incident response Log aggregation and analysis Metric collection and retention policies DevOps Best Practices Deployment Strategies:\nFeature Flags: Gradual feature rollouts A/B Testing: Comparing different versions Canary Deployments: Risk mitigation through gradual rollout Blue/Green Deployments: Zero-downtime updates Automated Testing:\nUnit, integration, and end-to-end testing Test automation in CI/CD pipelines Quality gates and test coverage Performance and load testing Incident Management:\nRunbook creation and maintenance Incident response procedures Postmortem analysis and learning Continuous improvement processes Key Takeaways DevOps Culture Transformation Cultural Change is Fundamental: Tools alone don\u0026rsquo;t make DevOps—culture and collaboration are key Measure What Matters: Use DORA metrics to track DevOps maturity Continuous Improvement: DevOps is a journey, not a destination Automation First: Automate repetitive tasks to focus on high-value work CI/CD Best Practices Start Simple, Scale Gradually: Begin with basic pipelines and add complexity over time Git Strategy Matters: Choose GitFlow or Trunk-based based on team size and release cadence Testing is Critical: Integrate automated testing at every stage Deployment Strategies: Use appropriate deployment strategy based on risk tolerance Infrastructure as Code: Always use IaC for reproducible and version-controlled infrastructure Container Orchestration Choose Wisely: ECS for simplicity, EKS for Kubernetes ecosystem Start with Serverless: Fargate eliminates node management overhead Optimize Images: Smaller images mean faster deployments and lower costs Security First: Scan images and use least-privilege IAM policies Observability Strategy Implement Full-Stack Observability: Metrics, logs, and traces together Proactive Monitoring: Set up alarms before incidents occur Meaningful Dashboards: Create dashboards that provide actionable insights Distributed Tracing: Essential for debugging microservices architectures Applying to Work Implement CI/CD Pipelines: Set up CodePipeline for automated deployments Adopt Infrastructure as Code: Use CloudFormation or CDK for all infrastructure Containerize Applications: Start containerizing applications for better portability Set Up Monitoring: Implement CloudWatch and X-Ray for observability Establish DevOps Practices: Create runbooks, incident response procedures, and postmortem templates Measure DevOps Metrics: Track DORA metrics to measure improvement Event Experience Attending the \u0026ldquo;DevOps on AWS\u0026rdquo; full-day workshop was an intensive and comprehensive learning experience that covered the entire DevOps spectrum from culture to implementation. The event provided both theoretical knowledge and practical demonstrations, giving me a complete understanding of implementing DevOps practices on AWS.\nLearning DevOps fundamentals The session started with DevOps mindset and culture, emphasizing that DevOps is more than just tools—it\u0026rsquo;s about collaboration and shared responsibility. I learned about DORA metrics (Deployment Frequency, Lead Time, MTTR, Change Failure Rate) and how to measure DevOps maturity. Understanding the benefits of DevOps helped me see the bigger picture beyond technical implementation. AWS CI/CD pipeline deep dive The CodeCommit, CodeBuild, CodeDeploy, and CodePipeline walkthrough showed me how to build a complete CI/CD pipeline. Learning about different Git strategies (GitFlow vs Trunk-based) helped me understand when to use each approach. The deployment strategies (Blue/Green, Canary, Rolling) demo was eye-opening, showing how to minimize risk and downtime. The live CI/CD pipeline demo demonstrated the entire workflow from code commit to production deployment. Infrastructure as Code mastery CloudFormation demonstrated how to manage infrastructure declaratively with templates. AWS CDK showed me how to write infrastructure code in familiar programming languages, making it more maintainable. The comparison between CloudFormation and CDK helped me understand when to use each tool. Learning about drift detection and change sets gave me confidence in managing infrastructure safely. Container services exploration Docker fundamentals refreshed my understanding of containerization and its benefits. Amazon ECR showed how to manage container images securely with scanning and lifecycle policies. Comparing ECS and EKS helped me understand the trade-offs between managed services and Kubernetes flexibility. AWS App Runner introduced a simpler way to deploy containers without managing infrastructure. The microservices deployment case study provided real-world insights into choosing the right container service. Monitoring and observability setup CloudWatch comprehensive coverage showed me how to collect metrics, logs, and set up alarms. AWS X-Ray distributed tracing demonstrated how to debug complex microservices architectures. The full-stack observability demo showed how to connect all monitoring pieces together. Learning about alerting best practices and on-call processes provided practical operational knowledge. Best practices and case studies Deployment strategies like feature flags and A/B testing showed advanced techniques for safe deployments. Automated testing integration demonstrated how to build quality gates into CI/CD pipelines. Incident management practices and postmortem templates provided structure for handling production issues. Case studies from startups and enterprises showed real-world DevOps transformations and lessons learned. Career and certification guidance The DevOps career pathways discussion helped me understand different roles and skill requirements. The AWS certification roadmap provided clear guidance on certifications relevant to DevOps. Understanding the career progression gave me a roadmap for professional development. Practical demonstrations Every session included live demos that showed real implementations, not just slides. The full CI/CD pipeline walkthrough demonstrated end-to-end automation. CloudFormation and CDK demos showed both approaches to infrastructure management. Container deployment comparison helped me visualize different approaches side by side. Networking and discussions The full-day format allowed for extended networking with other DevOps practitioners. Q\u0026amp;A sessions provided opportunities to get answers to specific questions. Discussing real-world challenges with peers helped me understand common pitfalls and solutions. Lessons learned DevOps is a cultural transformation that requires buy-in from both development and operations teams. Automation is essential but must be implemented thoughtfully to avoid creating technical debt. Infrastructure as Code is non-negotiable for modern DevOps practices. Monitoring and observability are crucial for maintaining production systems. Start simple and iterate rather than trying to implement everything at once. Measure everything using DORA metrics to track improvement over time. Some event photos Overall, this full-day workshop provided me with comprehensive knowledge of AWS DevOps services and best practices. The combination of cultural transformation principles, practical tool demonstrations, and real-world case studies gave me confidence to implement DevOps practices in my projects. The depth and breadth of content covered everything from CI/CD pipelines to container orchestration and observability, providing a complete foundation for building DevOps capabilities on AWS.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Võ Trường Thành Phát\nPhone Number: 0707712750\nEmail: phatvttse171823@fpt.edu.vn\nUniversity: FPT University\nMajor: Software Engineer\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Overview This worklog documents my journey through the AWS Cloud Journey internship program, where I completed a comprehensive 12-week learning and hands-on project experience. The program was structured to progressively build knowledge from fundamental AWS concepts to deploying a complete, production-ready web application architecture on AWS.\nDuration: 12 weeks (approximately 3 months)\nCompletion Date: November 2025\nFinal Project: Production-ready AWS web application with CI/CD, monitoring, and security\nWeekly Progress Week 1: Getting familiar with AWS and basic AWS services - Introduction to AWS ecosystem, console navigation, and fundamental services.\nWeek 2: Basic AWS services exploration - Deep dive into core AWS services and their use cases.\nWeek 3: Advanced AWS concepts - Exploring advanced features and service integrations.\nWeek 4: Hands-on labs and practice - Practical exercises and lab sessions to reinforce learning.\nWeek 5: Advanced labs and workshops - Complex scenarios and multi-service integrations.\nWeek 6: Database Services on AWS - Amazon RDS, Aurora, Redshift, ElastiCache, and database migration tools (DMS, SCT).\nWeek 7: Analytics and Data Lake Services - Amazon DynamoDB, AWS Glue, Amazon Athena, Amazon QuickSight, and building data lakes on AWS.\nWeek 8: Edge Layer and Frontend Infrastructure - Route 53, S3 static hosting, CloudFront CDN, AWS WAF, and ACM Certificate setup.\nWeek 9: VPC and Networking Core - VPC creation, subnets, Internet Gateway, NAT Gateway, Security Groups, IAM roles, and VPC Flow Logs.\nWeek 10: Backend and Database Deployment - EC2 backend setup, RDS database configuration, API Gateway, Cognito authentication, and Auto Scaling Group.\nWeek 11: CI/CD Pipeline and Monitoring - GitLab integration, CodePipeline, CodeBuild, SSH-less deployment, CloudWatch monitoring, CloudTrail, and SNS alerts.\nWeek 12: Project finalization and documentation - Final testing, documentation, and project handover.\nChallenges and Solutions Throughout the 12-week program, I encountered several technical challenges that required problem-solving and deeper understanding:\nChallenge 1: CloudFront Origin Access Control (OAC) Configuration Issue: Initially confused between Origin Access Identity (OAI) and the newer Origin Access Control (OAC). The OAI method was deprecated, and I needed to use OAC for S3 bucket access.\nSolution: Researched AWS documentation and learned that OAC is the recommended approach. Updated S3 bucket policies to work with OAC and configured CloudFront distribution accordingly. This required understanding the difference in permission models between OAI and OAC.\nChallenge 2: API Gateway VPC Link Setup for Private Resources Issue: Connecting API Gateway to EC2 instances in private subnet was challenging. Initially tried direct HTTP integration, but private subnet resources are not directly accessible.\nSolution: Implemented API Gateway VPC Link to establish a connection between API Gateway and the VPC. This required creating a Network Load Balancer (NLB) in the private subnet and configuring the VPC Link to point to the NLB. Learned about the importance of VPC endpoints and private connectivity patterns.\nChallenge 3: RDS Connection from EC2 in Private Subnet Issue: EC2 instance in private subnet couldn\u0026rsquo;t connect to RDS database initially. Security group rules were not properly configured, and I wasn\u0026rsquo;t using the correct RDS endpoint.\nSolution:\nVerified Security Group rules: RDS Security Group must allow inbound from EC2 Security Group on database port (3306/5432). Used AWS Secrets Manager to securely retrieve database credentials instead of hardcoding. Tested connectivity using AWS Systems Manager Session Manager to access EC2 without SSH. Challenge 4: CodeBuild and CloudFront Cache Invalidation Issue: After deploying frontend updates via CodeBuild to S3, changes weren\u0026rsquo;t reflected immediately due to CloudFront caching. Manual cache invalidation was time-consuming.\nSolution: Automated CloudFront cache invalidation in the CodeBuild buildspec.yml file. Added AWS CLI command to create invalidation after S3 upload, ensuring users see updated content immediately after deployment.\nChallenge 5: SSH-less Deployment for Backend Issue: Traditional SSH-based deployment was not secure and didn\u0026rsquo;t work well with Auto Scaling Group (instances are ephemeral). Needed a better approach for automated backend deployments.\nSolution: Implemented SSH-less deployment using AWS Systems Manager (SSM) Run Command. This allowed CodeBuild to execute deployment scripts on EC2 instances without SSH keys. Alternative approach using CodeDeploy was also explored for more complex deployment scenarios.\nChallenge 6: CloudWatch Alarms Not Triggering Issue: Created CloudWatch alarms but notifications weren\u0026rsquo;t being received. Initially, the alarm threshold was too high, and SNS topic subscriptions weren\u0026rsquo;t properly configured.\nSolution:\nAdjusted alarm thresholds based on actual application metrics (CPU \u0026gt;80% for 5 minutes, RDS connections \u0026gt;80% of max). Verified SNS topic subscriptions (email confirmation was required). Tested alarms by manually triggering conditions to ensure the notification flow worked correctly. Challenge 7: Cognito JWT Token Validation in API Gateway Issue: After setting up Cognito User Pool and Authorizer in API Gateway, API calls with JWT tokens were being rejected with 401 Unauthorized errors.\nSolution:\nVerified JWT token format and expiration. Checked Cognito User Pool App Client settings (allowed OAuth flows, callback URLs). Ensured API Gateway Authorizer was correctly configured with the Cognito User Pool ARN. Tested token generation and validation flow step-by-step. Challenge 8: Auto Scaling Group Launch Template Issues Issue: Auto Scaling Group failed to launch instances. The Launch Template referenced an AMI that wasn\u0026rsquo;t available in the target Availability Zone.\nSolution:\nCreated base AMI in the same region and Availability Zone as the Auto Scaling Group. Verified Launch Template configuration (instance type, security groups, IAM role, user data). Tested Launch Template manually before using it in Auto Scaling Group. Ensured all required resources (Security Groups, IAM roles) existed before ASG creation. Challenge 9: VPC Flow Logs Cost Optimization Issue: VPC Flow Logs were generating large amounts of data, leading to high CloudWatch Logs costs.\nSolution:\nConfigured log retention policies (7 days for detailed logs, 30 days for aggregated logs). Used S3 as destination for long-term log storage (more cost-effective than CloudWatch Logs). Implemented log filtering to capture only relevant traffic patterns. Set up lifecycle policies on S3 to transition logs to cheaper storage classes. Challenge 10: End-to-End Testing Complexity Issue: Testing the complete flow from Route 53 → CloudFront → WAF → API Gateway → EC2 → RDS was complex, and issues were hard to isolate.\nSolution:\nImplemented comprehensive logging at each layer (CloudFront access logs, API Gateway logs, EC2 application logs, RDS slow query logs). Used CloudWatch dashboards to visualize the entire request flow. Created test scripts to validate each component independently before end-to-end testing. Documented troubleshooting procedures for common issues at each layer. Key Learnings Infrastructure as Code (IaC): Learned the importance of using CloudFormation for reproducible infrastructure deployments.\nSecurity Best Practices: Implemented least-privilege IAM policies, network segmentation, and secure credential management with Secrets Manager.\nMonitoring and Observability: Established comprehensive monitoring with CloudWatch, CloudTrail, and VPC Flow Logs for security and performance insights.\nCI/CD Automation: Automated deployment pipelines reduced manual errors and improved deployment speed.\nCost Optimization: Learned to balance performance, security, and cost through proper resource sizing, caching strategies, and log retention policies.\nConclusion This 12-week journey provided hands-on experience with a wide range of AWS services and best practices. The final project demonstrates a production-ready architecture with proper security, monitoring, automation, and scalability. The challenges encountered and resolved during this period have significantly strengthened my understanding of cloud architecture and AWS services.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.2-week2/","title":"Worklog Week 2","tags":[],"description":"","content":"Week 2 Objectives: Understand the concept and structure of VPC (CIDR, Subnet, Route Table, ENI). Learn how to configure firewalls in VPC (NACL, Security Group). Get familiar with networking services: VPN, Direct Connect. Study and practice Load Balancer. Practice creating and configuring core components: VPC, Subnet, Route Table, IGW, EBS, Elastic IP. Learn how to connect and remote into EC2 via SSH. Get hands-on with Hybrid DNS using Route 53 Resolver. Practice connecting multiple VPCs using VPC Peering. Deploy AWS Transit Gateway to manage inter-VPC connections. Tasks for this week: Day Task Start Date Completion Date Reference 2 - Learn theory\n- What is VPC and how to optimize cloud service usage 15/09/2025 15/09/2025 AWS VPC Documentation 3 - Learn about VPC\n+ Subnet, CIDR + Route table + ENI (Elastic Network Interface) 16/09/2025 16/09/2025 YouTube - AWS VPC 4 - Configure VPC firewalls: NACL, Security Group\n- VPN, Direct Connect\n- Load Balancer\n- Extra Resources 17/09/2025 17/09/2025 YouTube - AWS Security 5 - Hands-on: + VPC + Subnet\n+ Route Table\n+ IGW\n+ EBS\n+ \u0026hellip;\n- Remote SSH into EC2\n- Learn Elastic IP 18/09/2025 18/09/2025 AWS Study Group - 000003 6 - Hands-on: + Set up Hybrid DNS with Route 53 Resolver 19/09/2025 19/09/2025 AWS Study Group - 000010 7 - Hands-on: + Set up VPC Peering 19/09/2025 19/09/2025 AWS Study Group - 000019 8 - Hands-on: + Set up AWS Transit Gateway 19/09/2025 19/09/2025 AWS Study Group - 000020 Week 2 Goals: Build a strong understanding of Amazon VPC, focusing on its key components: CIDR blocks, subnets, route tables, and ENIs. Learn how to secure VPCs using both Security Groups and Network ACLs, and understand their differences in scope and use cases. Explore AWS networking services such as VPN and Direct Connect to understand options for hybrid and enterprise connectivity. Study Elastic Load Balancing and its role in distributing traffic for high availability. Gain hands-on practice with VPC essentials: creating subnets, configuring route tables, attaching internet gateways, working with EBS, and managing Elastic IPs. Strengthen skills in accessing and managing EC2 instances securely via SSH. Implement advanced networking scenarios, including Hybrid DNS resolution with Route 53 Resolver. Practice connecting multiple VPCs through VPC Peering. Deploy AWS Transit Gateway to design and manage scalable multi-VPC architectures. Direction: The goal of Week 2 was not only to expand theoretical knowledge of AWS networking but also to translate that knowledge into practical experience, laying a foundation for building and managing complex cloud infrastructures.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.3-backend-deployment/5.3.1-build-upload/","title":"Build and Upload Backend","tags":[],"description":"","content":"Build Spring Boot Application In this section, you will build the Spring Boot backend application into a JAR file and upload it to S3 for deployment.\nStep 1: Build the JAR File Navigate to the backend directory: cd BE/workshop_BE Clean and build the project: On Windows:\n.\\mvnw.cmd clean package -DskipTests On Linux/Mac:\n./mvnw clean package -DskipTests Verify the JAR file was created: # Windows dir target\\workshop-0.0.1-SNAPSHOT.jar # Linux/Mac ls -lh target/workshop-0.0.1-SNAPSHOT.jar Expected Result: The file workshop-0.0.1-SNAPSHOT.jar should be in the target directory.\nStep 2: Upload JAR to S3 Get the backend bucket name from CloudFormation outputs: aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;BackendBucketName\u0026#39;].OutputValue\u0026#34; \\ --output text Upload the JAR file to S3: # Windows aws s3 cp BE\\workshop_BE\\target\\workshop-0.0.1-SNAPSHOT.jar s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/jars/ --region ap-southeast-1 # Linux/Mac aws s3 cp BE/workshop_BE/target/workshop-0.0.1-SNAPSHOT.jar s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/jars/ --region ap-southeast-1 Expected Result: upload: .../workshop-0.0.1-SNAPSHOT.jar to s3://...\nVerify the upload: aws s3 ls s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/jars/ --region ap-southeast-1 Step 3: Get EC2 Instance ID You\u0026rsquo;ll need an EC2 instance ID to deploy the application. Get it from the Auto Scaling Group:\naws autoscaling describe-auto-scaling-groups \\ --region ap-southeast-1 \\ --query \u0026#34;AutoScalingGroups[?contains(AutoScalingGroupName, \u0026#39;workshop-aws-dev\u0026#39;)].Instances[0].InstanceId\u0026#34; \\ --output text Or list all instances:\naws ec2 describe-instances \\ --region ap-southeast-1 \\ --filters \u0026#34;Name=tag:Name,Values=*workshop-aws-dev*\u0026#34; \\ --query \u0026#34;Reservations[*].Instances[*].[InstanceId,State.Name]\u0026#34; \\ --output table Note: Save the instance ID for the next section.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.4-frontend-deployment/5.4.1-prepare/","title":"Prepare Frontend Environment","tags":[],"description":"","content":"Prerequisites Before building and deploying the frontend, ensure you have:\nNode.js and npm installed (Node.js 18+ recommended) AWS CLI configured with appropriate credentials Frontend bucket name from CloudFormation outputs CloudFront Distribution ID from CloudFormation outputs Get Required Information Get the frontend S3 bucket name: aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;FrontendBucketName\u0026#39;].OutputValue\u0026#34; \\ --output text Get the CloudFront Distribution ID: aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CloudFrontDistributionId\u0026#39;].OutputValue\u0026#34; \\ --output text Get the API Gateway URL: aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;APIGatewayURL\u0026#39;].OutputValue\u0026#34; \\ --output text Verify Frontend Environment Variables Check that the .env file in the FE directory contains the correct API URL:\n# Windows type FE\\.env # Linux/Mac cat FE/.env The file should contain:\nVITE_API_URL=https://98385v3jef.execute-api.ap-southeast-1.amazonaws.com/dev/dna_service VITE_COGNITO_USER_POOL_ID=ap-southeast-1_4osSduRDx VITE_COGNITO_CLIENT_ID=51alb0b6n4h5unrojbshmqv12r VITE_COGNITO_REGION=ap-southeast-1 Note: Update VITE_API_URL with the actual API Gateway URL from step 3 above if different.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/4-eventparticipated/4.6-event6/","title":"Event 6","tags":[],"description":"","content":"Summary Report: \u0026ldquo;Kick-off AWS First Cloud Journey Workforce OJT FALL 2025\u0026rdquo; Event Objectives Welcome and kick-off the AWS First Cloud Journey Workforce OJT FALL 2025 program Introduce the program and future direction in Cloud Computing Share experiences from alumni and industry experts Connect students with AWS community and partner enterprises Inspire and motivate for the learning journey and career development Event Details Date: Saturday, September 6, 2025 Time: 8:30 AM – 12:00 PM Location: 26th Floor, Bitexco Financial Tower, 2 Hải Triều, P. Bến Nghé, Quận 1, TP.HCM Duration: 3.5 hours (including tea break and networking) About AWS First Cloud Journey Workforce Program Launched in 2021, the program has accompanied over 2,000 students across the country.\nMore than 150 students have received intensive training and are currently working at leading technology companies in Vietnam and internationally.\nMain Objectives:\nBuild a high-quality generation of AWS Builders for Vietnam Equip practical skills in Cloud, DevOps, AI/ML, Security, Data \u0026amp; Analytics Connect students with AWS Study Group community of 47,000+ members and AWS partner enterprises The program is not just technology training, but also an important bridge between knowledge – technology – career, helping students confidently integrate into the modern technology world and international integration.\nAgenda 8:30 – 9:00 AM | Welcome \u0026amp; Check-in Networking \u0026amp; commemorative photos Registration and document collection Meeting fellow students 9:00 – 9:15 AM | Opening \u0026amp; Welcome School Representative: Mr. Nguyễn Trần Phước Bảo – Head of Enterprise Relations (QHDN)\nOpening speech for the program Introduction to AWS First Cloud Journey Workforce Orientation and expectations for the course Attended by 2–3 staff members from QHDN Department Keynote \u0026amp; Industry Sharing 9:15 – 9:40 AM | AWS First Cloud Journey \u0026amp; Future Direction (25 minutes) Nguyễn Gia Hưng – Head of Solutions Architect, AWS Vietnam\nIntroduction to AWS First Cloud Journey Vision and future direction of the program Career opportunities in Cloud Computing Career development path with AWS Q\u0026amp;A 9:40 – 10:05 AM | DevOps \u0026amp; Future Career (25 minutes) Đỗ Huy Thắng – DevOps Lead, VNG\nWhat is DevOps and why it matters Career in DevOps field Required skills and how to develop them Real-world experience from VNG Q\u0026amp;A 10:05 – 10:20 AM | Tea Break \u0026amp; Networking (15 minutes) Break time Networking with speakers and participants Commemorative photos Alumni \u0026amp; Career Sharing 10:20 – 10:40 AM | From First Cloud Journey to GenAI Engineer (20 minutes) Danh Hoàng Hiếu Nghị – GenAI Engineer, Renova\nJourney from First Cloud Journey to GenAI Engineer Learning and development experience Opportunities in AI/ML field Advice for new students Q\u0026amp;A 10:40 – 11:00 AM | She in Tech \u0026amp; Journey with First Cloud Journey (20 minutes) Bùi Hồ Linh Nhi – AI Engineer, SoftwareOne\nWomen\u0026rsquo;s journey in technology Experience participating in First Cloud Journey Challenges and opportunities Advice for women wanting to pursue tech career Q\u0026amp;A 11:00 – 11:20 AM | A Day as Cloud Engineer (20 minutes) Phạm Nguyễn Hải Anh – Cloud Engineer, G-Asia Pacific\nA day in the life of a Cloud Engineer Real work and responsibilities Skills and tools used daily Challenges and solutions Q\u0026amp;A 11:20 – 11:40 AM | Journey to First Cloud Journey (20 minutes) Nguyễn Đồng Thanh Hiệp – Principal Cloud Engineer, G-Asia Pacific\nPersonal journey to First Cloud Journey Valuable lessons and experiences Development path from junior to principal engineer Advice for beginners Q\u0026amp;A 11:40 AM – 12:00 PM | Q\u0026amp;A \u0026amp; Wrap-up (20 minutes) Answering questions from speakers \u0026amp; mentors Summary of main event content Information about next steps in the program Commemorative photos 📸 Meet Our Speakers Nguyễn Gia Hưng Head of Solutions Architect, AWS Vietnam\nLeading expert in AWS architecture and solutions Extensive experience in consulting and implementing cloud solutions Leader of AWS First Cloud Journey program in Vietnam Đỗ Huy Thắng DevOps Lead, VNG\nExpert in DevOps practices and automation Experience in building and operating large-scale systems Mentor for many generations of DevOps engineers Danh Hoàng Hiếu Nghị GenAI Engineer, Renova\nAlumni of First Cloud Journey program Expert in Generative AI and Machine Learning Experience in developing AI solutions Bùi Hồ Linh Nhi AI Engineer, SoftwareOne\nAlumni of First Cloud Journey program Expert in AI/ML engineering Inspiration for women in technology Phạm Nguyễn Hải Anh Cloud Engineer, G-Asia Pacific\nExpert in cloud infrastructure and operations Real-world experience in managing and operating cloud systems Mentor for new cloud engineers Nguyễn Đồng Thanh Hiệp Principal Cloud Engineer, G-Asia Pacific\nAlumni of First Cloud Journey program Expert in cloud architecture and best practices Experience from junior to principal level Key Highlights AWS First Cloud Journey Program Program Introduction:\nOn-the-job training (OJT) program on AWS Cloud Computing Has accompanied over 2,000 students since 2021 More than 150 students have received intensive training and work at leading companies Objectives:\nBuild a high-quality generation of AWS Builders Equip practical skills in Cloud, DevOps, AI/ML, Security, Data \u0026amp; Analytics Connect with AWS Study Group community of 47,000+ members Benefits:\nIntensive training with AWS experts Opportunity to practice with real AWS environments Connection with partner enterprises Career development support Career Pathways in Cloud Computing DevOps Career:\nWhat DevOps is and its role in organizations Required skills: CI/CD, Infrastructure as Code, Monitoring Development path: Junior → Mid → Senior → Lead Career opportunities and salary ranges Cloud Engineer Career:\nDaily work of a Cloud Engineer Technical and soft skills Challenges and solutions Growth opportunities AI/ML Engineer Career:\nFrom Cloud Engineer to AI/ML Engineer Required skills for AI/ML Opportunities in Generative AI field Future trends and opportunities Alumni Success Stories From First Cloud Journey to Success:\nJourneys of alumni Valuable lessons and experiences Challenges and how to overcome them Advice for newcomers Diversity in Tech:\nWomen\u0026rsquo;s journey in technology Challenges and opportunities Support and resources available Inspiration and motivation Key Takeaways Understanding AWS First Cloud Journey What the program is: OJT program on AWS Cloud Computing Objectives and benefits: Build practical skills and connect with community Learning path: Stages and milestones in the program Career opportunities: Connection with partner enterprises Career Pathways DevOps: Understanding DevOps career and required skills Cloud Engineer: Real work and responsibilities AI/ML Engineer: Development path in AI/ML field Career progression: From junior to senior and principal level Real-world Insights Alumni experiences: Journeys and lessons from alumni Day-to-day work: Real work of different roles Challenges: Challenges and solutions Best practices: Advice and tips from experts Networking and Community AWS Study Group: Community of 47,000+ members Mentorship: Opportunity to be mentored by experts Peer learning: Learning from fellow students Industry connections: Connection with enterprises Applying to Work Set clear goals: Identify the career path you want to pursue Build skills: Focus on skills needed for your chosen career path Practice regularly: Utilize AWS environment to practice Connect and network: Join community and connect with mentors Learn from alumni: Apply lessons learned from successful alumni Develop soft skills: Not just technical skills but also communication, teamwork Event Experience Attending the Kick-off AWS First Cloud Journey Workforce OJT FALL 2025 event was an inspiring and motivating experience. The event not only provided information about the program but also inspired the learning journey and career development in Cloud Computing.\nOpening and Welcome Opening speech by Mr. Nguyễn Trần Phước Bảo created a formal and professional atmosphere. I understood clearly about the program\u0026rsquo;s objectives and expectations. Introduction to First Cloud Journey helped me visualize the upcoming journey. Keynote from AWS Session by Nguyễn Gia Hưng provided an overall vision of AWS First Cloud Journey. Understanding future direction and career opportunities in Cloud Computing. Career development path gave me a clear roadmap to follow. Industry Insights DevOps session by Đỗ Huy Thắng helped me understand DevOps career. Learning about required skills and how to develop in this field. Real-world examples from VNG helped me visualize actual work. Alumni Success Stories Journey of Danh Hoàng Hiếu Nghị from First Cloud Journey to GenAI Engineer was very inspiring. I learned about persistence and continuous learning. Session by Bùi Hồ Linh Nhi about She in Tech was very empowering. Understanding challenges and opportunities for women in tech. Day-to-day Work Insights Session by Phạm Nguyễn Hải Anh about a day as Cloud Engineer was very practical. I understood the actual work and responsibilities of a Cloud Engineer. Journey of Nguyễn Đồng Thanh Hiệp from junior to principal was very motivating. Learning about career progression and growth mindset. Networking and Connections Networking sessions allowed me to connect with speakers and participants. Sharing experiences and learnings with fellow students. Q\u0026amp;A sessions provided opportunities to ask specific questions. Meeting alumni and receiving advice about career development. Lessons Learned First Cloud Journey is a great opportunity: The program provides a solid foundation for cloud career. Diverse career paths: There are many development paths in cloud computing. Continuous learning is key: Need to keep learning to develop. Networking matters: Connecting with community and mentors is very important. Diversity and inclusion: Tech industry is becoming more inclusive. Set clear goals: Need clear goals and plan to achieve them. Some event photos Overall, this Kick-off event was an excellent start to the AWS First Cloud Journey. The combination of program information, career insights, and alumni success stories gave me motivation and clear direction. Particularly, networking with speakers, alumni, and participants provided valuable connections and support for my learning journey and career development in Cloud Computing.\nClosing Remarks Today\u0026rsquo;s Kick-off event is the starting point for the AWS Builders journey – where students not only access the most advanced cloud computing technology but also get inspired, connect with experts, and expand career opportunities.\nOnce again, congratulations to all students who have officially become part of AWS First Cloud Journey Workforce OJT FALL 2025. Let\u0026rsquo;s start this new journey together – a journey of learning, building, and developing, to take cloud computing technology in Vietnam far! 🚀\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/1-worklog/1.12-week12/","title":"Worklog Week 12","tags":[],"description":"","content":"Week 12 Objectives: Present the team\u0026rsquo;s project to mentors and supervisors.\nCollect feedback for improvement on architecture, implementation, and presentation quality.\nIdentify areas that need adjustment before the final submission.\nDocument all comments and action items for the next sprint.\nTasks to be carried out this week: Day Tasks Start Date Completion Date Resources Used 2 - Prepare final slide deck - Review speaking roles for each member 24/11/2025 24/11/2025 Slide deck 3 - Conduct internal rehearsal session - Adjust timing and transitions between presenters 25/11/2025 25/11/2025 Internal notes 4 - Official project presentation in front of mentors - Present system architecture, CI/CD pipeline, cost estimation, and demo 26/11/2025 26/11/2025 Presentation materials 5 - Receive mentor feedback on technical design, security considerations, and deployment approach - Record all comments for follow-up 27/11/2025 27/11/2025 Mentor feedback 6 - Analyze received feedback - Identify improvements required for architecture, diagrams, and slide clarity 28/11/2025 28/11/2025 Consolidated feedback 7 - Update documents and adjust the slide deck according to mentor comments - Prepare follow-up action plan for next week 29–30/11/2025 30/11/2025 Proposal, Slide deck Week 12 Achievements: Successfully presented the project to mentors and received detailed feedback on architecture, implementation, and presentation quality.\nDocumented all comments related to:\nSystem architecture improvements\nClarification of data flow and networking layers\nCI/CD pipeline explanation\nCost optimization suggestions\nSecurity considerations\nImproved the slide deck and project documentation based on mentor recommendations.\nIdentified key action items to refine before the final submission.\nCompleted rehearsal and delivery of the mid-stage presentation as planned.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/2-proposal/","title":"Proposal","tags":[],"description":"","content":"In this section, you need to summarize the contents of the workshop that you plan to conduct.\nBlood Donation Support System AWS Solution for Blood Donation Support Software 1. Executive Summary Blood Donation Support System (BDSS) is a web platform that supports the management and connection of blood donors with medical facilities. The project was developed by a group of students in Ho Chi Minh City to optimize the blood donation process, reduce the burden of searching for donors and improve the efficiency of medical communication.\nThe system is built on AWS Cloud architecture, using Amazon EC2, Amazon RDS, API Gateway, Cognito and CI/CD Pipeline (GitLab + CodePipeline) for automatic deployment. BDSS supports four user groups (Guest, Member, Staff, Admin), providing features for searching, registering for blood donation, managing blood banks, tracking blood donation processes and visual reporting.\n2. Problem Statement What’s the Problem? Healthcare facilities currently manage blood donation processes manually or through disparate tools. Finding donors who match blood type or region is difficult, especially in emergency situations. In addition, data storage systems are not synchronized, making it difficult to analyze, report and optimize blood donation campaigns.\nThe Solution Developed a comprehensive blood donation support platform on AWS Cloud, with functions for blood donation management, searching for donors and blood needers by blood type or geographic location, integrating user authentication via Amazon Cognito, and data governance on Amazon RDS. The frontend was deployed via Route 53 + CloudFront, the backend via API Gateway – EC2, a MySQL database on Amazon RDS, and an automated CI/CD pipeline using GitLab – CodePipeline.\nBenefits and Return on Investment Reduce the time it takes to find a matching donor by 60–70%. Increase the accuracy of blood type and location information. Optimize operating costs with a flexible, pay-as-you-go cloud architecture. Improve response to blood emergencies\n3. Solution Architecture The platform employs a comprehensive AWS cloud architecture to support blood donation management, connecting donors with medical facilities efficiently. The system integrates multiple AWS services to provide a scalable, secure, and cost-effective solution. The architecture is detailed below:\nThe system is divided into 4 main layers:\nEdge Networking Layer: Route 53 manages domain and DNS routing. CloudFront increases page loading speed and delivers static content. AWS WAF protects against web attacks (SQL injection, DDoS).\nApplication \u0026amp; Data Layer: Amazon EC2: Deploys backend API and handles main business. Amazon RDS (MySQL): Stores blood donor data, blood types, donation history. API Gateway: Communicates between frontend and backend. Elastic Load Balancer (ELB): Distributes load to EC2 instances. NAT Gateway \u0026amp; Internet Gateway: Supports secure Internet connection.\nCI/CD \u0026amp; DevOps Layer: GitLab: Source code management. AWS CodePipeline, CodeBuild: Deploy and update automatically.\nMonitoring \u0026amp; Security Layer: Amazon Cognito: Authentication and authorization (Guest, Member, Staff, Admin). CloudWatch, CloudTrail, IAM, Secrets Manager: Monitoring, security, system alerts. SNS: Send notifications when there is an event (blood emergency, suitable donor).\n4. Technical Implementation Implementation Phases\nAnalysis \u0026amp; Design (January) Gather requirements, define use cases, design ERD and AWS architecture. Infrastructure \u0026amp; Pipeline Setup (February) Configure Route 53, CloudFront, EC2, RDS and CI/CD on AWS. Development \u0026amp; Testing (March-April) Build main modules: blood donation registration, search, blood bank management. Integrate Cognito and SNS alert system. Deployment \u0026amp; Operation (May) Deploy the official product and monitor with CloudWatch. Key Technical Requirements: Frontend: React/Next.js or Angular (deploy via S3/CloudFront). Backend: Spring Boot on EC2, communicate via REST API Gateway. Database: Amazon RDS MySQL, optimize queries and periodic backups. CI/CD: GitLab → CodeBuild → CodePipeline → EC2. Auth: Cognito (4 roles: Guest, Member, Staff, Admin). Alert \u0026amp; Logs: SNS + CloudWatch + CloudTrail.\n5. Timeline \u0026amp; Milestones Timeline Phase Key Results Month 1 Requirements analysis \u0026amp; design AWS architecture + use case diagram Month 2 Infrastructure \u0026amp; pipeline setup EC2, RDS, API Gateway operational Month 3–4 Development \u0026amp; testing Key modules finalized Month 5 Live deployment System stable, with Dashboard reporting 6. Budget Estimation Services Estimated Cost/Month (USD) Notes EC2 (t3.nano) 3.50 Backend REST API Amazon RDS (MySQL) 2.80 20 GB storage API Gateway 0.50 5,000 requests CloudFront + S3 0.80 Website + CDN Route 53 0.50 Domain \u0026amp; DNS Cognito 0.10 \u0026lt;100 users CloudWatch + Logs 0.30 Monitoring \u0026amp; Alerting CI/CD (CodePipeline, CodeBuild) 0.40 Automated Deployment Total 8.9 USD/month ~106.8 USD/year Total costs may vary based on AWS Free Tier or spot instance usage.\n7. Risk Assessment Risk Impact Probability Mitigation Internet Outage Medium Medium Redundancy on EC2 Backup DDoS Attack High Low AWS WAF + CloudFront User Data Corruption High Low RDS Backup + IAM Restricted Access Cost Overrun Medium Low AWS Budget Alert CI/CD Deployment Disruption Low Medium Pipeline Testing Before Merging 8. Expected Outcomes Technology: Cloud-native system, automatic CI/CD, multi-user support and high security. Application: Helps medical facilities manage blood donations effectively, minimizing manual processes. Expansion: Can be replicated for many other hospitals, integrating AI to analyze blood group needs or predict upcoming blood donations.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.4-frontend-deployment/5.4.2-build/","title":"Build Frontend Application","tags":[],"description":"","content":"Build React Application In this section, you will build the React frontend application using Vite.\nNavigate to the frontend directory: cd FE Install dependencies (if not already installed): npm install Verify the .env file exists and contains the correct API URL: # Windows type .env # Linux/Mac cat .env Build the application for production: npm run build Expected Result: The dist directory should be created with production-ready files:\nindex.html assets/ folder with JavaScript and CSS files Verify the build output: # Windows dir dist # Linux/Mac ls -lh dist/ Note: The build process will use the VITE_API_URL from the .env file and embed it into the JavaScript bundle.\nBuild Troubleshooting If you encounter build errors:\nError: VITE_API_URL is not defined: Ensure the .env file exists and contains VITE_API_URL Error: Module not found: Run npm install to install dependencies Error: Port already in use: Stop any running development server "},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.3-backend-deployment/5.3.2-deploy-ec2/","title":"Deploy to EC2","tags":[],"description":"","content":"Connect to EC2 with Session Manager For this workshop, you will use AWS Session Manager to access EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances through an interactive browser-based shell without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance from your Auto Scaling Group (use the instance ID you saved from the previous section). Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nDeploy Backend Application Switch to the ec2-user: sudo su - ec2-user Navigate to the application directory: cd /opt/workshop Download the JAR file from S3: aws s3 cp s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/jars/workshop-0.0.1-SNAPSHOT.jar . --region ap-southeast-1 Stop any existing application (if running): pkill -f workshop-0.0.1-SNAPSHOT.jar || true Get the RDS endpoint from CloudFormation: RDS_ENDPOINT=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;RDSEndpoint\u0026#39;].OutputValue\u0026#34; \\ --output text) echo \u0026#34;RDS Endpoint: $RDS_ENDPOINT\u0026#34; Create the application.properties file: cat \u0026gt; /opt/workshop/application.properties \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; spring.application.name=workshop-aws # AWS RDS Database Configuration spring.datasource.url=jdbc:mysql://${RDS_ENDPOINT}:3306/workshop_aws?useSSL=true\u0026amp;requireSSL=false\u0026amp;allowPublicKeyRetrieval=true\u0026amp;serverTimezone=Asia/Ho_Chi_Minh spring.datasource.username=admin spring.datasource.password=phatsieuqua123 spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver # Connection Pool spring.datasource.hikari.maximum-pool-size=10 spring.datasource.hikari.minimum-idle=5 spring.datasource.hikari.connection-timeout=20000 # JPA Configuration spring.jpa.hibernate.ddl-auto=update spring.jpa.show-sql=false # Server Configuration server.port=8080 server.servlet.context-path=/dna_service # JWT Configuration jwt.signerKey=2VJ50pdhYm96e4VECp/vsZGVmkSl9xp1rSYAZKsZL7n9Ti1pZYle3k9mheQEKt6+ # CORS Configuration cors.allowed.origins=https://d3gmmg22uirq0t.cloudfront.net,https://98385v3jef.execute-api.ap-southeast-1.amazonaws.com # Logging logging.level.root=INFO logging.level.aws_project.workshop=DEBUG logging.file.name=/opt/workshop/application.log # Actuator Configuration management.endpoints.web.exposure.include=health,info,metrics management.endpoint.health.show-details=when-authorized EOF Note: Replace ${RDS_ENDPOINT} with the actual RDS endpoint value.\nStart the application: nohup java -jar workshop-0.0.1-SNAPSHOT.jar \\ --spring.config.location=file:/opt/workshop/application.properties \\ \u0026gt;\u0026gt; /opt/workshop/application.log 2\u0026gt;\u0026amp;1 \u0026amp; Wait a few seconds and check if the application is running: sleep 10 ps aux | grep java tail -20 /opt/workshop/application.log Verify Backend Deployment Test the health endpoint: curl http://localhost:8080/dna_service/actuator/health Expected Result: {\u0026quot;status\u0026quot;:\u0026quot;UP\u0026quot;}\nTest through the API Gateway (get the URL from CloudFormation outputs): API_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;APIGatewayURL\u0026#39;].OutputValue\u0026#34; \\ --output text) curl ${API_URL}/dna_service/actuator/health Section Summary Congratulations! You have successfully deployed the Spring Boot backend application to EC2. The application is now running in a private subnet, accessible through the Application Load Balancer and API Gateway. The VPC endpoints allow the EC2 instance to access S3 (for downloading the JAR) and Systems Manager (for Session Manager) without traversing the public internet.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:*\u0026#34;, \u0026#34;iam:*\u0026#34;, \u0026#34;s3:*\u0026#34;, \u0026#34;rds:*\u0026#34;, \u0026#34;apigateway:*\u0026#34;, \u0026#34;cloudfront:*\u0026#34;, \u0026#34;ssm:*\u0026#34;, \u0026#34;logs:*\u0026#34;, \u0026#34;autoscaling:*\u0026#34;, \u0026#34;elasticloadbalancing:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this workshop, we will use Singapore region (ap-southeast-1).\nTo deploy the infrastructure, use the following command:\naws cloudformation create-stack \\ --stack-name workshop-aws-dev \\ --template-body file://aws/infrastructure.yaml \\ --parameters file://aws/parameters.json \\ --capabilities CAPABILITY_NAMED_IAM \\ --region ap-southeast-1 Or update an existing stack:\naws cloudformation update-stack \\ --stack-name workshop-aws-dev \\ --template-body file://aws/infrastructure.yaml \\ --parameters file://aws/parameters.json \\ --capabilities CAPABILITY_NAMED_IAM \\ --region ap-southeast-1 Prerequisites Before deploying, ensure you have:\nAWS CLI installed and configured with appropriate credentials IAM Permissions as specified in the IAM permissions section above Parameters File: aws/parameters.json configured with your values EC2 Key Pair: Create a key pair in AWS Console (used for EC2 access, though we\u0026rsquo;ll use SSM Session Manager) CloudFormation Stack Deployment The CloudFormation deployment requires about 20-25 minutes to complete. The stack will create:\n1 VPC with public and private subnets across 2 Availability Zones 1 Auto Scaling Group with EC2 instances (t3.micro) 1 RDS MySQL database instance (db.t3.micro) 2 S3 Buckets (frontend and backend) 1 CloudFront Distribution for frontend 1 Application Load Balancer for backend 1 API Gateway REST API 5 VPC Endpoints (S3 Gateway, SSM, SSM Messages, EC2 Messages, CloudWatch Logs) IAM Roles and Policies for EC2, Lambda, and other services Security Groups for network access control Route Tables and Internet Gateway for networking Verify Deployment After the stack creation completes, verify the following resources:\nVPC: Check VPC console for workshop-aws-dev-vpc EC2 Instances: Check Auto Scaling Group for running instances RDS: Verify database endpoint in RDS console S3 Buckets: Confirm frontend and backend buckets exist CloudFront: Check distribution status API Gateway: Verify REST API is deployed "},{"uri":"https://Fatnotfatt.github.io/learning-aws/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Accelerating Enterprise ML Experimentation with Amazon SageMaker AI and Comet This blog introduces how to integrate Amazon SageMaker AI with Comet to accelerate enterprise machine learning experimentation. You will learn how to manage ML experiments, track model lineage, and ensure reproducibility in production environments. The article demonstrates a complete fraud detection workflow using SageMaker AI + Comet, showcasing experiment tracking, model comparison, and audit-ready logging that modern enterprises require. It covers both administrator and user journeys, from setting up the Comet Partner AI App to running experiments and comparing results in the Comet UI.\nBlog 2 - Using Apache Airflow workflows to orchestrate data processing on Amazon SageMaker Unified Studio This blog demonstrates how to use Apache Airflow workflows to orchestrate data processing pipelines on Amazon SageMaker Unified Studio. You will learn how to build, test, and run end-to-end ML pipelines using SageMaker workflows through the Unified Studio interface. The article walks through a practical example that includes ingesting weather and taxi data, transforming and merging datasets, then using ML to predict taxi fares - all orchestrated through SageMaker Unified Studio workflows powered by Amazon Managed Workflows for Apache Airflow (Amazon MWAA). It focuses on the code-based approach using Python DAGs for workflow management.\nBlog 3 - Migrating full-text search from SQL Server to Amazon Aurora PostgreSQL-Compatible Edition or Amazon RDS for PostgreSQL This blog guides you on how to migrate full-text search from SQL Server to Amazon Aurora PostgreSQL or Amazon RDS for PostgreSQL. You will learn how to migrate FTS queries and schema structures, as the implementation differs between the two systems. The article demonstrates how to use PostgreSQL\u0026rsquo;s tsvector and tsquery data types to achieve similar FTS functionality, and also shows how to implement FTS using the pg_trgm and pg_bigm extensions. It covers various use cases including CONTAINS predicates, FREETEXT queries, ranking with RANK, and performance optimization techniques using GIN indexes and stored generated columns.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.3-backend-deployment/","title":"Backend Deployment","tags":[],"description":"","content":"Deploy Backend Application In this section, you will build and deploy the Spring Boot backend application to EC2 instances. The backend will run in private subnets and be accessible through the Application Load Balancer and API Gateway.\nThe deployment process includes:\nBuilding the Spring Boot JAR file Uploading the JAR to S3 Deploying to EC2 instances via Session Manager Configuring application properties Starting the application service Content Build and Upload Backend Deploy to EC2 "},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.4-frontend-deployment/5.4.3-deploy/","title":"Deploy to S3 and CloudFront","tags":[],"description":"","content":"Upload Frontend to S3 Get the frontend bucket name (if you don\u0026rsquo;t have it): BUCKET_NAME=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;FrontendBucketName\u0026#39;].OutputValue\u0026#34; \\ --output text) echo \u0026#34;Bucket: $BUCKET_NAME\u0026#34; Upload the built files to S3: # Windows aws s3 sync dist\\ s3://$BUCKET_NAME/ --delete --region ap-southeast-1 # Linux/Mac aws s3 sync dist/ s3://$BUCKET_NAME/ --delete --region ap-southeast-1 Expected Result: Files are uploaded to S3 with output like:\nupload: dist/index.html to s3://... upload: dist/assets/... Verify the upload: aws s3 ls s3://$BUCKET_NAME/ --recursive --region ap-southeast-1 Invalidate CloudFront Cache Get the CloudFront Distribution ID: DIST_ID=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CloudFrontDistributionId\u0026#39;].OutputValue\u0026#34; \\ --output text) echo \u0026#34;Distribution ID: $DIST_ID\u0026#34; Create a CloudFront invalidation: aws cloudfront create-invalidation \\ --distribution-id $DIST_ID \\ --paths \u0026#34;/*\u0026#34; \\ --region ap-southeast-1 Expected Result: An invalidation ID is returned. The invalidation typically takes 1-2 minutes to complete.\nCheck invalidation status (optional): INVALIDATION_ID=$(aws cloudfront list-invalidations \\ --distribution-id $DIST_ID \\ --region ap-southeast-1 \\ --query \u0026#34;InvalidationList.Items[0].Id\u0026#34; \\ --output text) aws cloudfront get-invalidation \\ --distribution-id $DIST_ID \\ --id $INVALIDATION_ID \\ --region ap-southeast-1 Verify Frontend Deployment Get the CloudFront domain name: CLOUDFRONT_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CloudFrontDomainName\u0026#39;].OutputValue\u0026#34; \\ --output text) echo \u0026#34;Frontend URL: https://$CLOUDFRONT_URL\u0026#34; Open the frontend URL in your browser: https://d3gmmg22uirq0t.cloudfront.net Test the application: The frontend should load Try logging in or registering a new user Verify API calls are working (check browser console for errors) Section Summary Congratulations! You have successfully deployed the React frontend application to S3 and CloudFront. The frontend is now accessible globally via CloudFront CDN, providing low latency and high availability. The application communicates with the backend through API Gateway.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00, October 3, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I gained a deep understanding of how generative AI is changing software development. The event showed me how AI can help throughout the entire development lifecycle, from planning to maintenance. I learned that while AI tools are powerful, human judgment is still crucial. I also understood that we should adopt AI tools gradually and ensure the whole team is aligned. New Skills: I learned how to use Amazon Q Developer for code generation, debugging, and documentation. I also explored Kiro and discovered how it can enhance my productivity. I developed skills in identifying which tasks can benefit from AI automation and learned how to balance AI assistance with human review to maintain code quality. Contribution to Team/Project: I documented the key takeaways from the event and shared them with my team. I identified specific projects where we could pilot AI tools, which could improve our development speed by 20-30% for repetitive tasks. I created guidelines for using AI tools in our workflow and planned to organize internal training sessions. The knowledge I gained helps our team stay competitive by using cutting-edge AI development tools. Event 2 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about the complete AWS AI/ML ecosystem, from Amazon SageMaker for traditional ML to Amazon Bedrock for Generative AI. I gained deep understanding of prompt engineering techniques like Chain-of-Thought reasoning and Few-shot learning. I also learned about RAG (Retrieval-Augmented Generation) architecture and how it\u0026rsquo;s crucial for building accurate GenAI applications. The importance of guardrails for AI safety and content filtering in production applications was also emphasized. New Skills: I developed skills in using Amazon SageMaker Studio for ML model development and deployment. I learned how to implement RAG architecture for knowledge base integration. I gained practical knowledge of prompt engineering and how to build Bedrock Agents for multi-step workflows. I also learned about different foundation models (Claude, Llama, Titan) and when to use each one. Contribution to Team/Project: I shared comprehensive notes about SageMaker capabilities and Bedrock features with my team. I identified opportunities to implement RAG solutions for our domain-specific applications. I proposed pilot projects using Bedrock Agents for customer service automation. I also created guidelines for prompt engineering best practices and guardrail implementation for our GenAI projects. Event 3 Event Name: DevOps on AWS\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about DevOps culture and principles, including DORA metrics (Deployment Frequency, Lead Time, MTTR, Change Failure Rate) for measuring DevOps maturity. I gained deep understanding of AWS CI/CD services (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) and different deployment strategies (Blue/Green, Canary, Rolling). I also learned about Infrastructure as Code with CloudFormation and CDK, and when to use each approach. New Skills: I developed skills in building complete CI/CD pipelines using AWS DevOps services. I learned how to implement Infrastructure as Code with both CloudFormation and CDK. I gained practical knowledge of container services (ECR, ECS, EKS, App Runner) and when to use each one. I also learned how to set up monitoring and observability using CloudWatch and X-Ray. Contribution to Team/Project: I shared comprehensive notes about AWS DevOps services and best practices with my team. I proposed implementing CI/CD pipelines using CodePipeline for automated deployments. I suggested adopting Infrastructure as Code for all our infrastructure using CloudFormation or CDK. I also created guidelines for containerization strategies and monitoring best practices. The knowledge gained helps our team implement modern DevOps practices and improve deployment frequency and reliability. Event 4 Event Name: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about the AWS Well-Architected Framework Security Pillar and its five core pillars: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response. I gained deep understanding of core security principles including Least Privilege, Zero Trust, and Defense in Depth. I also learned about the Shared Responsibility Model and top security threats in cloud environments in Vietnam. The importance of building security into architecture from the start, not as an afterthought, was emphasized. New Skills: I developed skills in modern IAM architecture using IAM Identity Center, Service Control Policies, and permission boundaries. I learned how to implement comprehensive detection and monitoring using CloudTrail, GuardDuty, and Security Hub. I gained practical knowledge of network security with VPC segmentation, Security Groups, NACLs, WAF, and Shield. I also learned about encryption at rest and in transit, KMS key management, and secrets management with Secrets Manager and Parameter Store. Contribution to Team/Project: I shared comprehensive security best practices and the five pillars framework with my team. I proposed implementing modern IAM patterns with IAM Identity Center for SSO. I suggested setting up comprehensive monitoring using CloudTrail, GuardDuty, and Security Hub. I created incident response playbooks for common scenarios like compromised IAM keys, S3 public exposure, and EC2 malware detection. I also developed guidelines for encryption strategies and secrets management. The knowledge gained helps our team build secure cloud architectures following AWS Well-Architected best practices. Event 5 Event Name: Building Agentic AI: Context Optimization with Amazon Bedrock\nDate \u0026amp; Time: 09:00, December 5, 2025\nLocation: 26th Floor, Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about Building Agentic AI and Context Optimization with Amazon Bedrock. I gained deep understanding of how to build autonomous AI agents with Amazon Bedrock through hands-on techniques. I learned about agentic orchestration patterns and advanced context optimization techniques. I also understood the importance of context optimization in reducing costs and improving performance. The workshop emphasized that agentic AI is the future of AI applications and context optimization is key to scaling effectively. New Skills: I developed skills in building Bedrock Agents from scratch with guidance from experts. I learned about context optimization techniques such as compression, summarization, and relevant information extraction. I gained practical knowledge of agentic orchestration patterns and how to coordinate multiple agents. I also learned about the CloudThinker platform and how it simplifies building agentic systems. The hands-on workshop gave me opportunities to practice with real AWS environments. Contribution to Team/Project: I shared comprehensive notes about Building Agentic AI and Context Optimization with my team. I proposed pilot projects using Bedrock Agents for automation tasks. I created guidelines for context optimization best practices to reduce costs and improve performance. I also documented CloudThinker platform capabilities and integration patterns. The knowledge gained helps our team explore agentic AI solutions and optimize costs in AI/ML projects. Event 6 Event Name: Kick-off AWS First Cloud Journey Workforce OJT FALL 2025\nDate \u0026amp; Time: 08:30, September 6, 2025\nLocation: 26th Floor, Bitexco Financial Tower, 2 Hải Triều, P. Bến Nghé, Quận 1, TP.HCM\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about the AWS First Cloud Journey Workforce program, which has trained over 2,000 students since 2021, with more than 150 graduates working at leading technology companies. I gained insights into various career pathways in cloud computing including DevOps, Cloud Engineer, and AI/ML Engineer roles. I learned about the importance of continuous learning, networking, and setting clear career goals. The event emphasized that cloud computing offers diverse career paths and that security, DevOps, and AI/ML are all viable and rewarding directions. New Skills: I developed a better understanding of career development in cloud computing and learned about the skills required for different roles. I gained insights into how to build a career path from junior to principal level. I also learned about the importance of soft skills alongside technical skills, and how to leverage community and mentorship for career growth. Contribution to Team/Project: I shared the program information and career insights with my team. I documented the different career pathways and skill requirements for various cloud roles. I created a personal development plan based on the roadmap shared by speakers. I also identified networking opportunities and connections that could benefit our team. The knowledge gained helps me understand the broader cloud computing ecosystem and plan my career development accordingly. "},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.4-frontend-deployment/","title":"Frontend Deployment","tags":[],"description":"","content":"Overview In this section, you will build and deploy the React frontend application to S3 and CloudFront. The frontend will be served via CloudFront CDN for global content delivery and low latency.\nThe deployment process includes:\nBuilding the React application with Vite Uploading static files to S3 Invalidating CloudFront cache Verifying the frontend is accessible Content Prepare Frontend Environment Build Frontend Application Deploy to S3 and CloudFront Verify Full Stack Deployment "},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.4-frontend-deployment/5.4.4-verify/","title":"Verify Full Stack Deployment","tags":[],"description":"","content":"Test Complete Application Flow In this section, you will verify that the entire application stack is working correctly, from frontend to backend to database.\nStep 1: Verify Backend Health Test the backend health endpoint through API Gateway:\nAPI_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;APIGatewayURL\u0026#39;].OutputValue\u0026#34; \\ --output text) curl ${API_URL}/dna_service/actuator/health Expected Result: {\u0026quot;status\u0026quot;:\u0026quot;UP\u0026quot;}\nStep 2: Test Frontend Access Open the CloudFront URL in your browser: https://d3gmmg22uirq0t.cloudfront.net Open browser Developer Tools (F12) and check the Console tab for any errors.\nVerify the API URL is correctly configured by checking network requests.\nStep 3: Test User Registration/Login Try to register a new user or log in with existing credentials.\nVerify that:\nAPI calls are successful (check Network tab in DevTools) JWT token is stored in localStorage User is redirected to the appropriate page after login Step 4: Verify Database Connection Connect to EC2 via Session Manager and check application logs:\n# Connect to EC2 aws ssm start-session --target \u0026lt;INSTANCE_ID\u0026gt; --region ap-southeast-1 # On EC2, check logs tail -50 /opt/workshop/application.log | grep -i \u0026#34;database\\|connection\\|error\u0026#34; Expected Result: No database connection errors in logs.\nStep 5: Monitor Application Check CloudWatch Logs: aws logs tail /aws/workshop-aws/dev/application --follow --region ap-southeast-1 Check EC2 metrics in CloudWatch console: CPU utilization Network in/out Application health Troubleshooting Common Issues Frontend can\u0026rsquo;t connect to API:\nVerify VITE_API_URL in .env matches API Gateway URL Check CORS configuration in backend Verify API Gateway integration with ALB Backend not responding:\nCheck EC2 instance is running Verify application is running: ps aux | grep java Check application logs: tail -100 /opt/workshop/application.log Database connection errors:\nVerify RDS security group allows traffic from EC2 security group Check RDS endpoint is correct in application.properties Verify database credentials Section Summary You have successfully deployed and verified the complete full-stack application. The architecture includes:\nFrontend served via CloudFront from S3 Backend running on EC2 in private subnets API Gateway routing requests to ALB RDS MySQL database for data persistence VPC endpoints for secure AWS service access Section Summary You have successfully deployed and verified the complete full-stack application. The architecture includes:\nFrontend served via CloudFront from S3 Backend running on EC2 in private subnets API Gateway routing requests to ALB RDS MySQL database for data persistence VPC endpoints for secure AWS service access "},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Full-Stack Web Application Deployment on AWS Overview This workshop demonstrates how to deploy a complete full-stack web application on AWS using Infrastructure as Code (CloudFormation). You will learn to build a production-ready architecture with:\nBackend: Spring Boot REST API on EC2 with Auto Scaling Frontend: React application served via CloudFront from S3 Database: MySQL RDS for data persistence API Gateway: RESTful API with CORS support Load Balancer: Application Load Balancer for high availability VPC Endpoints: Private connectivity to AWS services (S3, SSM, CloudWatch) Architecture Highlights Infrastructure as Code: Entire infrastructure defined in CloudFormation templates High Availability: Multi-AZ deployment with Auto Scaling Groups Security: Private subnets, security groups, IAM roles, VPC endpoints Monitoring: CloudWatch logs, alarms, and metrics Cost Optimization: VPC endpoints to reduce NAT Gateway data transfer costs Scalability: Auto Scaling based on CPU metrics Workshop Content Workshop Overview - Introduction and architecture overview Prerequisites - IAM permissions and CloudFormation deployment Backend Deployment - Build and deploy Spring Boot application Frontend Deployment - Build and deploy React application Testing and Monitoring - Application testing and CloudWatch monitoring Clean up - Resource cleanup instructions Technologies Used AWS Services: VPC, EC2, RDS, S3, CloudFront, API Gateway, ALB, Auto Scaling, CloudWatch, Systems Manager Backend: Spring Boot, Java 17, MySQL Frontend: React, Vite, TypeScript Infrastructure: CloudFormation, IAM, Security Groups "},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.5-testing-monitoring/","title":"Testing and Monitoring","tags":[],"description":"","content":"Application Testing and Monitoring In this section, you will learn how to test the application and monitor its performance using AWS CloudWatch and other monitoring tools.\nTesting the Application Health Check Endpoints:\nBackend: https://98385v3jef.execute-api.ap-southeast-1.amazonaws.com/dev/dna_service/actuator/health Frontend: https://d3gmmg22uirq0t.cloudfront.net API Testing:\nUse Swagger UI: https://98385v3jef.execute-api.ap-southeast-1.amazonaws.com/dev/dna_service/swagger-ui.html Test endpoints using curl or Postman End-to-End Testing:\nRegister a new user Login and verify JWT token Create and manage resources Verify data persistence in database CloudWatch Monitoring View Application Logs: aws logs tail /aws/workshop-aws/dev/application --follow --region ap-southeast-1 Check EC2 Metrics:\nCPU Utilization Network In/Out Status Check Failed Monitor RDS:\nDatabase connections CPU utilization Storage space API Gateway Metrics:\nRequest count Latency Error rates Auto Scaling The Auto Scaling Group will automatically:\nScale up when CPU \u0026gt; 70% for 5 minutes Scale down when CPU \u0026lt; 30% for 5 minutes Maintain between 1-2 instances (configurable) Monitor scaling activities:\naws autoscaling describe-scaling-activities \\ --auto-scaling-group-name \u0026lt;ASG_NAME\u0026gt; \\ --region ap-southeast-1 Performance Optimization CloudFront Cache:\nStatic assets are cached at edge locations Invalidate cache when deploying updates Database Optimization:\nMonitor slow queries Optimize indexes Consider read replicas for high traffic Application Optimization:\nMonitor JVM heap usage Optimize database queries Use connection pooling effectively "},{"uri":"https://Fatnotfatt.github.io/learning-aws/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at First Cloud AI Journey Office (FCJ) from September 2025 to DECEMBER 2025, I had the opportunity to learn, practice, and apply cloud computing knowledge to a real-world AWS environment.\nI participated in the AWS Cloud Journey internship program, where I completed a comprehensive 12-week learning journey and deployed a production-ready web application architecture on AWS. Through this project, I improved my skills in cloud architecture design, AWS services, Infrastructure as Code (CloudFormation), CI/CD pipelines, monitoring and observability, security best practices, and problem-solving in cloud environments.\nThe main project involved designing and implementing a complete AWS web application architecture including:\nEdge Layer: Route 53, CloudFront CDN, AWS WAF, ACM Certificate, S3 static hosting Networking: VPC, subnets, Internet Gateway, NAT Gateway, Security Groups, VPC Flow Logs Compute \u0026amp; Database: EC2 with Auto Scaling, RDS, API Gateway, Amazon Cognito CI/CD: GitLab, CodePipeline, CodeBuild with automated deployments Monitoring \u0026amp; Security: CloudWatch, CloudTrail, SNS alerts, IAM, Secrets Manager In terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with mentors and colleagues to improve work efficiency. I maintained detailed worklogs documenting my progress, challenges encountered, and solutions implemented throughout the 12-week period.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ☐ ✅ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ☐ ✅ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ☐ ✅ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Strengths Strong Learning Ability: I demonstrated quick adaptation to new AWS services and concepts. When encountering unfamiliar services like API Gateway VPC Link or CloudFront OAC, I proactively researched documentation and implemented solutions effectively.\nProactive Approach: I took initiative in exploring advanced features beyond basic requirements, such as implementing SSH-less deployment using AWS Systems Manager and automating CloudFront cache invalidation in CI/CD pipelines.\nTechnical Documentation: I maintained comprehensive worklogs with detailed documentation of architecture decisions, challenges faced, and solutions implemented, which will be valuable for future reference and knowledge sharing.\nResponsibility and Quality: I consistently completed weekly tasks on schedule and ensured quality by thorough testing at each stage of the project, from edge layer setup to final end-to-end testing.\nNeeds Improvement Time Management and Discipline: While I completed all tasks, I sometimes struggled with time management when dealing with complex issues like API Gateway VPC Link configuration or CloudWatch alarm troubleshooting. I need to improve my ability to estimate task duration more accurately and allocate time more effectively.\nProblem-Solving Efficiency: When facing technical challenges (such as RDS connection issues or Cognito JWT token validation), I sometimes spent too much time troubleshooting before seeking help or consulting documentation. I should develop a more systematic approach to problem-solving: first checking documentation, then testing systematically, and finally seeking guidance when needed.\nCommunication Skills: I need to improve my ability to communicate technical issues and solutions more clearly, especially when presenting architecture decisions or explaining complex configurations to team members. This includes better documentation of troubleshooting steps and more effective verbal communication during team discussions.\nCost Optimization Awareness: Initially, I focused more on functionality than cost optimization. For example, VPC Flow Logs generated high CloudWatch costs before I implemented retention policies and S3 storage. I should consider cost implications earlier in the design phase.\nReflection This internship provided invaluable hands-on experience with AWS cloud services and best practices. The challenges I encountered, such as configuring API Gateway VPC Links, implementing SSH-less deployments, and troubleshooting CloudWatch alarms, have significantly strengthened my problem-solving skills and technical knowledge.\nThe project taught me the importance of:\nInfrastructure as Code: Using CloudFormation for reproducible deployments Security First: Implementing least-privilege IAM policies and network segmentation Monitoring and Observability: Establishing comprehensive logging and alerting Automation: Reducing manual errors through CI/CD pipelines Documentation: Maintaining detailed records for troubleshooting and knowledge transfer I am grateful for the opportunity to work on this comprehensive AWS project and look forward to applying these skills in future cloud architecture and DevOps roles.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations! You have successfully completed the Workshop AWS Project deployment! In this workshop, you learned:\nHow to deploy a full-stack application on AWS using CloudFormation Architecture patterns for high availability and scalability Security best practices with private subnets and VPC endpoints Monitoring and auto-scaling configurations Frontend and backend deployment processes Clean Up Resources To avoid incurring charges, delete all resources created during this workshop.\nMethod 1: Delete CloudFormation Stack (Recommended) The easiest way to clean up is to delete the CloudFormation stack:\naws cloudformation delete-stack \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 Note: Some resources may need manual deletion if they have deletion protection or dependencies.\nMethod 2: Manual Cleanup (if needed) If the stack deletion fails or leaves some resources, manually delete:\nStop EC2 Instances:\naws autoscaling update-auto-scaling-group \\ --auto-scaling-group-name \u0026lt;ASG_NAME\u0026gt; \\ --min-size 0 \\ --desired-capacity 0 \\ --region ap-southeast-1 Delete S3 Buckets:\n# Empty frontend bucket aws s3 rm s3://workshop-aws-dev-frontend-502310717700-ap-southeast-1/ --recursive aws s3 rb s3://workshop-aws-dev-frontend-502310717700-ap-southeast-1 # Empty backend bucket aws s3 rm s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/ --recursive aws s3 rb s3://workshop-aws-dev-backend-502310717700-ap-southeast-1 Disable and Delete CloudFront Distribution:\nFirst disable the distribution (wait for it to be disabled) Then delete it Delete RDS Database:\nTake a final snapshot if needed Delete the database instance (may take 10-15 minutes) Delete API Gateway:\nDelete the REST API Delete Load Balancer:\nDelete the Application Load Balancer Delete VPC Endpoints:\nDelete all VPC endpoints Delete CloudWatch Logs:\naws logs delete-log-group \\ --log-group-name /aws/workshop-aws/dev/application \\ --region ap-southeast-1 Verify Cleanup Check the following services to ensure all resources are deleted:\nEC2: No instances, security groups, or load balancers RDS: No database instances S3: No buckets CloudFront: No distributions API Gateway: No APIs VPC: VPC and related resources (if not managed by CloudFormation) CloudWatch: No log groups IAM: Review and delete custom roles/policies if created manually Cost Verification After cleanup, verify in AWS Cost Explorer that no charges are accruing for the deleted resources.\n"},{"uri":"https://Fatnotfatt.github.io/learning-aws/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://Fatnotfatt.github.io/learning-aws/5-workshop/5.1-workshop-overview/","title":"","tags":[],"description":"","content":"\u0026mdash; title: \u0026ldquo;Introduction\u0026rdquo; date: 2025-09-09 weight: 1 chapter: false pre: \u0026quot; 5.1. \u0026quot; Workshop AWS Project Overview This workshop demonstrates how to deploy a complete full-stack web application on AWS using Infrastructure as Code (CloudFormation). You will learn to build a production-ready architecture with:\nBackend: Spring Boot REST API running on EC2 instances in private subnets Frontend: React application served via CloudFront from S3 Database: MySQL RDS instance for data persistence API Gateway: RESTful API Gateway for frontend-backend communication Load Balancer: Application Load Balancer for high availability Architecture Components VPC: Custom VPC with public and private subnets across 2 Availability Zones EC2 Auto Scaling Group: Backend application servers with auto-scaling capabilities RDS MySQL: Managed database service for application data S3 Buckets: Frontend static hosting and backend artifact storage CloudFront: CDN for global content delivery API Gateway: RESTful API endpoint with CORS support VPC Endpoints: Private connectivity to AWS services (S3 Gateway, SSM, SSM Messages, EC2 Messages, CloudWatch Logs) Systems Manager: Secure access to EC2 instances without SSH keys Key Features Infrastructure as Code: Entire infrastructure defined in CloudFormation High Availability: Multi-AZ deployment with Auto Scaling Security: Private subnets, security groups, IAM roles, VPC endpoints Monitoring: CloudWatch logs, alarms, and metrics Cost Optimization: VPC endpoints to reduce NAT Gateway data transfer costs Scalability: Auto Scaling based on CPU metrics "},{"uri":"https://Fatnotfatt.github.io/learning-aws/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://Fatnotfatt.github.io/learning-aws/tags/","title":"Tags","tags":[],"description":"","content":""}]